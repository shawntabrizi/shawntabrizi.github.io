"use strict";(self.webpackChunkshawntabrizi=self.webpackChunkshawntabrizi||[]).push([[8749],{91895:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/02/05/the-future-of-web3","metadata":{"permalink":"/blog/2025/02/05/the-future-of-web3","source":"@site/blog/2025-02-05-the-future-of-web3.md","title":"The Future of Web3","description":"Here are the slides I presented at the BUIDL HERE Conference in San Juan, Puerto Rico, which gives a insight into the meaning and direction of Web3 and Polkadot.","date":"2025-02-05T00:00:00.000Z","tags":[{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"},{"inline":true,"label":"cloud","permalink":"/blog/tags/cloud"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"},{"inline":true,"label":"future","permalink":"/blog/tags/future"}],"readingTime":0.435,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"The Future of Web3","date":"2025-02-05T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["polkadot","cloud","web3","future"]},"unlisted":false,"nextItem":{"title":"Make Kusama Chaotic Again","permalink":"/blog/2024/12/12/make-kusama-chaotic-again"}},"content":"Here are the slides I presented at the [BUIDL HERE Conference](https://www.prblockchain.org/buidl-here-2025/) in San Juan, Puerto Rico, which gives a insight into the meaning and direction of Web3 and Polkadot.\\n\\n## Presentation\\n\\n<iframe src=\\"/assets/presentations/the-future-of-web3.pdf\\" width=\\"720px\\" height=\\"480px\\"></iframe>\\n\\nYou can find a link to [Google Slides here](https://docs.google.com/presentation/d/1nPDGVwVOMpdo103jitZjKx2tD-F54h4W4c7tqdV7h44/edit?usp=sharing).\\n\\nFeel free to fork and use this presentation as your own.\\n\\n## Questions?\\n\\nFeel free to contact me with any questions you might have.\\n\\nIf you enjoy this content and want to continue to support me, take a look at my [donations page](https://shawntabrizi.com/donate/)."},{"id":"/2024/12/12/make-kusama-chaotic-again","metadata":{"permalink":"/blog/2024/12/12/make-kusama-chaotic-again","source":"@site/blog/2024-12-12-make-kusama-chaotic-again.md","title":"Make Kusama Chaotic Again","description":"This is a repost from the Polkadot forum, where I describe my vision for the Kusama network.","date":"2024-12-12T00:00:00.000Z","tags":[{"inline":true,"label":"kusama","permalink":"/blog/tags/kusama"},{"inline":true,"label":"zero-knowledge","permalink":"/blog/tags/zero-knowledge"},{"inline":true,"label":"cypherpunk","permalink":"/blog/tags/cypherpunk"},{"inline":true,"label":"privacy","permalink":"/blog/tags/privacy"}],"readingTime":2.65,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Make Kusama Chaotic Again","date":"2024-12-12T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["kusama","zero-knowledge","cypherpunk","privacy"]},"unlisted":false,"prevItem":{"title":"The Future of Web3","permalink":"/blog/2025/02/05/the-future-of-web3"},"nextItem":{"title":"The Polkadot Cloud","permalink":"/blog/2024/10/30/the-polkadot-cloud"}},"content":"##### This is a [repost from the Polkadot forum](https://forum.polkadot.network/t/make-kusama-chaotic-again/11123), where I describe my vision for the Kusama network.\\n\\nI want to open this forum post as a place for coordination on efforts to bring life and vibes back to the Kusama ecosystem.\\n\\n## Facts\\n\\n- Kusama was one of the coolest things to be created in the Polkadot ecosystem.\\n    - Expect Chaos\\n    - Cypherpunk / Glitch Vibes: [[ex1]](https://www.youtube.com/watch?v=bMegZaFfrzI) [[ex2]](https://www.youtube.com/watch?v=cDHPFsPX9iA) [[ex3]](https://www.youtube.com/watch?v=oDN9ClmGqkc)\\n    - Pushing the Limits (validators, spammening, parachains, etc...)\\n- Kusama plays a fundamental role in the security and safety of the Polkadot Network.\\n- Making Kusama important and relevant for the world is good for Polkadot.\\n- There are things Kusama can do that Polkadot cannot, both physically and spiritually.\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/bMegZaFfrzI?si=KFfAyVo0Nacw3joQ\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n## Hypothesis\\n\\n- Kusama has died down recently:\\n    - Polkadot has been really stable recently, causing people to forget how important Kusama is.\\n    - Kusama lacks a fundamentally unique selling point and feature set.\\n    - Kusama lacks a fundamentally unique narrative and theme which is independent from Polkadot.\\n\\n## Potential Solution\\n\\nMake Kusama Chaotic Again.\\n\\n- What do we mean by this?\\n\\n  Let\'s make Kusama the number one home for the original Cypherpunk movement:\\n\\n  * Privacy-focused\\n  * Fully and Truly Decentralized\\n  * Self-Sovereign\\n  * Cryptography Pioneers\\n  * Open, Public Communication\\n  * Anti-Censorship\\n  * Technological Transparency\\n  * Activist Mindset\\n  * Anonymity Valuing\\n  * DIY Ethos\\n  * Freedom-Preserving\\n\\n- How can we do this?\\n\\n    - Bring ZK primitives to Kusama.\\n        - Support low-level host functions to execute ZK functions fast and efficiently.\\n        - Expose such functions through the upcoming Kusama Hub contracts platform.\\n        - Maybe a community seeded [\\"Trusted Setup Ceremony\\"](https://a16zcrypto.com/posts/article/on-chain-trusted-setup-ceremony/)\\n    - Experiment with better / more resilient forms of on-chain randomness generation.\\n    - Create a home for Privacy.\\n        - Privacy Preserving Stablecoin: Kash\\n        - Privacy Creating Services (will remain unnamed)\\n        - Privacy Preserving Voting Systems\\n    - Post-Quantum Signatures and Experiments\\n    - Create a Kusama community manifesto and vision\\n        - Be the Web3 Cloud for the [\\"Dark Renaissance\\"](https://github.com/darkrenaissance):\\n          - [DarkFi](https://github.com/darkrenaissance/darkfi)\\n          - ZCash\\n          - Monero\\n          - Prediction Markets\\n          - True, unapologetic DEXs\\n          - P2P Markets\\n          - etc...\\n      - Enshrine the cypherpunk movement and spirit.\\n      - Describe how Kusama and Polkadot differ, yet are still important for one another.\\n      - Ensure that Kusama treasury allocates and funds projects in this nature.\\n  - What else? Comment in this thread.\\n\\n## Action Items\\n\\nSuch a future does not create itself. We must create alignment and a community of people who will steward this future forward.\\n\\n- Discussion: Are there enough people aligned within the Kusama community to this narrative?\\n- Formation: Who is willing to step up to lead such efforts, and create groups for people to work together on this?\\n- Adoption: Who can represent the Kusama technology stack to other like-minded projects and individuals who may want to participate?\\n- Execution: How can we reclaim access to the Kusama treasury, and allocate funds for this specifically? Start with a bounty?\\n\\n## Related\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/ziXIjY5MeVo?si=hDTQAzPi1r5saxm7\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nContinue the conversation further in the [Polkadot Forum](https://forum.polkadot.network/t/make-kusama-chaotic-again/11123)."},{"id":"/2024/10/30/the-polkadot-cloud","metadata":{"permalink":"/blog/2024/10/30/the-polkadot-cloud","source":"@site/blog/2024-10-30-the-polkadot-cloud.md","title":"The Polkadot Cloud","description":"This is a repost from the Polkadot forum, where I describe my vision of the Polkadot Cloud.","date":"2024-10-30T00:00:00.000Z","tags":[{"inline":true,"label":"cloud","permalink":"/blog/tags/cloud"},{"inline":true,"label":"hub","permalink":"/blog/tags/hub"},{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"}],"readingTime":21.12,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"The Polkadot Cloud","date":"2024-10-30T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["cloud","hub","polkadot"]},"unlisted":false,"prevItem":{"title":"Make Kusama Chaotic Again","permalink":"/blog/2024/12/12/make-kusama-chaotic-again"},"nextItem":{"title":"The Role of the Polkadot Treasury","permalink":"/blog/2024/07/08/the-role-of-the-polkadot-treasury"}},"content":"##### This is a [repost from the Polkadot forum](https://forum.polkadot.network/t/the-polkadot-cloud/10670), where I describe my vision of the Polkadot Cloud.\\n\\nI think it\'s time to open a thread dedicated to the ideas which have been spreading like wildfire (very much by intention) around the Polkadot Hub and Polkadot Cloud.\\n\\nFirst, before we even get started, it is important to note that this is just a **discussion of ideas**. Nothing here is locked in yet, official, or anything like that. The point of these conversations is to get community alignment of ideas, terminology, vision, and direction.\\n\\nThe phrase I have been using is that Polkadot is like a bunch of cats in a room, totally doing their own thing. We need a laser pointer for us all to focus on a long-term vision, and to move together towards that goal.\\n\\nSuch a vision should allow us to:\\n\\n- make better decisions as a community\\n- better represent Polkadot outside of our community\\n- ensure that we are building things that are relevant for our needs\\n\\nWhat is also important is that our vision is not made up. It must be something we can actually achieve, and that we have ideas to support.\\n\\n## What is Polkadot?\\n\\nPolkadot is not a blockchain. Polkadot of course, has a blockchain, and a token, but the meaning of Polkadot extends far beyond that.\\n\\n**Polkadot is a vision toward a world with less trust and more truth.**\\n\\n![](/assets/images/cloud1.jpeg)\\n\\nThis is the friendly way to tell people that we are trying to build and innovate with **Web3** principles.\\n\\nI think if you look at all that Polkadot has done in the past, is doing now, and will do into the future, this single vision will permeate every decision and action we take.\\n\\nThis vision will likely never change, nor should it. I think Polkadot is defined by the journey we take toward this vision.\\n\\nPolkadot has a **mission**, which describes the direction we go in achieving that vision. Our mission **can change**, but practically, changes in our mission should only occur over long periods of time. Only when what is needed to reach our vision changes, or we have achieved our current mission.\\n\\nWe can describe Polkadot\'s mission up until now and into the near future:\\n\\n**Polkadot\u2019s mission is to provide a scalable, secure, and resilient platform for Web3 applications and services.**\\n\\n![](/assets/images/cloud2.jpeg)\\n\\nAn image to put in your mind is:\\n\\n![](/assets/images/cloud3.jpeg)\\n\\nThat is to say, Amazon Web Services has fundamentally changed the internet today by making it **cheap, easy, and scalable** to launch Web2 applications and services into the cloud.\\n\\nPolkadot\'s current mission is to do the same thing, but for Web3 services. We believe this is the way that the Polkadot ecosystem can currently bring this vision of a world with less trust and more truth to the world, and while we acknowledge achieving this vision requires more than just technology, we think it is the first primitive the world needs to get started.\\n\\nSo what does it mean to have a vision towards less trust and more truth?\\n\\nTo answer this, we must ask \\"What is Web3?\\".\\n\\n### What is Web3?\\n\\nWeb3 is a fundamental shift of removing trust from Web2, the technology stack that we currently use to power the internet.\\n\\nIn my recent presentation, I represented this as a difference in *resilience*.\\n\\n![](/assets/images/cloud4.jpeg)\\n\\nBut in fact, there are other principles of Web3 we should not forget:\\n\\nAs quoted by Gav:\\n\\n> #### Driving Factors and Web3 Maxims\\n>\\n> - Resilience\\n> - Generality\\n> - Performance\\n> - Coherency\\n> - Accessibility\\n\\nI would also like to include the [5 pillars of open blockchains from Andreas Antonopoulos](https://www.youtube.com/watch?v=qlAhXo-d-64), which was certainly the predecessor to Web3 ideologies:\\n\\n> - Open\\n> - Public\\n> - Borderless\\n> - Neutral\\n> - Censorship Resistant\\n\\nThis post is probably not best suited to be the introduction into the principles and philosophies of Web3.\\n\\nIf there are nice posts which consolidate and teach the ideas of Web3 which can be linked here, please feel free to post them in this thread. Otherwise, perhaps look at the old youtube videos of Gav or other Web3 leaders about their vision of the space.\\n\\n## Polkadot\'s Products\\n\\nSo we have established our vision, and have a clear mission to bring technologies into the world which can power Web3 applications and services.\\n\\nHow does this actually manifest into products?\\n\\nWell my perspective is that Polkadot has always been building toward two products, which attempt to satisfy this mission:\\n\\n- The Polkadot Cloud\\n- The Polkadot Hub\\n\\n> NOTE: It is important at this point to not get too attached to these specific names. Many resonate with the Polkadot Cloud, but some have opinions about the \\"Polkadot Hub\\", and this is the time to discuss those opinions and come to a consensus. I personally prefer these two names, and would be happy to have those discussions with anyone here in this thread.\\n\\nUnfortunately the history of developing the Polkadot, we had not clearly defined these two products. But it does not change the fact that if we think about what we have been building so far and the vision we are going toward, we have ALWAYS been building these two products.\\n\\nSo, what is the Polkadot Cloud and the Polkadot Hub?\\n\\n### The Polkadot Cloud\\n\\nThe Polkadot Cloud is a secure, scalable, and resilient platform for Web3 applications and services.\\n\\nThe Polkadot Cloud **is** our current mission.\\n\\nIf you were to make a sales pitch for the Polkadot Cloud, it might look something like this:\\n\\n> The Polkadot Cloud is a platform for Web3 applications and services.\\n>\\n> On the Polkadot Cloud, we provide services with high throughput, native interoperability, and shared security. Our cloud platform is elastic, dynamic and multi-core.\\n>\\n> With over 100 execution cores we are able to achieve 150,000 transactions per second across the Cloud, and over 150 MB/s data availability throughput!\\n>\\n> The Polkadot Cloud offers a number of different Web3 Services such as:\\n> - Cloud Execution Service\\n> - Settlement / Finality Service\\n> - Data Availability Service\\n> - Object Storage Service\\n> - Blockchain Hosting Service\\n> - and more!\\n>\\n> All of these services work together seamlessly to create an all-in-one platform for deploying your app. We provide everything you need, so you can focus on what you are building.\\n>\\n> Using the Polkadot Cloud, you are able to deploy any kind of Web3 application or service cheap, easy, and at scale.\\n\\nNote here that the Polkadot Cloud represents all that we have accomplished so far, and even looks into what we want to do in the future.\\n\\nIt also breaks down the various features of Polkadot into separate services that are offered and bundled by the Cloud. And this better represents what you can actually do, rather than what we have currently built.\\n\\nThere are teams already experimenting with using our individual services like [data availability](https://github.com/thrumdev/blobs) or [cloud execution](https://github.com/polkadot-fellows/RFCs/pull/127) to secure rollups on other ecosystems. We could be part of every \\"modular blockchain\\" story. It\'s just that we have been focused on building an all-in-one solution.\\n\\n#### History of the Polkadot Cloud\\n\\nWith this framing of the Polkadot Cloud, I think we are able to actually look back at history, and define a clear story of what we have been building so far.\\n\\nNot to make up history, but to re-frame what we\'ve done.\\n\\n- Polkadot Cloud - Genesis: May 2020\\n- Polkadot Cloud - Milestone I (Parachains): November 2021\\n    - First Cloud Services Deploy: December 2021\\n- Polkadot Cloud - Milestone II (Elastic): October 2024\\n- Polkadot Cloud - Milestone III (JAM): TBD\\n\\nSo really, the initial Polkadot launch was really around creating an all-in-one **blockchain hosting service** for other blockchains on the Polkadot Cloud.\\n\\nInitially that blockchain hosting service was very simple in how it created, allocated, and used blockspace.\\n\\nOur work since the launch of Polkadot have been to make our hosting service more agile, elastic, and flexible. This is what we previously called \\"Polkadot 2.0\\", but it really isn\'t a new product at all! It is an iteration of the Polkadot Cloud vision.\\n\\nJust like JAM is also not a brand new idea, even though the architecture of the Polkadot Cloud will change significantly from it. It is yet another iteration on the mission to create the best platform for Web3 applications and services.\\n\\nWith the third milestone of the Polkadot Cloud (codename JAM), we are looking to extend functionality of our Web3 Cloud platform to support even applications and services which are NOT blockchains.\\n\\nI think this kind of positioning helps explain to the world what the heck is going on with things like JAM, which we have really struggled to explain in respect to the existing Polkadot Cloud.\\n\\nAnd certainly there will be milestone 4, 5, 6, etc... The development and improvement of the Polkadot Cloud will always continue. It is wrong to look at the development of Polkadot as \\"building new products\\". The product is the same, it is just iteratively (or sometimes radically) getting better.\\n\\nThe question you need to ask when digging into technical development, is:\\n\\n> \\"How does this improve the Polkadot Cloud?\\".\\n\\nThis is what we need to be communicating.\\n\\n#### Comparisons to Web2 Clouds\\n\\nI think that this vision of the Polkadot Cloud allows us to evaluate what we have been building, and where we should be building toward. Thankfully, we have large businesses like Amazon, Google, and Microsoft who have all built Web2 clouds, of which I argue we are not that different from an architectural standpoint.\\n\\nFor example, look at the service offerings for Amazon:\\n\\n- [Amazon EC2](https://aws.amazon.com/ec2/)\\n    - EC2 = Elastic Compute Cloud\\n    - \\"Amazon EC2 is AWS\'s service that provides secure, scalable computing capacity in the cloud.\\"\\n    - \\"Reliable and scalable infrastructure on-demand, with 99.99% availability SLA\\"\\n    - \\"purchase model to help you best match the needs of your workload\\"\\n- [Amazon S3](https://aws.amazon.com/s3/)\\n    - S3 = Simple Storage Service\\n    - \\"Amazon S3 is an object storage service offering industry-leading scalability, data availability, security, and performance.\\"\\n    - Benefits:\\n        - Scalability\\n        - Durability and availability\\n        - Security and data protection\\n        - Lowest price and highest performance\\n- [AWS Lambda](https://aws.amazon.com/lambda/)\\n    - \\"Run code without thinking about servers or clusters\\"\\n    - \\"AWS Lambda is a serverless computing service that runs your code in response to events without requiring provisioning or management of servers. It automatically scales compute resources and you pay only for the compute time used. The main benefits are no server management, automatic scaling, pay-per-use billing, and performance optimization options.\\"\\n\\nDoesn\'t this draw a lot of parallels to:\\n- Our execution service\\n- Our data availability service\\n- The proposed [\\"CorePlay\\" service](https://github.com/polkadot-fellows/RFCs/blob/ba59c9f4675e072603dd6a6c6dccdcd9c7d1524a/text/coreplay.md)\\n\\nTake a look at the [Google Cloud](https://cloud.google.com/) landing page. Couldn\'t you see this structure and style being an effective way to explain and sell the Polkadot Cloud?\\n\\nImagine all that we can learn about creating a better Web3 cloud for the world by really framing ourselves as a product architected similar to the traditional cloud services we want to replace.\\n\\n\\n#### A \\"10 Year\\" Vision of the Polkadot Cloud\\n\\nWhat is important about this broad narrative for the Polkadot Cloud, is that it allows us to not be limited by what we are doing today, and think about what we could be building tomorrow.\\n\\nMany teams around the Polkadot ecosystem are looking for the right ideas to support, and decentralized organizations like the Polkadot DAO have a hard time understanding if we really need some of those things.\\n\\nWith the Polkadot Cloud mentality, we can easily ask the question: \\"How are you looking to improve the experience for applications and services using the Polkadot Cloud?\\"\\n\\nWe can also get ourselves out of the gutter of thinking we are just a blockchain product. The Web3 Cloud will have many services that are NOT blockchain based! (or at least not directly related to Parachains)\\n\\nFor example:\\n\\n- Mix Nets (a la [Nym](https://nymtech.net/))\\n- Privacy Layers (a la [ZCash](https://z.cash/))\\n- Oracle Services (a la [Chainlink](https://chain.link/))\\n- P2P Communication Services (a la [Whisper](https://github.com/ethereum/whisper))\\n- Persistent File Storage Services (a la [IPFS](https://ipfs.tech/))\\n\\nIt\'s really not crazy to think that the Polkadot Cloud would want to build or directly integrate these services into our offerings and how such services will bring better experiences to our users, and allow them to build more secure, scalable, and resilient Web3 applications and services.\\n\\n### The Polkadot Hub\\n\\nThe Polkadot Hub is the home for the Polkadot community, the DOT token, and other tokens across the Polkadot ecosystem.\\n\\nIf you were to make a sales pitch for the Polkadot Hub, it might look something like this:\\n\\n> The Polkadot Hub is a Layer 1 blockchain that supports smart contracts and is deployed on and secured by the Polkadot Cloud.\\n>\\n> The Polkadot Hub has native features such as:\\n> - Smart Contracts\\n> - Staking\\n> - Governance\\n> - Treasury\\n> - Stablecoins\\n> - Token Registry\\n> - etc...\\n>\\n> The Polkadot Hub uses a Ethereum-compatible smart contract platform. This allows anyone to add their own programmable layers to Polkadot.\\n>\\n> The purpose of the Polkadot Hub is to bring coordination, funding, and direction for the development and future of the Polkadot Cloud.\\n\\nSo the primary vibe of the Polkadot Hub should be **community**.\\n\\nEvery part of the Polkadot Hub is about tapping into the Polkadot community, and using them as a coordination machine bringing decentralization to the Polkadot Cloud.\\n\\n#### History of the Polkadot Hub\\n\\nOur mission to create a platform for Web3 applications and services necessitated the creation of the Polkadot Hub.\\n\\nYou cannot have a decentralized platform without having a decentralized set of decision makers. In many ways, the Polkadot Hub is what separates the Polkadot Cloud from products which are \\"blockchain\\" or \\"web3\\" only by name.\\n\\nSo let\'s again re-frame our history with this product in mind:\\n\\n- Features launched on the Polkadot Cloud, but no Hub yet\\n- Creation of the Polkadot Hub, with Token Registry (Asset Hub)\\n- Proper Launch of the Polkadot Hub with Ethereum-compatible smart contracts (TBD, see launch details below)\\n- Transition Staking, Governance, and Treasury to the Polkadot Hub (TBD)\\n\\nSince the genesis of the Polkadot Cloud, features of the Polkadot Hub have always been present. But these features lived directly in the Cloud, and were initially built this way because the Polkadot Cloud did not have its Hosting Service ready to use yet.\\n\\nA number of projects have been ongoing, which would now be captured by the idea of the Polkadot Hub:\\n- The Asset Hub\\n    - Token Registry\\n    - Pay fees with any token\\n    - Hold tokens without DOT\\n- The Minimal Relay Chain\\n- Smart Contracts on Polkadot\\n- etc...\\n\\nAgain, these projects were mostly being done in parallel, but without a clear vision of the goal and how they will all work together to paint a cohesive story. The Polkadot Hub brings a vision of what we are trying to build, and gives meaning to these various features.\\n\\n#### Launch of the Polkadot Hub\\n\\nThe Polkadot Hub is an opportunity to \\"relaunch\\" the Polkadot brand.\\n\\nFor the first time, Polkadot will be open season for any developers, tinkerers, or builders to be able to quickly and easily deploy applications and contracts to the Polkadot Ecosystem. You no longer need to be a \\"cloud developer\\" in order to use Polkadot. Making smart contracts is easy, and thus building in Polkadot is easy.\\n\\nImagine messaging like the following:\\n\\n> Polkadot is proud to announce the Polkadot Hub.\\n>\\n> The Polkadot Hub is a new L1 blockchain running on the Polkadot Cloud, and a launchpad for builders to tap into over 6 billion dollars in tokens across the Polkadot ecosystem.\\n>\\n> The Polkadot Hub will be Ethereum-compatible, meaning you can deploy your favorite Solidity smart contracts directly to the Polkadot ecosystem.\\n>\\n> The Polkadot Hub has native access to stable coins like USDC and USDT, as well as all of the tokens from the Polkadot ecosystem. The Polkadot Hub also has trustless bridges to Ethereum and other top blockchain ecosystems, meaning that you have worry-free access to essentially any token within the crypto market.\\n>\\n> On the Polkadot Hub, you can pay transaction fees with any of these tokens, meaning you can even hold and use stablecoins without needing any other token!\\n>\\n> The Polkadot Hub is a brand new, untapped resource for explosive product growth by tapping into one of the largest developer communities in the blockchain ecosystem.\\n>\\n> With millions of users at your fingertips, there is no better place to launch than the Polkadot Hub. You can have your **launchpad ready In: 3, 2, 1...**\\n\\n##### The Launchpad\\n\\nA relaunch is not something that can be done without planning.\\n\\nI would expect a number of specific initiatives to make the Polkadot Hub feel new, exciting, and like a proper launchpad for the next big projects in Web3.\\n\\n- The Polkadot Wallet\\n\\n   The goal is to have a clear and concise story around how users onboard into our ecosystem, and use applications and services across the Polkadot Cloud.\\n    - It should have fiat onboarding flows.\\n    - It should allow you to purchase in the real world with your Polkadot ecosystem tokens.\\n    - It should give you a mobile gateway into the hub.\\n    - It should allow you to \\"log in\\" to any Polkadot Ecosystem application or service.\\n        - With a standard like Wallet Connect.\\n\\n- \\"Polkadot Bundle\\" Token Sale\\n\\n    The goal is encourage people to buy and use Polkadot ecosystem tokens, thereby rebooting the ecosystem.\\n    - A new token sale coming from the Polkadot Treasury.\\n    - Sold at the market price of the DOT token.\\n    - \\"Bundled\\" with ecosystem tokens which are provided by ecosystem teams.\\n        - Some amount of compensation from the treasury can be used.\\n        - Many should contribute just based on it being a bootstrapping event.\\n        - Very much \\"airdrop\\" vibes.\\n    - Fully locked for 1 year. Fast vesting period afterward.\\n        - But with the ability to participate or spend the tokens on the various Polkadot Services.\\n    - Sold tokens go back to the treasury as stables, or to purchase more ecosystem tokens to include with the bundle.\\n\\n- Contract Builder Rewards\\n\\n     The goal is to incentivize contract creators to deploy on the Polkadot Hub, and also to iterate and design high quality contracts for the ecosystem. Basically, users that deploy smart contracts to the Polkadot Hub, should get a fraction of the gas fees used to call that contract. So currently 20% go to block producer, and 80% go to the treasury, we could probably shift 20% to 40% from the treasury directly to the contract creator, specifically when users call smart contracts. This can make it profitable to deploy and maintain contracts in the Polkadot Hub, without creating possibilities to \\"game\\" the system, since it is all coming from fees anyway. Also it helps normalize contracts which do not have \\"hidden fees\\" built into the contract code.\\n\\n- Micro-grants\\n\\n    The goal is to get a bunch of people building in the Polkadot Hub, bringing and executing new ideas.\\n    - Bounties for different application verticals:\\n        - DAO templates for opengov\\n        - staking pools templates\\n        - account abstractions (on top of our native abstractions)\\n        - cross chain messaging templates\\n        - memecoin communities\\n        - on-chain games\\n        - etc..\\n    - Rewards to the top teams in each vertical driving organic growth, and sustained usage for 6 months, as reviewed by a DAO / curator set.\\n\\n\\n- Participation Lotteries\\n\\n    The goal is to drive engagement of the Hub by end users.\\n    - Imagine some percentage of all transaction fees get funneled into a special smart contract which holds weekly lotteries.\\n    - You receive tickets from the lottery by submitting proofs that you have interacted with the Polkadot Hub.\\n        - Any interaction where you Pay a fee should qualify, for a ticket.\\n        - Users get a proportional number of tickets to the fees they have paid.\\n        - This should not be gameable or exploitable.\\n    - The contract selects a winner at random from among the tickets.\\n        - Can\'t really expect the lottery to be lifechanging, but is certainly a non-zero incentive.\\n        - This adds some positive EV to simply using the Polkadot Hub!\\n\\n- Hackathons and Education\\n\\n    The goal is to radically shift entry level hackathons and education from the Polkadot Cloud (as it is today) to the Polkadot Hub.\\n    - Create and work with teams to build new entry level educational material about the Polkadot Hub.\\n    - Tutorials, videos, guides, templates, etc...\\n    - Hackathon kits, which can be deployed by around the world for their local communities.\\n\\n#### Journey To Scale\\n\\nThe Polkadot Hub will now be an entry point for developers into the Polkadot ecosystem. But we know that the true power of Polkadot lies in the Polkadot Cloud.\\n\\nWe must have a story for how we scale these developers and teams from the Hub to the Cloud.\\n\\nFor this, there are two components:\\n\\n- Contract Hosting Service\\n- Universal Tokens\\n\\n##### Contract Hosting Service\\n\\nThis was previously known as CorePlay.\\n\\nI wrote some details here: https://forum.polkadot.network/t/the-polkadot-cloud/10670/39?u=shawntabrizi\\n\\n##### Universal Tokens\\n\\nUniversal tokens is the idea on how we should support tokens in the Polkadot ecosystem through the Polkadot Hub. As noted above, one of the native features of the Polkadot Hub is a token registry system.\\n\\nToday, when new Cloud Services launch, they mint their own token local to their blockchain, and then transfer them over to the hub for interoperability.\\n\\nIn smart contract ecosystems, they mint tokens in storage specific to smart contracts.\\n\\nAlso, unfortunately, the DOT token is not treated the same as other tokens on the Polkadot Hub today.\\n\\nIf we allowed this to continue, the Polkadot Hub would have like 4 different kinds tokens across the product!\\n\\nUniversal Tokens is a vision toward having all tokens in the Polkadot Ecosystem natively minted and managed on the Polkadot Hub. Ownership of those tokens can then be assigned to contracts on the Hub or services on the Cloud.\\n\\nWhat is key here is that Universal Tokens will allow a seamless pathway to transition ownership of the tokens from a contract to a cloud service. So even without a Unified Programming Language, end users of a product in the Polkadot ecosystem, will feel no pain at all when a transition like this happens. And in the scenario where we do have a Unified Programming Language, it might literally be seamless.\\n\\nThe other key feature of Universal Tokens is universal compatibility! If you launch your token natively on the Polkadot Hub, any contract or cloud service should be able to recognize and interact with that token in a standard way.\\n\\nImagine getting an NFT from a gaming service in the Polkadot Cloud, and then easily using or trading it on the Polkadot Hub, thanks to the fact that it is built using the Universal Token standard.\\n\\n#### A \\"10 Year\\" Vision of the Polkadot Hub\\n\\nThis is where I will put some ideas in your head about the immense vision of what the Polkadot Hub could be. Not that this is what SHOULD happen, but it is perfectly possible given the mission of the Polkadot Hub.\\n\\nIf we are imagining a community centered product, it is easy to see how it could manifest similar to Web2 social networks. Why not when you open up the \\"Polkadot Hub\\", it could land you on a page similar to Facebook?\\n\\nImagine seeing an activity feed of everything going on in the Polkadot Hub and broader Polkadot Ecosystem:\\n\\n- What proposals are currently being discussed.\\n- Latest announcements / features.\\n- Technical discussions / RFCs.\\n- Collectives reporting their activity.\\n- Ambassadors providing proof of their work.\\n- Memes and marketing.\\n- Education and tutorials.\\n- etc...\\n\\nNow imagine in your head the left sidebar:\\n\\n- Groups: The various DAOs / chat groups you are a part of.\\n- Events: The next in-person or online meetups for the Polkadot ecosystem.\\n- Marketplace: A place to use cryptocurrency trade for digital or real-world goods!\\n- Finance: A home for all things DeFi.\\n- Apps: Gateways to other applications and services hosted on the Polkadot Cloud.\\n- Games: A portal into Web3 enabled games and communities.\\n\\nImagine in the top right, clicking your avatar, and managing your settings / profile:\\n\\n- The accounts, wallets, and balances you have control of.\\n- Various account abstraction settings, like multisigs and proxies.\\n- Contracts you have deployed and their activity (users, revenue)\\n- Posts you have made in the Hub, and revenue generated from engagements.\\n- Your public identity.\\n- Your private individuality proofs.\\n- etc...\\n\\nFinally, a cohesive and familiar experience for all the craziness we currently feel in the space.\\n\\nAnd all of these features would be built with the principals of Web3! There is not a single team which builds this whole portal. It is a collection of work from the Polkadot ecosystem, unified by the Hub\'s vision and mission.\\n\\nWe can literally rebuild the systems which spy on and extract profits from us, with trust free alternatives!\\n\\nContinue the conversation further in the [Polkadot Forum](https://forum.polkadot.network/t/the-polkadot-cloud/10670)."},{"id":"/2024/07/08/the-role-of-the-polkadot-treasury","metadata":{"permalink":"/blog/2024/07/08/the-role-of-the-polkadot-treasury","source":"@site/blog/2024-07-08-the-role-of-the-polkadot-treasury.md","title":"The Role of the Polkadot Treasury","description":"This is a repost from the Polkadot forum, used to capture my thoughts on how Polkadot should manage spending in its treasury.","date":"2024-07-08T00:00:00.000Z","tags":[{"inline":true,"label":"treasury","permalink":"/blog/tags/treasury"},{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"}],"readingTime":6.205,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"The Role of the Polkadot Treasury","date":"2024-07-08T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["treasury","polkadot"]},"unlisted":false,"prevItem":{"title":"The Polkadot Cloud","permalink":"/blog/2024/10/30/the-polkadot-cloud"},"nextItem":{"title":"Disposable Parachains","permalink":"/blog/2024/01/20/disposable-parachains"}},"content":"##### This is a [repost from the Polkadot forum](https://forum.polkadot.network/t/is-the-treasury-polkadots-biggest-vc/9015), used to capture my thoughts on how Polkadot should manage spending in its treasury.\\n\\nIn my opinion, OpenGov and the Polkadot Treasury have been some of the most exciting experiments coming out of the entire Web3 movement so far.\\n\\nThe rules of these systems are relatively simple and well-defined:\\n- The Treasury perpetually grows through transaction fees and inflation.\\n- Spending of the treasury is determined by existing token holders.\\n\\nHowever, the final outcome of this game is not simple or well-defined at all.\\n\\nOne thing is clear though: because the treasury is controlled by DOT token holders, voters are naturally incentivized to spend funds that improve the Polkadot Ecosystem and the DOT token value.\\n\\nBut beyond that, what is the role of the Polkadot Treasury?\\n\\n## The Role of the Treasury\\n\\nCompare the relationship between:\\n\\n- Modern Societies\\n- Governments\\n- Public Treasuries\\n\\nand\\n\\n- The Blockchain Ecosystem\\n- Polkadot\\n- The Polkadot Treasury\\n\\nMuch of the advancement of modern society can be attributed to innovation created by free-market economics. The blockchain ecosystem operates under similar principles, where competition and the drive for profit lead to technological advancements and efficiencies. However, just as in modern societies, there are certain essential resources and services that are best provided as public goods.\\n\\n**My view is that the Polkadot Treasury should primarily be used to support public goods in the Polkadot Ecosystem and should avoid speculative investments.**\\n\\n### Support Public Goods\\n\\nIn modern societies, governments use their treasuries to fund public goods and services that benefit everyone, such as roads, public parks, education, and healthcare. These are things that the free market either cannot provide effectively or would provide only to those who can afford them, leading to inequality and inefficiency.\\n\\nSimilarly, in the Polkadot ecosystem, the Polkadot Treasury should be used to fund initiatives that provide broad, non-excludable benefits to the entire community.\\n\\nThese can include (but are not limited to):\\n\\n- Development SDKs: Tools and libraries that make it easier for developers to build on Polkadot.\\n- Block Explorers: Platforms that allow users to explore blockchain data freely and easily.\\n- Client Libraries: Libraries in multiple programming languages that facilitate interaction with the Polkadot network.\\n- Public Nodes: Nodes that allow light clients and their users access to the network.\\n- Public Education: Initiatives that educate users and developers about the Polkadot ecosystem.\\n\\nJust as public parks and roads benefit everyone in a community, public goods in the Polkadot ecosystem benefit all users and developers. Most importantly, these kinds of services have few sensible pathways to be funded except for a public treasury.\\n\\n### Avoid Speculative Investments\\n\\nI would also argue that treasury spending controlled by the public usually leads to poor speculative decision-making. Generally, voters lack the technical expertise needed to evaluate complex proposals and cannot easily gain insight into the inner workings of a business, resulting in decisions based on superficial understanding or emotional appeal.\\n\\nPublic votes (especially in the blockchain space) can also be swayed by \\"popular trends\\" and short-sighted results. This results in projects that attempt to bribe the treasury or decisions that compromise on the principles of what we are trying to achieve in the long term.\\n\\nIn contrast, VC funding usually yields higher quality results due to accountability and expertise. Investors put their own money and reputation at risk when backing a speculative project, and as a result, are incentivized to conduct thorough research and make informed decisions. VCs are also more capable of supporting teams directly and gaining transparency into the actions of a team. This is why free markets have generated great results.\\n\\nIf we could get VCs in the Polkadot Ecosystem interested in evaluating and supporting public goods efforts, that would be great! We have seen evidence of this happening, for example, Consensys in the Ethereum ecosystem. But generally speaking, we cannot expect that philanthropy and corporate sponsorships will be there when we need them, and thus the treasury must exist to provide access to resources to fund public goods.\\n\\n### Flexibility and Experimentation\\n\\nThere are no hard-coded rules for treasury spending, nor would we want to introduce any. It is important in a decentralized system to keep low-level APIs unopinionated and allow for community influence to dictate the behavior of the system.\\n\\nI am not against the idea of the treasury being used in other creative ways, especially when the cost/risk is relatively low or the experimentation value is high.\\n\\nFor example, we have recently seen a surge of various marketing efforts through the treasury. While I may not agree with many of the specific pathways and choices, I could see how marketing Polkadot to the world is a kind of public good for our ecosystem. Thus, perhaps treasury funding is the right place to fund marketing efforts.\\n\\nThere were also recent posts on having the treasury invest in Parachain projects. As I have already said, I don\'t think OpenGov can make great decisions on speculative investment opportunities, but that doesn\'t mean I would be against trying it out to see what happens. Especially if the pool of teams that qualify for funding would be limited.\\n\\nHowever, I think that these kinds of speculative efforts should be the exception, not the norm. We can only establish a norm by discussing our collective belief on where the treasury should be spending and making clear pathways for projects that don\'t align with that vision.\\n\\n## My Take on Other Funding Avenues\\n\\nWhile at PBA Singapore 2024, we had the opportunity to hear from Web3 Foundation\'s Seraya from the Grants team talk about funding avenues in the Polkadot Ecosystem.\\n\\nHis presentation can be found here:\\n\\n[Web3 Foundation Grants Presentation](https://docs.google.com/presentation/d/1klUCa1QonjVxV_yc4VWWo3I2aXwgELj8np1ZnfCzc8I/edit?usp=sharing)\\n\\nHopefully, a video link will be made available in the near future, and I will happily update this post to include that when it is.\\n\\nHis presentation covered many different avenues such as:\\n\\n- Decentralized Futures Program\\n- W3F Grants Program\\n- Treasury Funding\\n- VC Funding (like through Polimec)\\n\\nHe spoke on the mechanics and practicalities of these funding avenues, but not much on which one to pick for your project or idea.\\n\\nHere is my mental model on where I would expect certain projects to best fit in terms of getting their funding request approved:\\n\\n![Funding Decision Tree](/assets/images/funding-decision-tree.png)\\n\\n<details>\\n\\n<summary>Mermaid Diagram Text</summary>\\n\\n ```mermaid\\ngraph TD\\n    FP(Is your product for profit?)\\n    FP--\x3e|No|PUBVAL\\n    FP--\x3e|Yes|INVESTOR\\n\\n    PUBVAL(Does your product provide clear/immediate public value?)\\n    PUBVAL--\x3e|Yes|T[Treasury Funding]\\n    PUBVAL--\x3e|No|LONGTERM\\n\\n    LONGTERM(Does your product align with the vision of Web3?)\\n    LONGTERM--\x3e|Yes|W3F[W3F Grants Program]\\n    LONGTERM--\x3e|No|WHY[Why should we fund you?]\\n\\n    ALIGN(Does your product align with the vision of Web3?)\\n    ALIGN--\x3e|Yes|DFP[Decentralized Futures Program]\\n    ALIGN--\x3e|No|WHY[Why should we fund you?]\\n\\n    INVESTOR(Does the product have clear/existing investor value?)\\n    INVESTOR--\x3e|No|ALIGN\\n    INVESTOR--\x3e|Yes|VC[VC Funding]\\n\\n    style T fill:#f9f,stroke:#333,stroke-width:4px;\\n    style W3F fill:#f96,stroke:#333,stroke-width:4px;\\n    style DFP fill:#6f9,stroke:#333,stroke-width:4px;\\n    style VC fill:#69f,stroke:#333,stroke-width:4px;\\n    style WHY fill:#f99,stroke:#333,stroke-width:4px;\\n```\\n\\n</details>\\n\\nAgain, this is just my opinion, so I would be really happy to hear about other important funding avenues not included, and other people\'s take on the decision path to select that avenue.\\n\\n## What do you think?\\n\\nI would be interested to hear other people present their opinions.\\n\\n- What is the role of the Polkadot Treasury?\\n- How should the treasury approach speculative investments?\\n- In what ways should we experiment with Treasury spending?\\n- What are other key funding avenues which were not mentioned and how should they be used?\\n\\nContinue the conversation further in the [Polkadot Forum](https://forum.polkadot.network/t/is-the-treasury-polkadots-biggest-vc/9015)."},{"id":"/2024/01/20/disposable-parachains","metadata":{"permalink":"/blog/2024/01/20/disposable-parachains","source":"@site/blog/2024-01-20-disposable-parachains.md","title":"Disposable Parachains","description":"This is a repost from the Polkadot forum, discussing the possibility of short lived parachains on the Polkadot Network for efficiently executing airdrops.","date":"2024-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"parachain","permalink":"/blog/tags/parachain"},{"inline":true,"label":"airdrop","permalink":"/blog/tags/airdrop"}],"readingTime":5.045,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Disposable Parachains","date":"2024-01-20T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["parachain","airdrop"]},"unlisted":false,"prevItem":{"title":"The Role of the Polkadot Treasury","permalink":"/blog/2024/07/08/the-role-of-the-polkadot-treasury"},"nextItem":{"title":"9 Ideas for the Decentralized Future of Polkadot","permalink":"/blog/2023/11/20/9-ideas-for-decentralized-future"}},"content":"##### This is a [repost from the Polkadot forum](https://forum.polkadot.network/t/disposable-parachains-for-airdrops-and-other-ideas/5769), discussing the possibility of short lived parachains on the Polkadot Network for efficiently executing airdrops.\\n\\nI had some unorganized thoughts I wanted to share, especially with all the talk about $DED, and what an efficient airdrop might look like on Polkadot.\\n\\nI want to start by saying that I truly believe most airdrops are just ponzinomics. I don\u2019t necessarily support or like the airdrop culture or think it is healthy for an ecosystem. That being said, it is an interesting technical problem.\\n\\nFor context, doing an Airdrop on Polkadot is inherently harder than other chains. This is because, unlike many other blockchains, Polkadot takes care to ensure that its state properly scales. It does this through things like the existential deposit and other storage deposits, which is also often the blocker for people to make large scale airdrops work.\\n\\nI want to make clear, this is by design, and it seems it is working well.\\n\\nBut let\u2019s brainstorm, given these constraints, how you could make the largest and most efficient airdrop possible on Polkadot.\\n\\nMy current answer is: Disposable Parachains.\\n\\nA disposable parachain is a parachain with a short lifetime.\\n\\nThis means that after some relatively short period, the entire parachain, its state, and everything else can be completely dropped.\\n\\nThis is quite the opposite in thinking to most blockchains which are expected to live forever. If we look back to why Polkadot has these limitations on storage, it is precisely because we are constantly concerned with the long term scalability of the chain.\\n\\nBut a disposable parachain does not have this problem. A disposable parachain can be extremely \u201cinefficient\u201d, for example:\\n\\n- having no existential deposit\\n- having lots of items in storage (like the airdrop information for a 1M accounts)\\n- fixed fees / weights, to simplify development\\n- etc\u2026\\n\\nSo what does this look like in the context of an airdrop?\\n\\nLet\u2019s imagine we wanted to create an airdrop for DOT2, which is exactly 1:1 for DOT to all existing DOT holders. At the time of writing, this needs to support 1.2M users.\\n\\nThe Assets Parachain / Assets Pallet is just not set up for this kind of task. Each user who wants to receive the airdrop on the Assets Parachain needs to have an existential deposit on that parachain, and perhaps other deposits needed to hold the storage of that asset. For the person who is executing the airdrop, this is a massive up-front cost that would prevent the airdrop all together.\\n\\nThe more scalable solution would be to create some kind of merkle trie which contains all of the claims for tokens into a single merkle root, and then individual claimants could provide proof of their claims and mint the tokens themselves paying their own fees.\\n\\nIn this case, everyone has the \u201copportunity\u201d to claim their DOT2, but not everyone will do it. Some users have such a little amount, it is not worth it for them to go through the process. Some users are not active, so they won\u2019t claim. Some users don\u2019t see value in the token, so they won\u2019t claim.\\n\\nIMO, this is all good. All the different ways users can filter themselves from an airdrop that they don\'t find valuable is better for the whole system and bloat. Having a \u201creactive\u201d airdrop is much better than a \u201cproactive\u201d one, and is much easier to scale. Most airdrops do this through a claims process.\\n\\nBut now we would need to create and launch some custom Airdrop pallet on the Assets parachain. More than that, we will need to keep track of all claims from users for the lifetime of the airdrop to prevent someone from double claiming. In the case where 500K people claim the airdrop, we would still need to manage those 500k storage entries, which will bloat the chain and the chain history, especially if there will be multiple airdrops in the future.\\n\\nThis is where the disposable parachain comes in.\\n\\nPolkadot is already designed to shard data and logic to parachains. The entire state of a parachain is boiled down to a single root hash, and that root hash is stored on chain. That means we could have a parachain, handling billions of airdrops, and the impact onto Polkadot is the same as any other parachain.\\n\\nSo imagine, for DOT2, we launch a disposable parachain, which holds all the data about the airdrop for DOT holders. We can store all the data we need on this parachain to make the airdrop work. Users could even pre-trade the token on the parachain, allowing small holders to sell their bags to those who are actually interested in claiming the token.\\n\\nThe parachain would be set up so that it is the owner of an asset on the Assets Parachain. Users would be able to trigger a claim on the Airdrop Disposable Parachain, which will send an XCM to mint the appropriate asset on the Assets Parachain. All fees required to do this need to be covered by the claimant, and can be included in the XCM message. Users who are interested in claiming their token go through this process, and ultimately get their final tokens minted on the Assets Parachain where the tokens are now officially in the Polkadot ecosystem, and should be well supported.\\n\\nAfter some period of time, let\u2019s say 6 months, the disposable parachain will \u201ckill itself\u201d, or just let the community decide how long they want to keep it around. But the idea is that once the parachain is no longer needed, it can just be \u201cforgotten\u201d and all of the state and bloat can disappear with it. The only impact it leaves on Polkadot is its state root, which can also be cleaned up if needed.\\n\\nIn this case, we offload the decision making of the airdrop to the users, and do so in a way which leaves no historical debt to Polkadot or any long term parachain.\\n\\nFollow the conversation further in the [Polkadot Forum](https://forum.polkadot.network/t/disposable-parachains-for-airdrops-and-other-ideas/5769)."},{"id":"/2023/11/20/9-ideas-for-decentralized-future","metadata":{"permalink":"/blog/2023/11/20/9-ideas-for-decentralized-future","source":"@site/blog/2023-11-20-9-ideas-for-decentralized-future.md","title":"9 Ideas for the Decentralized Future of Polkadot","description":"This is a repost from the Polkadot forum, responding to the announcement of the Web3 Foundation\'s Decentralized Futures Program.","date":"2023-11-20T00:00:00.000Z","tags":[{"inline":true,"label":"web3foundation","permalink":"/blog/tags/web-3-foundation"},{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"}],"readingTime":22.545,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"9 Ideas for the Decentralized Future of Polkadot","date":"2023-11-20T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["web3foundation","polkadot"]},"unlisted":false,"prevItem":{"title":"Disposable Parachains","permalink":"/blog/2024/01/20/disposable-parachains"},"nextItem":{"title":"Tips for New FRAME Developers","permalink":"/blog/2023/08/09/tips-for-new-frame-devs"}},"content":"##### This is a [repost from the Polkadot forum](https://forum.polkadot.network/t/9-ideas-for-the-decentralized-future-of-polkadot/4731), responding to the announcement of the Web3 Foundation\'s Decentralized Futures Program.\\n\\nThe Web3 Foundation just announced their [Decentralized Futures](https://futures.web3.foundation/) program for the Polkadot Network.\\n\\nIn their site, they describe 9 areas which are of key importance for these funds and for development to occur in order to support and grow the Polkadot ecosystem.\\n\\n![Decentralized Future Program](/assets/images/decentralized-futures-program.png)\\n\\nI wanted to present an idea for each area that I would love to see funded by this initiative.\\n\\nIndeed, I would like to directly contribute to one or more of these ideas if I can find the right people to work along side with.\\n\\nObviously each of these ideas needs more time spent to flesh out the specifics, but hopefully these ideas are both concise enough to allow people to read them all, but detailed enough for people to understand the underlying philosophy and goal behind the project.\\n\\nFinally, I apologize, but many of these ideas have been brewing for a while (as you will see below, I have linked to some previous writing when applicable), but this post was written fast and off the top of my head. Hopefully feedback can allow me to edit and refine the ideas further.\\n\\nFind a chatgpt summary of this post here: https://forum.polkadot.network/t/9-ideas-for-the-future-of-polkadot/4731/2?u=shawntabrizi\\n\\n## Technology\\n\\nIn the technology section, the Web3 Foundation identified the following areas: Developer Experience, Growth, and Interoperability. Here is an idea for each of these areas.\\n\\n### Developer Experience: An Alternative to FRAME\\n\\nSubstrate has always positioned itself as being flexible to the needs of the developer. From its inception, they have always described at least 3 ways to approach the SDK:\\n\\n![Substrate Dev Time vs Complexity](/assets/images/substrate-dev-time-vs-complexity.png)\\n\\nIt is obvious to someone who is an expert in FRAME how flexible and modular it is, and how it really can be used to develop fully customized systems.\\n\\nHowever, to many just entering the ecosystem, FRAME itself can be quite complex to use, and especially to use correctly and safely.\\n\\nI would argue that of the 3 options presented in Substrate, nearly no one builds by modifying core, and nearly no one builds by simply configuring the genesis of the node template. Entirely everyone builds using FRAME, but then the goal of flexibility in Substrate is lost due to the fact that there is only a single product to satisfy the needs of everyone.\\n\\nWith this in mind, I think there should exist a product between building with the Node Template and FRAME. A framework which is more simple, stupid-easy, safe, standard, satisfying, and so-on....\\n\\nPerhaps a framework like: \\"Here is Substrate\'s Super Simple Safe ... STF SDK\\", aka \\"Hissss...\\" \ud83d\udc0d (not really important what the name is, but i have had this one in mind for a while)\\n\\nThe core ideas:\\n\\n- Less configurable... but works out of the box.\\n    - You do not get to customize every single type in your runtime... but then you also don\'t need to worry about or eat the costs of having every type be generic in your runtime too.\\n- Providing all the primitives you expect, exported as a single system.\\n    - For example not feeling that you have a System Module, Balances Module, Utility Modules, XCM Modules, Consensus Modules, etc which all need to work with what you are building. It is all packaged with standards already accepted in the wider blockchain ecosystem, and you build against that.\\n- Targeted toward simplifying specific ecosystem objectives, like cross chain transfers, defi, multi-token systems, smart contracts, etc...\\n    - Lets build the framework not to maximize flexibility, but to be the best at the goals which the current crypto ecosystem are focused on.\\n- Compatibility with the chains using FRAME.\\n- Ideally, well developed user stories allowing people to transition from FRAME to Hiss and Hiss to FRAME.\\n\\nIn summary, FRAME was the correct decision for frameworks to provide to the Polkadot ecosystem. It is designed to be maximally flexible. But now that it exists, we should start to look at lowering the barrier of entry for developers who want a more simple product, just like what was promised with developing with just the Node Template.\\n\\nI think it is time for Polkadot to start getting alternative runtime SDKs.\\n\\n### Growth: FRAME + ink! (The Merge)\\n\\nWhat if you only needed to learn a single language to join any part of the Polkadot ecosystem? What if you wanted to build a smart contract, an on-demand parachain, or a dedicated parachain, that you could take the exact same codebase, and use it to launch all three when the time and conditions are right?\\n\\n![Pathway for a developer](/assets/images/the-merge.jpeg)\\n\\nThat is the main vision behind a merge of FRAME and ink!. My belief is that runtime development is a complete superset of smart contract development. For historical reasons, both of these frameworks were built separately, but also found their way to converge over time. The shift from FRAME v1 to FRAME v2 was actually due to ink! and how they masterfully used attribute macros to build a much better Rust eDSL. However, simple syntax (like declaring calls, storage, events, errors) which could be standardized across the two languages are not, and as such, users feel that they need to learn two languages, not one.\\n\\nThe steps between tinkering with Polkadot and launching a parachain is just too massive at the moment. There is really no middle ground or story for a single individual to launch something like a parachain without a large team and a lot of funds.\\n\\nInstead, we can market the first step toward building a parachain is building a contract with ink! and deploying it to an existing parachain. Then, since in our world, ink! and runtime development use the same language, successful products can simply turn their contract into a more performant on-demand parachain. Finally, if their on-demand parachain is demanding so much blockspace, they can obtain a dedicated slot, potentially with all the same codebase.\\n\\nOf course, if you are developing code for the runtime, you get access to more APIs and more low level access to your blockchain, but the main idea should be that through your development process in the Polkadot ecosystem, you are only writing new code, not rewriting existing code.\\n\\nIn this story, users have a really comprehensive story from tinkering as an individual, to launching parachains on Polkadot, exactly the kind of growth we need in our ecosystem.\\n\\nMost of the ideas around the merge I had were initially documented here: [The Merge of ink! and FRAME](./2022-08-11-the-merge-of-ink-and-frame.md).\\n\\nPerhaps the goal would not be to merge FRAME into ink!, but maybe Hiss (suggested above), could be that extension of ink! to provide this end to end story.\\n\\n### Interoperability: Reading State Across Parachains\\n\\nAmong the 9 ideas in this write up, this is the one with the least amount of concrete thought. However, the basic idea is this:\\n\\nA huge downside of multi-chain interoperability is coordinating the state of chains as a part of constructing cross-chain messages. With monolithic chains like ethereum it is easy and completely synchronous to inspect the state of any contract or application in the system.\\n\\nIn Polkadot, XCM is already an extremely expensive protocol relative to basic runtime functions, but it almost entirely removes state reading and reporting from the protocol because it is a bad practice.\\n\\nAs of today, I do not believe there exists a set of tools which standardizes how two parachains who want to communicate to each other with XCM should synchronize their state and trigger their logic based on changes from the other chain.\\n\\nIt seems some kind of offchain worker with light client proofs could be used to solve this problem, but likely this is not a trivial tool to create, and it certainly should be one which is standardized across our ecosystem.\\n\\nI think such tools are the catalyst needed for us to create oracle chains, which report up to date prices of other cryptocurrencies, which is a fundamental primitive for many defi tools. Hopefully in the future, these tools are also used to bring data from robust identity chains to the rest of our ecosystem.\\n\\nAs much as it is important to have protocols which allow us to describe moving an asset from one chain to another, I think for us to unlock the full potential of interoperability in Polkadot, we need to start also thinking about how we can let chains focus on generating high quality data, and making it dead simple and cheap for other chains to access that data when they need it.\\n\\n## Development Ecosystem\\n\\nIn the Development Ecosystem section, the Web3 Foundation identified the following areas: Development Services, High Value Partners, and Enterprise. Here is an idea for each of these areas.\\n\\n### Development Services: Interactive and Meaningful Tutorials\\n\\nAt the ground level, we need to continue to develop new ways to teach others to build with the Polkadot SDK, and actually have them use that information to launch meaningful products.\\n\\nWhat do we want to see more of in our ecosystem?\\n\\n- More defi products\\n- More cross chain interactions\\n- NFTs\\n- Games\\n\\nOne of the best ways to achieve this is to have tutorials which show the creation of _working_ systems, which build these kind of products from zero to one hundred, with room to customize where needed.\\n\\nThe Polkadot developer community has always been good at producing high quality code, but not so good at educating others to be high quality coders. For this, we need to throw money and incentives at the problem.\\n\\n- create and maintain a number of high quality tutorials\\n    - these should be written by the experts, not having people who themselves are learning trying to teach others. I often think we are plauged with blind leading the blind.\\n    - target developers who need \\"a break\\" from coding. As a developer, I can speak to the fact that after large projects, having the ability to transition to create tutorials or other documentation is a nice escape from development.\\n    - treat this kind of work and maintenance as important as writing core code. That includes rewarding high quality authors and maintainers similar to core contributors\\n    - create frameworks and tools which make tutorial creation minimal effort for core knowledge holders\\n        - Use real rust projects, git, basic markdown, VS Code editor, and other familiar tools as the basis for creating and expressing tutorials.\\n        - On-hand writers to help shape \\"raw\\" content into expressive stories.\\n        - On-hand graphic/web artists to turn tutorials into something visually appealing and unique in story.\\n- each tutorial should result in a product ready working products that we would like to see exist in our ecosystem\\n    - There is no value in a tutorial which teaches users to install the \\"name-pallet\\" into their chain. Or a tutorial which has users use unbounded vectors, inefficient storage structures, or other anti-practices. Tutorials should result in something useful and to be used.\\n    - A simple practice to achieve this could be to specifically write tutorials on creating the pallets found in Polkadot itself. However, this is also probably not the right formula.\\n- tutorials should capture the whole scope of learning, from substrate specific rust, to pallet development, to parachain / solo-chain deployment, to upgrades, migrations, and other long term maintenance.\\n- tutorials should be gamified and fun for the author. We should look to create full end to end experiences that users can pick up and put down on their free time, and on-chain rewards like NFTs when users complete some tutorial.\\n\\nSome of these efforts are already in progress in different forms, but in my opinion the vision, incentives, and people working on this have never quite been the right mix for success. With the decentralized futures program and the right people, perhaps this can change.\\n\\nI am working on some of this stuff already as a part of my efforts in the academy, with the web3 foundation education team, and personal education initiatives I am doing locally to my home. Hopefully we will see a revival of substrate kitties soon.\\n\\n\\n### High Value Partners: Community Auditors and Audits\\n\\nAudits are key to the survival and trust developed for ecosystems. However, for many development teams, audits are very expensive, and for many potential audit teams, they do not have access to the information or resources needed to be experts in our ecosystem, and actually provide proper audits.\\n\\nFor this, we need to combine two different initiatives into a single story:\\n\\n- An education path for successful audit teams outside of Polkadot to become expert Polkadot auditors.\\n- A path toward free/subsidized audits from these teams for our ecosystem.\\n\\nThis starts with funding and education. We need to draw in audit teams by paying them to complete an audit in our ecosystem, but targeted also as a learning experience for them. Perhaps in this situation, they will act always as a secondary audit for a primary auditor. These audit teams would be guaranteed some funding for their time (paid for by treasury / decentralization funding), and they would see a large queue of teams who are looking for audits for their work, meaning much more work in the future.\\n\\nThere should be a process for teams to submit a request for audit, and some community / technical feedback allowing prioritization and subsidization of those requests. Also some tracking to see how much support teams have received in total, and the history and knowledge of the audit teams.\\n\\nIn many ways, this reminds me of existing systems in our world like court assigned lawyers for defendants that cannot afford legal representation, or government contract requests which are placed in public, and allowing different parties to bid on fulfilling those requests.\\n\\nIn the end, any level of respectable auditing will have long term impact in our ecosystem. We can draw in and even create more high valued partners by ensuring our products are not as likely to be attacked or have erroneous bugs.\\n\\n### Enterprise: Competitive Research\\n\\nIn the spirit of attracting existing successful Web2 companies into the Polkadot ecosystem, we need to make it obvious to them that Polkadot is the correct choice among competitors and peers.\\n\\nThe truth is that many of the most successful blockchain ecosystems got that way off of narrative manipulation and marketing, and not actually working or philisophically aligned technology.\\n\\nBeyond that, other teams have found a product market fit which seems to still elude the Polkadot ecosystem. Such problems are cyclic. Without users you don\'t draw in new products. Without new products, you don\'t draw in users.\\n\\nOf course improving our own product is absolutely the best first step to remedy this problem, and plenty of that is already happening in Polkadot. But what I suggest here is different in that we should also be more on the offensive and call out fact from fiction.\\n\\nA lot of this proposal has already been written down here: [Polkadot Competitive Research Proposal](./2023-04-05-polkadot-competitive-research.md)\\n\\nI think there are two key outcomes which come from competitive research:\\n\\n- calling out narratives and stories which are false in other ecosystems, many of which were pioneered by Polkadot\\n    - Does a chain claim to achieve 1 Million TPS? What does that even mean? How would this actually compare to other protocols, and not some hand crafted benchmark.\\n    - \\"Instant finality\\" sounds cool, but is it really?\\n    - Other chains claim to have \\"forkless upgrades\\" or \\"shared security\\", is that actually true?\\n    - Modular blockchains are the future, but where in the stack is modularity a benefit, and where is it actually a poor design decision?\\n- learning from our competitors to better shape our product\\n    - Polkadot as a culture seems very deep into our own research and ideas. Many of the most successful teams have used ideas and standards from other protocols to accelerate the development of their products.\\n    - What products can we create which directly compete with what enterprises and users find exciting in other ecosystems.\\n    - how can we create more directly comparible products, and prove that our technology stack is actually better. For example, growing Polkadot SDK into a platform for developing the best L2s, in competition with arbitrum and optimism, as well as parachains.\\n\\nUnfortunately, existing parties have not seemed as interested in directly funding this work, however the decentralized futures program may be the way to get such an effort going.\\n\\n## Community\\n\\nIn the Community section, the Web3 Foundation identified the following areas: On-Chain Governance, Decentralized Marketing, Events. Here is an idea for each of these areas.\\n\\n### On-Chain Governance: Treasury Proposal Templates\\n\\nOpen gov has been an interesting experiment so far. From my observations, the blockchain level APIs and mechanisms which exist are pretty good, but the community is WRONG in trying to interface directly with it.\\n\\nEven the existing UIs today like Polkassembly and Subsquare seem to be just wrappers on top of low level blockchain APIs, and not themselves an app which is expected to be used first and foremost by casual builders and new faces in our ecosystem.\\n\\nA lot of this idea is already written down in this post: [A Better Treasury System](./2022-09-04-a-better-treasury-system.md)\\n\\nBut at a high level, I imagine that we use our social standards to push forward behaviors beyond the rules of the blockchain.\\n\\nAs an initial MVP, I would expect that we try to standardize the following:\\n\\n- An online form that users can fill out easily to describe the funding they want. Not a word doc to fill out, but actual web based UX, that can modify itself and help users through the process.\\n    - These could even be different depending on the track.\\n- Publishing to the public FIRST, before posting anything on chain. The discussion of proposals should happen far before anything shows up on chain. Is it possible to design systems where the outcome of proposals are known before they are posted on chain? Or as close to this as possible.\\n- Gamification of treasury spending. No one wants grifters or others to take advantage of the treasury funds. No one goes to an investor and asks for millions of dollars without any evidence of previous work done successfully. We should guide people through a tier system, where their first proposal is small in scope, and small in cost. Perhaps even just paying users to make a more formal roadmap for large ideas. Then, as users successfully complete proposals, their ability to ask for more increases, up to the point where they are getting large funds and large projects that we expect to support well established teams.\\n- From gamification, we will also be able to establish off-chain reputation sysytems. We should have a hub which is centered around individual contributors and teams. When someone asks for funds, it should be easy to see what things they have delivered in the past, and how well those products have been received by the ecosystem. You would never buy a product online from a seller with no reputation. Why would you give a stranger money to build something with no background information?\\n- The proposal process does not end when the vote passes. There should be entire hubs and discussion areas for each proposal. Imagine forums where users can query the builders on progress, provide feedback, and even hold builders accountable for the funds they received. This is all in the hopes of finding teams which don\'t just market and sell their idea well, but also execute well and show effort to be integrated in our community.\\n\\nOf course all of these things are not rules established on chain. Anyone could make a proposal directly on chain asking for any amount of funds. But we as a community can direct users to our socially accepted process, and encourage users to show their good intentions by having them grow in our community, not just ask for millions of dollars from out of nowhere.\\n\\nIn this context, I think there is nothing wrong with having more \\"centralized\\" thinking and processes around the treasury, knowing that this often provides more efficient processes to finding high quality teams and output. Most importantly, it does not compromise the decentralized nature of the underlying protocol.\\n\\n### Decentralized Marketing: Modular Marketing Materials\\n\\nPeople who are reading this post are probably in the top 1% of the top 1% of people following the Polkadot ecosystem. So let me ask you:\\n\\n> If you had to make a presentation to describe and sell Polkadot to the world, how would you get started? What resources would you use?\\n\\nAs someone who has given countless presentations, I feel that no such resources exist, and in fact I am making most of my content from scratch. From the presentations I create, I see many people in the ecosystem asking for links to the presentation, and I have seen my images, words, and even ideas show up in twitter threads, blog posts, other presentations, and more.\\n\\nWhen thinking about decentralized marketing, this is the kind of things we need. Not create bigger megaphones to blast out our ideas, but create reusable scripts that can be spread by smaller voices, and customized to the needs of the communities being reached out to.\\n\\nWith this in mind, I envision a marketing oriented resource website, providing content in multiple forms about many different key topics in our space.\\n\\nFor example, imagine that each topic had the following formats available:\\n\\n- One sentence summary.\\n- One paragraph summary.\\n- One essay summary.\\n- One (or more) image(s).\\n- One powerpoint slide.\\n- One Twitter post.\\n- One Twitter thread.\\n- One (or more) competitive comparisons.\\n- etc...\\n\\nThen imagine we had this for all the key topics and differentiators in the Polkadot ecosystem, now and in the future:\\n\\n- Open Gov\\n- Data Availability\\n- Execution Sharding\\n- Treasury\\n- XCM\\n- FRAME\\n- Polkadot SDK\\n- Forkless Upgrades\\n- etc...\\n\\nThese words could be copied, could be mutated, or could just be read by others. But the resources themselves are high quality and coming from and reviewed by the experts in the field. Providing ammunition to everyone who has access to the internet and a belief in what we are building.\\n\\nBeyond that, I imagine that we provide many additional resources to make content development in Polkadot easier.\\n\\n- Access to graphics and themes which can be used throughout the ecosystem:\\n    - multiple visuals of the DOT token\\n    - multiple versions of the polkadot logo\\n    - multiple themes (for example, most recently the spacemen theme)\\n    - generators for custom parachain / xcm visuals\\n    - the best image representations of slogans and philosphies of Polkadot\\n- More competitive research, and specific talking points to counter false narratives.\\n- Reusable, short video clips with subtitles from various presentations, meetups, interviews.\\n- Photos of our vibrant ecosystem, people, companies, meetings, development, workshops, etc...\\n- And so much more.\\n\\nAll content would be completely open source, zero copyright, and free for the public to use as they see fit.\\n\\nI think we if had to be critical of marketing efforts in the space, it was that we had people who did not have the right ammunition to effectively spread the winning messages in this space. If we want to truly adopt decentralized marketing, we need to give everyone a powerful voice with easy access to persuasive data and messaging.\\n\\n### Events: Familiar Faces Fund\\n\\nWhen I think about the future of decentralized events in Polkadot, I worry that we won\'t properly curate the voices which are used to represent our mission and direction. It seems that large events like Web3 Summit, Polkadot Decoded, and Sub0 are some of the only places where our public gets an insight into what we are building and where we are going.\\n\\nAs Polkadot becomes more decentralized, it will not be as easy for individual catalysts and leaders to be able to travel and represent their work. Having the right people at our events and shaping the narrative around Polkadot is just as important as having the right developers architecting our codebase.\\n\\nHaving seen how these kinds of large events are prepared behind the scenes, I know that we make the agenda based on those who request to present, not by actually reaching out and curating a story that we want to tell. I don\'t think this is the right way to get the best representation for Polkadot.\\n\\nWe have to remember many of our best speakers and representatives are already over their head in work. Without such a support program, these speakers would need to coordinate and pay for all this travel at their own cost. This usually only makes financial sense for leaders who want to shill their product, and this does not lead to the kind of content that is best for the whole ecosystem. For those looking to the treasury to support this, the overhead is currently massive, and the politics at the time are a large deterrent to open that can of worms.\\n\\nWe can achieve the desired outcome using a team and set of funds support key speakers. We need to figure out with them how we can make these events more accessible to them, and the best use of their time.\\n\\nSimple things like managing hotel reservations, flight tickets, and other basic administration, to more complex things like shaping a narrative of what we want the public to see and hear from Polkadot, and finding the right people to present those parts of the story.\\n\\nAnd we should think even beyond just the Polkadot ecosystem, ensuring that we have proper representation at large multi-ecosystem events like EthCC, EthDenver, Devcon, Devconnect, etc...\\n\\nIf we do our job well in this context, we can create famous and recognizable faces outside of just Parity and the original Polkadot Founders who represent our mission and contributions to this ecosystem, and whom other look to for direction and inspiration.\\n\\nSo really, I would like to see:\\n\\n- changing the direction from selecting representation from among those who ask to present, to reaching out to key presenters and asking them to present at key events\\n    - ultimately making things more proactive from the events side, and pushing work onto event coordinators and off of the presenters where possible\\n- supporting those presenters to be at those events: funds and basic administrative help for their travel\\n- collaborative narration: taking advantage of these key speakers, and forming ahead of time a set of presentations which capture the key takeaways we want the audience to know about Polkadot.\\n    - We should never have two presenters talk about the same topic.\\n    - We should never have two presenters contradict the messaging between their talks.\\n\\nFor something like this to exist, we need funding and a small group of individuals who can help shape this and make it happen.\\n\\nContinue the conversation in the [Polkadot Forum](https://forum.polkadot.network/t/9-ideas-for-the-decentralized-future-of-polkadot/4731)."},{"id":"/2023/08/09/tips-for-new-frame-devs","metadata":{"permalink":"/blog/2023/08/09/tips-for-new-frame-devs","source":"@site/blog/2023-08-09-tips-for-new-frame-devs.md","title":"Tips for New FRAME Developers","description":"At the Polkadot Blockchain Academy, we both teach students how to build a custom blockchain using the Polkadot SDK, Substrate, and FRAME, and have them complete a project using that knowledge.","date":"2023-08-09T00:00:00.000Z","tags":[{"inline":true,"label":"frame","permalink":"/blog/tags/frame"},{"inline":true,"label":"pba","permalink":"/blog/tags/pba"}],"readingTime":12.75,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Tips for New FRAME Developers","date":"2023-08-09T00:00:00.000Z","authors":"shawntabrizi","categories":["Substrate"],"tags":["frame","pba"]},"unlisted":false,"prevItem":{"title":"9 Ideas for the Decentralized Future of Polkadot","permalink":"/blog/2023/11/20/9-ideas-for-decentralized-future"},"nextItem":{"title":"Does Shared Security Improve Interoperability?","permalink":"/blog/2023/05/18/shared-security-and-interoperability"}},"content":"At the Polkadot Blockchain Academy, we both teach students how to build a custom blockchain using the Polkadot SDK, Substrate, and FRAME, and have them complete a project using that knowledge.\\n\\nThis post is a retrospective for that assignment meant for the students to continue to learn and improve their FRAME skills after the academy.\\n\\nSharing it here, since I believe it could help anyone getting started with FRAME development.\\n\\n## FRAME Assignment Retrospective\\n\\nCongratulations everyone for completing the academy.\\n\\nI truly understand that this is one of the most challenging 5-week courses on the planet. I hope that everyone walks away from it with a sense of pride in what they\'ve learned and how they\'ve grown.\\n\\nI wanted to spend a little bit of time recapping some of the things I saw in the FRAME Assignment which can help round out your learnings and allow you to continue to grow as a future Substrate developer.\\n\\n## Documentation\\n\\nI think students lost the most number of \\"easy\\" points by not valuing documenting their code or spending extra time on their README.\\n\\nWhen I was grading your projects, I was very much relying on documentation to express **intention** and **design**. Students who made a mistake, but documented their intention accurately on average got more points than students who made a mistake and wrote nothing.\\n\\nMany students took the opportunity to design unique and interesting pallets but did not adequately document expected behaviors, how functions work, or the assumptions made for the state machine.\\n\\nI reviewed your projects as if I was doing a code review for a peer in the ecosystem. If you had public functions, pallet configurations, public types, etc... I expected them to be documented. Even internal logic within your extrinsics, it is helpful to describe what your intended logic will be in words, and then to see the code doing that, else how can I differentiate design from  potential error? I understand time is limited, but these kinds of docs can be written as you are writing the code!\\n\\nFor the README, students on average didn\'t really seem to \\"own\\" their project. Students who got full points made up stories on how their pallet was going to be used. Described in detail the mechanics of their system. Really spent time to think about how these \\"simple projects\\" could be enhanced to real production ready systems.\\n\\nThese kinds of READMEs allowed me to understand your comprehension of not just coding FRAME, but actually tying together the things you learned earlier in Cryptography, Game Theory, Economics, and Blockchain.\\n\\n## Rust-isms\\n\\nFortunately, Rust doesn\'t really let you make outright mistakes. It seemed like everyone got their code compiling which is a good sign that everyone is over the hump. However, many students put a lot of non-ergonomic Rust code into their Pallets, which can vary from very wrong to \\"not as pretty\\".\\n\\n### Dealing with Results and Options\\n\\nMost everyone could easily up their game by becoming more familiar with and practicing rust-ic ways to handle Results and Options.\\n\\nTry to be fluent with:\\n\\n- `ok_or`\\n- `map_err(|e| ...)`\\n- `?`\\n- `if let Some(var) = an_option { ... }`\\n- etc...\\n\\n\\n### Never Panic\\n\\nWe said it many times in class, yet it still showed up in some projects. Some people used `.unwrap()` where there were no guarantee that it would be safe, and that is the worst case situation.\\n\\nHowever, other students wrote code which contained `.unwrap()` or `.expect()` where it was checked to be safe, but still, this is not the best practice! Rust has all of the syntax needed so that **you never need to unwrap or expect ever!**.\\n\\nFor example, I saw many times:\\n\\n```rust\\nlet my_value = MyValue::<T>::get();\\nensure!(my_value.is_some(), Error);\\nlet value = my_value.expect(\\"we checked above qed\\");\\n```\\n\\nI want to be clear, this is _safe_, but would never pass a code review!\\n\\nYou saw in class many times the more ergonomic and Rust-ic way to handle this:\\n\\n```rust\\nlet value = MyValue::<T>::get().ok_or(Error)?;\\n```\\n\\nThat\'s it!\\n\\nYou should physically fight yourself to ever include an `unwrap` or `expect` inside of your Rust code. There is likely ALWAYS a better way to handle it, and especially since FRAME now can return an error at any time, you always should.\\n\\n### Helper Functions\\n\\nStudents with the cleanest code often took advantage of many helper functions which had very clean inputs, outputs, and assumptions. I saw many students write the same blocks of code over and over, and ideally that should tickle your brain to turn this into a helper function!\\n\\nThis is doubly true for the Pallet Tests which seemed like a prime place to create simple helpers to setup your pallet into certain states so you can quickly execute tests.\\n\\nI promise that this kind of thinking will actually save you time, as you refactor, improve, or fix bugs in your code. It is better to have shared logic which can be edited, rather than having to hunt down every copy and paste of a certain piece of logic.\\n\\n### Object Oriented Design\\n\\nA lot of the better Pallets took advantage of well defined objects and apis to make their systems more consistent.\\n\\nSome ideas for each of the projects:\\n\\n- DEX: create a struct which manages asset pairs, with their balances, and always sorts them. Have your internal functions input that single object, and perform logic over that, rather than dealing with 4 uncoupled variables.\\n- Voting: create a struct which represents the proposal, and apis which allow updating the ayes / nays, getting the result, getting the end block etc...\\n- DPoS: create a struct which represents a delegator, and apis to manage their frozen balance, who they are delegating to, and so on...\\n\\nAvoid tuples where possible! It is almost always better to deal with structs with well defined fields. Imagine reading this code:\\n\\n```rust\\nif (user_1.0 < user_2.0) { ...}\\n```\\n\\nvs\\n\\n```rust\\nif (user_1.balance < user_2.balance) { ... }\\n```\\n\\nAgain, my recommendation is to just quickly jump into creating structs and other objects, as they are usually very easy to update and maintain. Wait until you need to change a tuple, and see how long it takes to refactor that :)\\n\\n## FRAME-isms\\n\\nFRAME is a huge library with pretty crazy APIs. There was no expectation that anyone would walk away from the academy being able to be a FRAME expert, so here are some things which you can take away to continue your development.\\n\\n### Common Unsafe Math\\n\\nStudents still got caught doing unsafe math in their Runtime. While `checked_add` and `checked_sub` doesn\'t necessarily lead to pretty code, it is nearly free and you should always use it.\\n\\nI saw a few times the pattern of:\\n\\n```rust\\nfn create_proposal(origin, proposal_length: BlockNumber, ...) {\\n    let current_block = frame_system::Pallet::<T>::block_number();\\n    let end_block = current_block + proposal_length;\\n    // ...\\n}\\n```\\n\\nObviously this is ripe for any user to input a huge proposal length, and cause an overflow in your addition. You must be EXTRA careful when you are doing math or any kind of operation with data being inputted by the user. Remember that you must treat every extrinsic as an attack on your system, and code like the above would be bad.\\n\\nOn the flip side, some students were using `saturating` math, which is better than unsafe math, but probably not what you want to use in general. When you use saturating math, you prefer executing code and completing successfully rather than bubbling up error conditions. I think this is normally not what you should prioritize. Instead, just fail fast.\\n\\nThankfully, Substrate harnesses the power of on-chain governance and runtime upgrades. If your pallet were to get stuck because of some checked math, it is always something that can be repaired later. It is better than having some silent error / saturation occur and then trying to repair it later when the damage is already done.\\n\\n### Using FRAME Types\\n\\nI saw a lot of students struggle to deal with manipulating the various FRAME types. Most commonly the Balance and BlockNumber type. As you have learned, these types are not concrete in your Pallet, and so you need to figure out ways to work with them.\\n\\nThe most common thing you **should** do is just use the same types together! For example, if you want to calculate some math using the current block number, you should use the `BlockNumber` type for all your variables! The same is true for all the FRAME types!\\n\\nThis means configuring your config with the right type:\\n\\n```rust\\ntype ProposalLengthLimit: Get<BlockNumberFor<T>>;\\n```\\n\\nUsing the right type for input parameters:\\n\\n```rust\\n// if you want to use vote_weight^2 = balance\\n// then vote_weight should be the balance type\\nfn my_function(origin, vote_weight: BalanceOf<T>) {\\n    let balance_to_freeze = vote_weight.checked_mul(vote_weight)?;\\n    // ...\\n}\\n```\\n\\nAnd the right types for storage:\\n```rust\\n// using the same example as above\\nstruct UserVote<T: Config> {\\n    weight: BalanceOf<T>,\\n    aye: bool,\\n}\\n```\\n\\netc...\\n\\nA lot of students were using `u32` concretely in their runtime, and then constantly converting it to the FRAME type. The compiler will allow this, but a code review will not. Become comfortable with referencing numbers and other FRAME types as their generic self.\\n\\n### Origin Checks\\n\\nFor some reason, a lot of students were using `ensure_signed_or_root` in their code. I don\'t remember teaching this explicitly, but somehow it seemed to get passed around. This origin check is perfectly fine to use, and many of you handled it correctly for functions where you wanted to allow root direct access to do something, or a user to do something for themselves. A common use case was de-registration, where Root could deregister any user they want, and a user could choose to deregister themselves.\\n\\nHowever, this origin check also showed up in places where the signed origin WAS NOT CHECKED AT ALL! This is super bad! People had mint functions or other low level calls where they mistakenly allowed any user to call that function and potentially mess up their blockchain.\\n\\nIf you want to use this origin check, it should look something like this:\\n\\n```rust\\nif let Some(who) = ensure_signed_or_root(origin)? {\\n    // handle the case where signed origin is here\\n    // you have access to `who`\\n} else {\\n    // handle case where root called\\n}\\n```\\n\\nHere we can see that `?` will return an error if the origin is not `Signed` or `Root`. Otherwise, the function will return `Some(account)` if it is `Signed` and `None` if it is `Root`.\\n\\nI saw others handle this a lot less ergonomically, when it can be this simple.\\n\\n### Pallet Design Patterns\\n\\nArguably, all of these design patterns are opinions that intelligent and well-informed developers can choose to follow or not. If you do not directly agree with some of these comments, I will respect your choice to have your own opinions, but I think it is worth considering these as \\"best practices\\" in the Substrate ecosystem.\\n\\n#### Try to avoid loops\\n\\nWhere possible, try to avoid loops in your runtime, as this kind of code is harder to benchmark, and may actually make your blockchian less efficient.\\n\\nImagine the scenario where a user wants to unlock their funds across multiple expired votes. The initial urge is to create a function called `fn unlock_all_votes(who)`, which inside would check:\\n- each vote the user had\\n- if that proposal was expired / completed\\n- free up their balance\\n\\nInstead it is better to have users unlock votes one by one, and use things like `Utility.batch` to unlock multiple things at once.\\n\\n#### Fail Early and Fail Fast\\n\\nThankfully, everyone seemed to be good at checking their inputs and state before executing any extrinsic. The checks are super important to keep your state machine functioning as expected. However, order of the checks matter too!\\n\\nFor example, I saw a lot of:\\n\\n```rust\\nensure!(SomeStorage::<T>::contains_key(), Error);\\nensure!(param.len() < T::MaxLenInput::get(), Error);\\n// you should flip these two ensure statements for better efficiency!\\n```\\n\\nIn this case, you are always reading storage to do some check first, whereas you could simply check the length of some input which is MUCH CHEAPER, and would allow you to return an error fast.\\n\\n#### Do not try to compensate for the user / state\\n\\nIt is not the primary goal of your runtime code to be \\"user friendly\\".  Your primary goal is to create safe, fast, and efficient code for your blockchain network.\\n\\nMany users want to make their code super ergonomic for end users.\\n\\n- \\"Did you send me malformed data?\\"\\n    - \\"I\'ll try to fix that for you!\\"\\n- \\"Did you want to de-register a user where they have a lot of state?\\"\\n    - \\"Let me clean that up for you!\\"\\n- \\"Did you want to remove some item in a list?\\n    - \\"Let me find that for you!\\"\\n\\nBut almost all of these are always wrong. Here is how you should be writing your code:\\n\\n- \\"Did you send me malformed data?\\"\\n    - ERROR! Try again with good data please.\\n- \\"Did you want to de-register a user where they have a lot of state?\\"\\n    - ERROR! Please clean up the data related to your account before you call this function!\\n- \\"Did you want to remove some item in a list?\\n    - You MUST provide me with the index or location of that data before you can call this function, and if the location is wrong... ERROR!\\n\\nRemember that users can \\"freely\\" observe and analyze the data on your chain. They can freely calculate hashes, or any complex logic. When they do so, they do it only the one time locally. When your runtime does these kinds of calculations or database searches, it does it for every node on your network! Your computation time is already extremely limited, there is no reason you should be doing extra work on behalf of your users!\\n\\n### Test Coverage\\n\\nOverall, tests were very strong across the whole academy. If you are in doubt if you have written the right tests, do the following:\\n\\n- write a test which does the greenfield scenario for each extrinsic\\n- write a test which tests each extrinsic for EACH error that can occur\\n- write a test which involves multiple users interacting with your system at the same time\\n- write a full end to end test which tests the whole thing works as you expect\\n\\nRemember that simply checking your extrinsic passes `assert_ok!` is not enough. Think about an extrinsic which only does:\\n\\n```rust\\n// this will always pass `assert_ok!`\\nfn my_extrinsic(origin) -> DispatchResult {\\n    Ok(())\\n}\\n```\\n\\nYou should always be checking the STATE of your blockchain to ensure that the state machine is running as expected. THis might be the votes in your storage, the balance of users / pools, the frozen balance of your delegators, etc... Just because the code compiles and runs, does not mean it is correct :)\\n\\n## Anything else?\\n\\nWas there any feedback I gave to you that I missed here? Leave a comment and I will write about it!\\n\\nI hope you all have a lovely rest of the summer, and I hope to build the future of Polkadot along side each and every one of you."},{"id":"/2023/05/18/shared-security-and-interoperability","metadata":{"permalink":"/blog/2023/05/18/shared-security-and-interoperability","source":"@site/blog/2023-05-18-shared-security-and-interoperability.md","title":"Does Shared Security Improve Interoperability?","description":"This is a repost from the Polkadot forum, discussing the relationship between shared security and interoperability.","date":"2023-05-18T00:00:00.000Z","tags":[{"inline":true,"label":"shared security","permalink":"/blog/tags/shared-security"},{"inline":true,"label":"interoperability","permalink":"/blog/tags/interoperability"}],"readingTime":2.29,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Does Shared Security Improve Interoperability?","date":"2023-05-18T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["shared security","interoperability"]},"unlisted":false,"prevItem":{"title":"Tips for New FRAME Developers","permalink":"/blog/2023/08/09/tips-for-new-frame-devs"},"nextItem":{"title":"Polkadot Competitive Research Proposal","permalink":"/blog/2023/04/05/polkadot-competitive-research"}},"content":"##### This is a [repost from the Polkadot forum](https://forum.polkadot.network/t/does-shared-security-improve-interoperability/2895), discussing the relationship between shared security and interoperability.\\n\\nI wanted to start a thread which puts into words an advantage of Polkadot which is not covered that often (that I can see). Hopefully I get everything right below, but I am also writing this as an opportunity to bring further clarity to the actual features that exist, allow others to call out any mistakes, allow others to further the story, and ultimately spread the knowledge to our community.\\n\\nSo the topic at hand is how Polkadot\u2019s cross-chain messaging protocol (XCMP) is advantaged over other protocols, for example IBC or any non-protocol-native messaging system, primarily due to the implications of Shared Security across the interacting chains.\\n\\nSo what is the problem normally?\\n\\nAny two blockchains can communicate to each other over a bridge. There are many kinds of bridges, ranging from fully trustless to fully trusted, but in the end of the day, messages are passed over the bridge, and transition the state of these chains.\\n\\n**The issue is that these two interacting chains have completely independent security guarantees.**\\n\\nThe concern is that two chains both change their state due to some cross-chain interaction, but a weak chain is attacked / reorged, and that would leave the two chains in a conflicting state, with no definitive path toward resolution, especially if there are different parties and incentives at play.\\n\\nThis means the security around interactions between two or more chains is as weak as the weakest chain. Compared to Polkadot, where all parachains, and their interactions are secured by the full security of the Polkadot relay chain.\\n\\nThat being said, all chains (even those in Polkadot) are potentially susceptible to some kind of attack where the logic of the chain has a backdoor which manipulates the state of the chain after a cross-chain interaction. This means that even today, there is some level of trust needed between two chains which want to interact with one another, that they will not act maliciously within the sovereignty of their own state transition function.\\n\\nHow would we solve this?\\n\\nAnother seldomly talked about topic which can help solve this is [SPREE](https://wiki.polkadot.network/docs/learn-spree).\\n\\nAt a high level, this is where interactions between two chains is managed by logic which is stored on the relay chain, and also the state is maintained on the relay chain. This would mean that even the two chains interacting with one another would not have control over the specific data and logic around their interaction, but instead follow the rules defined by their common security provider.\\n\\nThis too highlights the need for a shared security layer for a future of truly trustless interoperability.\\n\\nContinue the discussion on the [Polkadot Forum](https://forum.polkadot.network/t/does-shared-security-improve-interoperability/2895);"},{"id":"/2023/04/05/polkadot-competitive-research","metadata":{"permalink":"/blog/2023/04/05/polkadot-competitive-research","source":"@site/blog/2023-04-05-polkadot-competitive-research.md","title":"Polkadot Competitive Research Proposal","description":"Goals","date":"2023-04-05T00:00:00.000Z","tags":[{"inline":true,"label":"research","permalink":"/blog/tags/research"},{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"}],"readingTime":4.2,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Polkadot Competitive Research Proposal","date":"2023-04-05T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["research","polkadot"]},"unlisted":false,"prevItem":{"title":"Does Shared Security Improve Interoperability?","permalink":"/blog/2023/05/18/shared-security-and-interoperability"},"nextItem":{"title":"A Better Treasury System","permalink":"/blog/2022/09/04/a-better-treasury-system"}},"content":"## Goals\\n\\nLook deeply into the technical abilities and developer experiences provided by other Blockchain and Web3 ecosystems, and bring back learnings, features, and facts to the Polkadot ecosystem.\\n\\n## Funding Process\\n\\nListed below is a set of phases and deliverables for competitive research.\\n\\nEach item can be funded and delivered separately, and further phases can be added on as needed.\\n\\nA retainer (or bounty) should be allocated to show intent of funding future work.\\n\\nThen, the expected output can be agreed upon:\\n\\n- Which protocol(s)?\\n- What phases and deliverables?\\n\\nAn estimate will be generated:\\n\\n- How much time (in days) is estimated to complete this work?\\n- What timeline should the work be completed by?\\n- What check-points (if any) can be used to check the progress of the work?\\n\\nIf everyone is satisfied with the work and output, more funding can be allocated and the process can repeat.\\n\\n## Phases\\n\\n### Research\\n\\nLet\'s look to understand what other protocol teams are trying to build and provide to the Web3 ecosystem.\\n\\n#### Work\\n\\n- Determine the key product they are trying to create.\\n    - Where does it fit into the Web3 space?\\n    - What challenges are they trying to tackle?\\n- Identify the best video (or set of) that explains their technical product.\\n- Identify best source of documentation for technical implementation and design decisions.\\n\\n#### Output\\n\\n- Written report of the protocol\\n    - Their product space / mindshare\\n    - Key technical advantages / innovations\\n    - Technical disadvantages / tradeoffs\\n\\n#### Example\\n\\n- What are the different kinds of bridges, and their tradeoffs?\\n    - Compare protocols\\n    - Compare technical design\\n    - Compare strengths / weaknesses\\n    - Compare \\"unsolved problems\\"\\n\\n### Benchmarks\\n\\nLet\'s look at the raw performance of other protocols today.\\n\\n#### Standard TPS\\n\\nUse a base standard of measuring TPS:\\n\\n- Transactions are non-killing transfers from unique existing accounts to new accounts.\\n- Optimistic improvements (like optimistic parallelization) are disabled.\\n- Investigation can look both at hypothetical limits and practical running limits.\\n    - Reusable and accessible hardware where possible.\\n\\n#### Other Potential Benchmarks\\n\\n- Sync Time\\n- Disk space per block\\n- Cross chain messaging benchmarks\\n- Non-standard TPS\\n    - Heavy TX Storage (contract execution)\\n    - Heavy TX Payload (remark)\\n    - Heavy TX Execution (ZK or similar)\\n\\n\\n#### Output\\n\\n- Publicly available benchmark table with all statistics collected and compared.\\n- Reproduction steps written up and included with all benchmarks.\\n\\n### Developer Experience\\n\\nLet\'s look at how well they facilitate the new developer onboarding process.\\n\\n#### Green Path\\n\\n- Identify the best \\"weekend learning path\\" for the protocol. As in, the set of documentation, tutorials, videos, etc which can be accessed over a 3 day period to onboard users into their ecosystem.\\n\\n#### MVP Product\\n\\n- Create an MVP product on their ecosystem, and compare the complexity and customizability of the platform.\\n- Product Ideas:\\n    - Proof of Existence\\n    - Basic Art NFT\\n    - Name Service\\n\\n- Product Extensibility Tests\\n    - Control / Manage Fees\\n    - Upgradability\\n\\n### Feature Development\\n\\nLet\'s identify and bring back high quality features to the Polkadot ecosystem.\\n\\n#### Pallet Development\\n\\n- Where it is identified as helpful, Substrate Runtime Pallets can be developed which encapsulate features of other protocols.\\n- For example:\\n    - Account Abstractions\\n    - NFT / Fungible Token behaviors\\n    - Games\\n    - On-Chain Faucets\\n    - On-Chain Governance\\n    - DAOs\\n    - Staking Systems\\n    - etc...\\n\\n#### Client Optimizations\\n\\n(questionable how well I would be able to do this alone, but certainly could work with people to make this happen)\\n\\n- Where it is identified as helpful, improvements to Substrate Client SDK or the Polkadot Node can be developed which encapsulates the features of other protocols.\\n- For example:\\n    - Storage caching, optimizations\\n    - Syncing speed\\n    - Transaction queue\\n    - etc...\\n\\n#### Protocol Porting\\n\\n- Where it is identified as helpful, other protocols can be ported to usable Rust modules / libraries which can be used in the Substrate ecosystem.\\n- For example:\\n    - Messaging formats / protocols\\n    - Encoding schemes\\n    - Contract Virtual Machines\\n    - etc...\\n\\n### Video / Media Content\\n\\nLet\'s look to promote the information gained from this research to fuel better knowledge of Polkadot\'s advantages in the ecosystem.\\n\\n#### Competitive Battlecard\\n\\n- For each protocol researched, provide a competitive battlecard which allows someone in the Polkadot ecosystem to intelligently counter misinformation and shilling.\\n- Honest criticism of Polkadot where other protocols may be ahead, and suggestions / feedback on how to make up for lost ground.\\n\\n#### YouTube Shorts\\n\\n- Short form video content to get across main ideas and learnings from the competitive research.\\n- Working with other existing media outlets to help them create similar content for their video/audio/media.\\n\\n#### Presentations\\n\\n- Long form (30 min to 1 hour) presentations which specifically highlight Polkadot and other protocols\\n\\n#### Articles / Documentation\\n\\n- Long form written analysis of specific topics and information gained during the research process.\\n- Can be fitted for:\\n    - Technical Blogs\\n    - Marketing Pages\\n    - News Outlets (articles)\\n    - Technical / Developer Documentation"},{"id":"/2022/09/04/a-better-treasury-system","metadata":{"permalink":"/blog/2022/09/04/a-better-treasury-system","source":"@site/blog/2022-09-04-a-better-treasury-system.md","title":"A Better Treasury System","description":"This is a repost from the Polkadot forum, where I propose a vision for a better on-chian treasury for the Polkadot DAO.","date":"2022-09-04T00:00:00.000Z","tags":[{"inline":true,"label":"treasury","permalink":"/blog/tags/treasury"},{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"}],"readingTime":14.735,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"A Better Treasury System","date":"2022-09-04T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["treasury","polkadot"]},"unlisted":false,"prevItem":{"title":"Polkadot Competitive Research Proposal","permalink":"/blog/2023/04/05/polkadot-competitive-research"},"nextItem":{"title":"The Merge of ink! and FRAME","permalink":"/blog/2022/08/11/the-merge-of-ink-and-frame"}},"content":"##### This is a [repost from the Polkadot forum](https://forum.polkadot.network/t/a-better-treasury-system/291), where I propose a vision for a better on-chian treasury for the Polkadot DAO.\\n\\nThis post will dump various ideas of mine on how we can improve the treasury system of Polkadot, both on and off chain.\\n\\nAs a Council member on Kusama and Polkadot since the genesis election, I have reviewed, accepted, and even denied many treasury spends, and throughout my experience, I have formed opinions on what I like, and what I think can be improved.\\n\\n## Why Care\\n\\nI will be very brief here, but I think it is worthwhile to convince those of you reading this post why a focus on Polkadot\'s treasury system is important.\\n\\nUltimately, the Polkadot treasury is the one way that the Polkadot network can reach its hand out into the real world, and make things happen. The treasury is quite a unique feature of Polkadot compared to the last generation of chains.\\n\\nIf we can make sure that the treasury is easy to access for the Polkadot community, it will ensure that we will be able to fund and support growth and development of our ecosystem.\\n\\n### Treasury Today\\n\\nThe Polkadot treasury can be simplified to two main parts:\\n\\n - The Pot - The funds available to the treasury, funded by a portion of the fees on the network.\\n - Spending Logic - Ways for the treasury to transfer funds from the pot to end users.\\n\\nThe treasury is currently has 3 spending methods:\\n\\n* Tips - Easy process to spend small amounts of treasury funds. Low overhead, relatively hard to abuse.\\n* Proposals - A more laborious process to spend large amounts of treasury funds immediately and to a specific account.\\n* Bounties - The most complex treasury process, which involves multiple actors to coordinate the spending of funds for projects which are agreed upon in advance, but the recipient of the funds may not be known at the time of creation. (this is maybe a bad summary, but yeah, you can [read more about it](https://wiki.polkadot.network/docs/learn-treasury#bounties-spending).)\\n\\n## Improving Proposals (Treasury V2)\\n\\nProposals were designed to be very simple, but I think that some additional complexity can greatly improve the end to end experience of creating and approving large treasury spends.\\n\\n### Proposal Phases\\n\\nCurrently, if a team opens a proposal for a treasury spend, if accepted, all of those funds are transferred to the team immediately. This is true for small projects, but also large, multi-year projects.\\n\\nI have been concerned in the past that unknown teams come to the treasury and ask for a lot of money, for a huge project, and we have no idea how to evaluate if the team will actually deliver that project. We may ask the team to reduce the size of their ask, but then a team may be worried that the treasury will not commit to continuing the project once they get started.\\n\\nUltimately the proposal system right now is not actually good for allocating funds for really large projects.\\n\\nI suggest we introduce some simple phases to each treasury proposal, and have teams ask to allocate funds to each specific phase:\\n\\n1. Starting Spend\\n2. Reoccurring Spend\\n3. Final Spend\\n\\nFor the sake of example, I will take the role of a team lead who wants to ask for 100,000 DOT to build a new mobile wallet for Polkadot, and then use this role in the sections below.\\n\\n#### Starting Spend\\n\\nWhen a team asks for treasury funds, usually they will need some cash up front to get started with the project. This is what the **Starting Spend** can be used for.\\n\\nProposals today are basically ALL starting spend. Without having done any work, I would have asked for 100,000 DOT up front, and basically it would have been very hard to get my proposal approved.\\n\\nIn this case, I know I only need 10,000 DOT to get started on this project, and so I only ask for that amount as my Starting spend.\\n\\nThe treasury has less risk at loosing funds to people that do not deliver, and the proposer can still get the up-front capital they need to begin execution of their project.\\n\\n#### Reoccurring Spend\\n\\nBuilding a large project may take multiple months or even years. The reoccurring spend is the amount of funds the proposer will need on some reoccurring time scale to keep the project going.\\n\\nFor my example, I expect the project to take around 10 months, and I expect my monthly costs to amount to 5,000 DOT.\\n\\nIn this case, this reoccurring spend logic will be baked into the proposal, and if approved, every month, I can pull from the treasury funds my 5,000 DOT.\\n\\nIn order to get my next months funds, I must also submit a proof of the work done in the last month. This proof does not need to (and cannot really) be verified on chain. Instead, the team puts the data out there, and any user in the Polkadot ecosystem can read the update, and check for themselves the work being done is high quality and worthy of the monthly payments. As you can probably guess, later down we will describe a process where the updates are NOT high quality, and the governance system of Polakdot can end a proposal early, returning any unspent funds back to the treasury.\\n\\nBut generally, this process should be minimal and efficient for the proposing teams. Just provide proof that you have been doing what you said you would, and you can automatically pull out the funds you need to keep building month to month.\\n\\n#### Final Spend\\n\\nFinally, at the end of the project, the development team can potentially get a reward for their hard work done. It could be that the funds requested above were just enough to keep them afloat, but we should reward good contributors to our ecosystem with a profit for their work and hard time.\\n\\nIn this case, I have spent a total of 60,000 DOT for the 10 months of work and the starting spend, and thus my final reward will be 40,000 DOT for a job well done.\\n\\nThis will be a slightly delayed payment, where the proposer can again submit evidence that their work was completed successfully, and the public is given a period of time to review that work, and verify that the initial proposal was indeed satisfied by the work done.\\n\\nIf no one objects to the final spend, the my imaginary team walks away with 40,000 DOT profit on a job well done.\\n\\n#### How Phases Effect Proposals\\n\\nNow lets look at other ways this proposal could have gone, when a team asks for DOT.\\n\\n1. Only Ask for Starting Spend\\n\\n   This is exactly how we treat proposals today, and should be backwards compatible if there are UI/UX already developed to optimize this.\\n\\n1. Only Ask for Reoccurring Spend\\n\\n   In this scenario, I do not ask for any starting spend or final spend reward to execute my proposal. Instead, I simply ask that over the course of 10 months, I get a 10,000 DOT payment each month for the work done.\\n\\n   This might make sense for projects where the treasury is already comfortable with the delivery of a team, and does not feel the need to keep the \\"DOT profits\\" from the team until the final spend.\\n\\n   This looks very similar to \\"child bounties\\" which exist today on Polkadot.\\n\\n1. Only Starting Spend and Ending Spend\\n\\n   This might make sense for projects which are short in their timeline, thus the overhead of doing regular updates for the reoccurring spend would rather just be moved to the final spend.\\n\\n\\n1. Only Ending Spend\\n\\n   If implemented cleverly, this can look very similar to the regular bounties system which exists today for the treasury, and potentially could replace it.\\n\\n1. Etc...\\n\\n\\nAs you can see, based on the amount of money, the kind of proposal, the trust we have in the proposer, etc... these phases can be tuned to spend the same amount of DOT, but over a different schedule, which allows the public to audit and keep track of the work done.\\n\\nAdditionally, we could potentially simplify UI/UX by combining the behaviors of bounties and proposals under a single unified process.\\n\\n#### Other Ideas for Phases\\n\\nThere are a million other features and nice to haves which can be added to a phase based treasury spend, and will probably need to be in the final implementation:\\n\\n- Allowing post-approval adjustments of things like reoccurring spend and final spend amount.\\n- Giving only a fraction of the Final Spend if the final product is \\"okay\\", but not great.\\n- Ways to extend the reoccurring spend time period\\n- Ways to pause reoccurring spend, to allow for closer public audit\\n- Ways to update where the spends go\\n- Users placing bonds against asking for funds or stopping a spend from happening\\n- etc...\\n\\n## Improving Funds\\n\\nIn the example above, we talked about a 10 month project, which is asking for 100,000 DOT. But practically speaking, most of us still live in a world where we need fiat to live and pay people for work.\\n\\nIt could be in a bull market, the 100,000 DOT over 10 months could grow a lot in USD value. However, in bear markets, it could be that your estimated monthly payments are not actually enough to keep the project going.\\n\\nOne way or another, I feel the treasury needs to bring some sense of a stable coin to the system.\\n\\nWe could:\\n\\n* Have the Polkadot treasury hold and distribute a stable coin.\\n* Record information about the expected DOT value on chain when a proposal is proposed and passed.\\n* Use ideas like \\"gilt\\".\\n* other ideas?\\n\\n## Reputation\\n\\nThis is more of an offchain recommendation, but I do not find that any UIs today do a good job at representing the history of users who ask for funding from the treasury.\\n\\nSimilarly I don\'t think we do a good job at making recommendations for new applicants on the size of proposals that the treasury would be comfortable to give out on a first try.\\n\\nI don\'t have any specific ideas here, just rough ideas.\\n\\nFor example, imagine the following table:\\n\\n| # of Proposals Successfully Delivered | $ of Funds for Next Proposal |\\n|-----|------------|\\n|  0  | $10,000    |\\n|  1  | $50,000    |\\n|  2  | $100,000   |\\n|  3  | $500,000   |\\n|  4  | $1,000,000 |\\n| ... | ...        |\\n\\nI would when users as for a proposal, I should not need to hunt for their history to see if they have delivered on time, that they are asking for an amount appropriate to what they have delivered in the past, etc... It should be a part of the process that this information is presented to everyone, that users will want to build up their reputation, and that they know what is reasonable to get their proposal approved.\\n\\n## Follow Up + Impact\\n\\nThis is one of the problems that I think will be solved mostly by Proposal Phases and Reputation, but I want to call it out here as a weak point of the current treasury system.\\n\\nI have reviewed and approved many different proposals on both the Polkadot and Kusama, however, I find it very hard to track or follow the specific impact of those treasury spends.\\n\\nI think there is a lot of meta things we can do to improve this kind of stuff.\\n\\nFor example:\\n\\n- Add a logo at the top/bottom of applications and websites with things like \\"Funded by the Polkadot Treasury\\"\\n- At certain funding amounts, expect / require teams to create videos highlighting their work.\\n    - Tie those videos on-chain or off-chain to the proposals themselves, and the reputation of the proposers.\\n- Require teams to describe expected impact on the ecosystem, and measure those goals.\\n- etc...\\n\\n## Decentralizing Data\\n\\nAt the moment, there is no metadata about treasury spends on-chain. If you want to learn what a proposal, bounty, or tip is trying to do, you need to visit a third party website like Polkassembly to get that data. (tips have a small amount of data, but not that good)\\n\\nI think we should look to add decentralized forms of the treasury spend metadata on-chain. So, adding some new fields to store and update some IPFS hash, which then links to the actual proposal metadata. This would allow anyone to build their own version of a treasury application, and allow us to iterate much more quickly on providing high quality user experiences.\\n\\nIdeally, we could do the same thing here with the conversation around treasury spends too, but I am not sure what exactly that would look like.\\n\\n## Metrics\\n\\nI think the general consensus is that the treasury does not spend enough of its funds, but I have still yet to find a place which really paints a clear picture of what is happening with the treasury funds at a high level:\\n\\n- Which team(s) has been allocated the most overall funds?\\n- Which teams have had the most proposals approved?\\n- What was the most expensive proposal approved?\\n- What is the ratio of approved to denied proposals?\\n- What percentage of the treasury is spent between spending periods?\\n- What categories have had the most treasury spending?\\n    - Wallets\\n    - Defi\\n    - Identity\\n    - Privacy\\n    - Block Explorers\\n    - RPC Nodes\\n    - etc...\\n- How much is being burnt from the treasury, and what would that look like at different spending amounts?\\n\\nOnce we know at a high level what is happening with the treasury, we can start to give direction and planning to it too. While I am sure that many people are very excited about funding defi projects, my guess is these kinds of proposals are over-represented compared to privacy and identity projects, which is probably a much more compelling use case of Polkadot.\\n\\n## UI / UX Ideas\\n\\nI want to write down more concrete ideas for UI/UX which I think can be worked on in the near future and have positive impact into the involvement of users into our democracy system.\\n\\nI am not a UX guy, so here are some very rough mocks:\\n\\n![Tinder Gov 1](/assets/images/tinder-gov-1.jpeg)\\n\\n![Tinder Gov 2](/assets/images/tinder-gov-2.jpeg)\\n\\nThe goal is to make a **proactive** and **simple to use** application for people to vote with.\\n\\n- Users can be notified on their mobile / desktop when new proposals are available to vote on.\\n- Users are presented with a card containing all the relevant information users need to make their decision.\\n- Voting can be as easy as swiping left or right to the card.\\n- Users can go through all proposals and make their decisions.\\n- At the end, a summary page is shown to the user, and a single batch transaction is submitted for all of the votes.\\n\\nI think something like this could greatly increase voter participation. We would probably need custom cards for different kinds of proposals like:\\n\\n- Runtime Upgrades\\n- Treasury Spends (tips, proposals, etc...)\\n- Fellowship / Society\\n- Configuration changes (staking, parachains, etc...)\\n- General extrinsics which do not match a category above\\n\\nBeyond this, I think we need to create a \\"dashboard\\" for governance similar to the successful [Staking Dashboard](https://staking.polkadot.network/#/overview).\\n\\nThe main features I would want to see in the dashboard is individual / group profiles, and a simple to use proposal creation form. It is possible we would want to build these functionalities on top of an existing platform like [Polkassembly](https://polkadot.polkassembly.io/).\\n\\nFor profiles, we will need a history of treasury spends in the past so that we can do the reputation and follow up / impact ideas I originally posted.\\n\\n- There should be a process for organizations to easily create a multisig with the individual users\\n- Organization profiles should list the users of that multisig\\n- All profiles should have a list of previous proposals they submitted, whether they passed, and any details on the impact of those proposals.\\n- Any other details which allows organizations and individuals to build up a **reputation** which can help sway the vote of people one way or another.\\n\\nFinally, the current method for people to create proposals is to fill out a word document, but I think this process can be made much more streamlined and easy with a web form to fill out.\\n\\n![Create Proposal](/assets/images/create-proposal.png)\\n\\nCompare that to this: https://docs.google.com/document/d/1-b_DQXHVyRuAoYubCtL9dk0R6a_ev7xbwYMV518ONpo/edit\\n\\nIn this case, the UI can do a few things:\\n\\n- Give users a template expected for filling out treasury proposals.\\n- Detect organizations, and make suggestions based on that.\\n    - For example, if the beneficiary has no identity, we can suggest that the user creates an on-chain identity for the account.\\n    - If the beneficiary has little or no reputation, we can put a warning if the user is asking for too mach funds. Otherwise, we can suggest to the user an upper limit based on their history and reputation.\\n- We can submit the proposal into a public review draft, which is handled off-chain, and allows people to get fast feedback before officially making the submission on-chain.\\n- The entire proposal can be turned into a well defined format, and uploaded to the chain using the preimage pallet and on-chain metadata. See https://github.com/paritytech/substrate/pull/12568\\n\\nAs always, these are just ideas, probably a lot has been built already, and some people with better design tastes than me can make these things a reality.\\n\\nContinue the discussion on the [Polkadot Forum](https://forum.polkadot.network/t/a-better-treasury-system/291);"},{"id":"/2022/08/11/the-merge-of-ink-and-frame","metadata":{"permalink":"/blog/2022/08/11/the-merge-of-ink-and-frame","source":"@site/blog/2022-08-11-the-merge-of-ink-and-frame.md","title":"The Merge of ink! and FRAME","description":"In this post, I propose a better developer experience and pathway for developers in the Polkadot ecosystem by merging the development language used for ink! smart contracts and FRAME pallets.","date":"2022-08-11T00:00:00.000Z","tags":[{"inline":true,"label":"frame","permalink":"/blog/tags/frame"},{"inline":true,"label":"ink","permalink":"/blog/tags/ink"},{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"},{"inline":true,"label":"devex","permalink":"/blog/tags/devex"},{"inline":true,"label":"smart contract","permalink":"/blog/tags/smart-contract"}],"readingTime":6.02,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"The Merge of ink! and FRAME","date":"2022-08-11T00:00:00.000Z","authors":"shawntabrizi","categories":["Polkadot"],"tags":["frame","ink","polkadot","devex","smart contract"]},"unlisted":false,"prevItem":{"title":"A Better Treasury System","permalink":"/blog/2022/09/04/a-better-treasury-system"},"nextItem":{"title":"Web3 Reading List","permalink":"/blog/personal/web3-reading-list/"}},"content":"##### In this post, I propose a better developer experience and pathway for developers in the Polkadot ecosystem by merging the development language used for ink! smart contracts and FRAME pallets.\\n\\n![Pathway for a developer](/assets/images/the-merge.jpeg)\\n\\nink! and FRAME are two Rust based languages developed by Parity for developing state transition functions. The first focuses on smart contracts, and the second focuses on the blockchain runtime.\\n\\nBoth of these languages heavily rely on Rust macros which form an eDSL for developing these decentralized applications.\\n\\n### Vision\\n\\nIn short, I believe that Wasm Smart Contract and Substrate Runtime development could use a single shared language, and we could continue to brand that language as ink!.\\n\\nink! will be a general language for developing decentralized applications in the Substrate / Polkadot ecosystem.\\n\\n### Why\\n\\nHere are some of the reasons why a merge of ink! and FRAME would be valuable to us and our ecosystem.\\n\\n#### Reduced Onboarding Costs\\n\\nParity is working on developing two distinct developer communities at the moment: ink! Smart Contract Developers and FRAME Runtime Developers.\\n\\nPractically speaking these two communities do not share much besides working within the Substrate / Polkadot ecosystem, and building decentralized applications in Rust.\\n\\nThere is a great potential opportunity to combine these ecosystems, and have them work together on a single shared goal.\\n\\nImagine you go to a conference, and rather than there being multiple different workshops on ink! and FRAME, there is instead a single set of workshops on a single language with different levels of difficulty, ranging from contract development, to basic runtime development, to advance runtime development.\\n\\nDocumentation, videos, and everything else related to education become more simple, and from this, we will reduce the costs of onboarding new developers into our ecosystem.\\n\\n#### Raw Speed Improvements\\n\\nOne of the original sells of Polkadot is to provide a scalability solution to contract developers that feel limited by a single contract chain.\\n\\nIt is likely that ink! contract developers will face similar scaling issues on their layer 1 chains as other parachains.\\n\\nWe know that by simply running code within the Substrate runtime environment, we can get \\"off the chart\\" performance increases.\\n\\n![Pathway for a developer](/assets/images/execution-time.png)\\n\\n> In the graph above, smaller is better.\\n\\n\\n#### Parachain Lifecycle\\n\\nAnother problem we can solve here making development on Polkadot more approachable.\\n\\nCurrently, a developer in the Polkadot ecosystem basically needs to believe that their idea is good enough to build a whole team, win a parachain slot, launch, and maintain.\\n\\nThere is basically no room here for simply experimenting with the technology. Allowing users to write a contract, knowing that they can one day use that same code to then launch a parathread or parachain is a great sell for people to feel that even small investments in our ecosystem are worthwhile.\\n\\nThe story becomes:\\n\\n* Do you have an idea?\\n    * Make a contract! Try it out! Fast, easy, and nearly zero overhead for you.\\n* Did you validate your idea with a contract?\\n    * Upgrade to a parathread! Get the blazing speeds of working in the runtime, with little to no additional coding required! We can even help you migrate your contract data to a parathread.\\n* Is your parathread going viral?\\n    * Upgrade it to a parachain! Guarantee your cost of business over the next 2 years.\\n\\n\\n### How\\n\\nI claim that every ink! Smart Contract is a valid Substrate Runtime. That is, the set of applications you can develop with ink! is a strict subset of the set of applications you can build with FRAME.\\n\\nAt a high level, both languages break down application development into these components:\\n\\n* Storage\\n* Calls / Messages\\n* Events\\n* Errors\\n* Origin / Caller\\n* etc...\\n\\n#### ink! Example\\n\\n```rust\\n#![cfg_attr(not(feature = \\"std\\"), no_std)]\\n\\nuse ink_lang as ink;\\n\\n#[ink::contract]\\npub mod flipper {\\n    #[ink(storage)]\\n    pub struct Flipper {\\n        value: bool,\\n    }\\n\\n    impl Flipper {\\n        /// Creates a new flipper smart contract initialized with the given value.\\n        #[ink(constructor)]\\n        pub fn new(init_value: bool) -> Self {\\n            Self { value: init_value }\\n        }\\n\\n        /// Creates a new flipper smart contract initialized to `false`.\\n        #[ink(constructor)]\\n        pub fn default() -> Self {\\n            Self::new(Default::default())\\n        }\\n\\n        /// Flips the current value of the Flipper\'s boolean.\\n        #[ink(message)]\\n        pub fn flip(&mut self) {\\n            self.value = !self.value;\\n        }\\n\\n        /// Returns the current value of the Flipper\'s boolean.\\n        #[ink(message)]\\n        pub fn get(&self) -> bool {\\n            self.value\\n        }\\n    }\\n\\n    #[cfg(test)]\\n    mod tests {\\n        use super::*;\\n        use ink_lang as ink;\\n\\n        #[ink::test]\\n        fn default_works() {\\n            let flipper = Flipper::default();\\n            assert!(!flipper.get());\\n        }\\n\\n        #[ink::test]\\n        fn it_works() {\\n            let mut flipper = Flipper::new(false);\\n            assert!(!flipper.get());\\n            flipper.flip();\\n            assert!(flipper.get());\\n        }\\n    }\\n}\\n```\\n\\n#### FRAME Example\\n\\n```rust\\n#![cfg_attr(not(feature = \\"std\\"), no_std)]\\n\\npub use pallet::*;\\n\\n#[cfg(test)]\\nmod mock;\\n\\n#[frame_support::pallet]\\npub mod pallet {\\n\\tuse frame_support::pallet_prelude::*;\\n\\tuse frame_system::pallet_prelude::*;\\n\\n\\t#[pallet::pallet]\\n\\t#[pallet::generate_store(pub(super) trait Store)]\\n\\tpub struct Pallet<T>(_);\\n\\n\\t#[pallet::config]\\n\\tpub trait Config: frame_system::Config {}\\n\\n\\t#[pallet::storage]\\n\\tpub type Flipper<T> = StorageValue<_, bool, ValueQuery>;\\n\\n\\t#[pallet::genesis_config]\\n\\tpub struct GenesisConfig {\\n\\t\\tpub start_value: bool,\\n\\t}\\n\\n\\t#[cfg(feature = \\"std\\")]\\n\\timpl Default for GenesisConfig {\\n\\t\\tfn default() -> Self {\\n\\t\\t\\tSelf { start_value: false }\\n\\t\\t}\\n\\t}\\n\\n\\t#[pallet::genesis_build]\\n\\timpl<T: Config> GenesisBuild<T> for GenesisConfig {\\n\\t\\tfn build(&self) {\\n\\t\\t\\tFlipper::<T>::set(self.start_value)\\n\\t\\t}\\n\\t}\\n\\n\\t#[pallet::call]\\n\\timpl<T: Config> Pallet<T> {\\n\\t\\t#[pallet::weight(10_000 + T::DbWeight::get().writes(1))]\\n\\t\\tpub fn flip(_origin: OriginFor<T>) -> DispatchResult {\\n\\t\\t\\tFlipper::<T>::mutate(|value| *value = !*value);\\n\\t\\t\\tOk(())\\n\\t\\t}\\n\\t}\\n}\\n\\n#[cfg(test)]\\nuse crate::mock::{new_test_ext, Origin, TemplateModule, Test as T};\\n\\n#[test]\\nfn default_works() {\\n\\tnew_test_ext().execute_with(|| {\\n\\t\\tassert!(!Flipper::<T>::get());\\n\\t});\\n}\\n\\n#[test]\\nfn it_works() {\\n\\tnew_test_ext().execute_with(|| {\\n\\t\\tassert!(!Flipper::<T>::get());\\n\\t\\tassert!(TemplateModule::flip(Origin::signed(1)).is_ok());\\n\\t\\tassert!(Flipper::<T>::get());\\n\\t});\\n}\\n```\\n\\nThese components are simplified and exposed to the user in an expected syntax. This eDSL is then parsed by the macro, and then regular Rust code is generated in the background, which calls low level apis that access the underlying VM environment.\\n\\nMy (naive) belief is that we can manipulate these macros to basically fork the underlying generated code whether\\n\\n```\\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n                                  \u2502               \u2502\\n                                  \u2502   ink! dApp   \u2502\\n                                  \u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\\n                                  \u2502   Storage     \u2502\\n                                  \u2502   Calls       \u2502\\n                                  \u2502   Events      \u2502\\n                                  \u2502   Errors      \u2502\\n                                  \u2502   etc...      \u2502\\n                                  \u2502               \u2502\\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n                                          \u2502\\n                                          \u2502\\n                                          \u2502\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n\u2502 Target Contracts API \u2502                  \u25bc                   \u2502   Target FRAME API   \u2502\\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502                                      \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\\n\u2502                      \u2502 \u25c4\u2500\u2500 ink::contract or ink::pallet \u2500\u2500\u25ba \u2502 sp-io w/ hashed keys \u2502\\n\u2502                      \u2502                                      \u2502 dispatchable trait   \u2502\\n\u2502                      \u2502                                      \u2502 outer event enum     \u2502\\n\u2502                      \u2502                                      \u2502 dispatchable error   \u2502\\n\u2502                      \u2502                                      \u2502                      \u2502\\n\u2502                      \u2502                                      \u2502                      \u2502\\n\u2502                      \u2502                                      \u2502                      \u2502\\n\u2502                      \u2502                                      \u2502                      \u2502\\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n\\n```\\n\\nIn the case where someone built an ink! smart contract, without any changes (maybe just benchmarks), we should be able to compile it to a Substrate Runtime.\\n\\n\\n### Concerns\\n\\n- Is it possible?\\n- Is it maintainable?\\n- Contracts are immutable by nature, where pallets are not\\n- Polkadot does not treat contracts as a first class entity today, should the narrative change?\\n\\n### Known Problems\\n\\n- weights / metering\\n- hooks\\n- runtime config\\n- coupling of pallets / contracts\\n- low level api calls (specific to runtime or contract)\\n- The `<T>` runtime\\n- Potentially multi-ink things cannot easily talk to each other (contract and pallet have a hard time to talk to each other)\\n- Release strategy (upgrades are harder in ink!)\\n- No errors with data\\n- panics!"},{"id":"/personal/web3-reading-list/","metadata":{"permalink":"/blog/personal/web3-reading-list/","source":"@site/blog/2022-07-14-web3-reading-list.md","title":"Web3 Reading List","description":"In this post, I will try to convince you to quit your current job and join the Web3 movement to change the way the world works.","date":"2022-07-14T00:00:00.000Z","tags":[{"inline":true,"label":"books","permalink":"/blog/tags/books"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"}],"readingTime":2.97,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Web3 Reading List","date":"2022-07-14T00:00:00.000Z","authors":"shawntabrizi","slug":"/personal/web3-reading-list/","categories":["Personal"],"tags":["books","web3"]},"unlisted":false,"prevItem":{"title":"The Merge of ink! and FRAME","permalink":"/blog/2022/08/11/the-merge-of-ink-and-frame"},"nextItem":{"title":"Transparent Keys in Substrate","permalink":"/blog/substrate/transparent-keys-in-substrate/"}},"content":"##### In this post, I will try to convince you to quit your current job and join the Web3 movement to change the way the world works.\\n\\nHaving worked professionally in the blockchain space for the last 4 years, it can be important to reflect on the influences that got me here in the first place.\\n\\nI was quite happy with my job at Microsoft, and still think that Seattle is my favorite place where I have lived. (although Puerto Rico comes close...)\\n\\nSo then, what could compel me to quit my job, move to a 30m<sup>2</sup> apartment in Berlin, at half the pay?\\n\\nTo discover that, you will need to read these 5 influential books which convinced me to make the jump into a job in Web3. If these books resonate with you, then maybe it is time for you to take the jump too!\\n\\n## The Lean Startup\\n\\nWritten by [Eric Ries](https://en.wikipedia.org/wiki/Eric_Ries)\\n\\n<img src=\\"/assets/images/the-lean-startup.jpg\\" height=\\"300px\\" />\\n\\nThis is my go-to book on how to effectively and successfully lead a project at a tech company. This was the way my mentor at Microsoft (Praveen Rutnam) ran his team, and this is the way I try to run all of my teams here at Parity.\\n\\n> Reading Time: 6 hours, 299 pages.\\n\\n[Summary](https://contentfiesta.com/book-notes/the-lean-startup-summary/)\\n\\n## Zero to One\\n\\nWritten by [Peter Thiel](https://en.wikipedia.org/wiki/Peter_Thiel)\\n\\n<img src=\\"/assets/images/zero-to-one.jpg\\" height=\\"300px\\" />\\n\\nThis book touches on the challenges of being the first to do something, and how to overcome those challenges and change the world along the way. This book is reassuring in times of doubt, since our goal at Parity is to fundamentally change the world with blockchain. Thiel uses his experiences from the dot com bubble, and I think there are many noticeable parallels happening in our space.\\n\\n> Reading Time: 4 hours, 195 pages in book\\n\\n[Summary](https://howdo.com/book-summaries/zero-to-one-summary-and-review/)\\n\\n## Radical Markets\\n\\nWritten by [Eric A. Posner](https://en.wikipedia.org/wiki/Eric_Posner) and [E. Glen Weyl](https://en.wikipedia.org/wiki/Glen_Weyl)\\n\\n<img src=\\"/assets/images/radical-markets.jpg\\" height=\\"300px\\" />\\n\\nThis book is a bit denser and more academic than the others, but suggests tangible solutions to some of the many social and financial problems that we are trying to solve with blockchain and Web3 technology.\\n\\nIn fact, at one point, they mention that some of their suggestions would only be possible on blockchain technologies.\\n\\nBetter voting systems, ways to control and reduce the costs of land ownership, better immigration programs, disassembling monopolies, etc... If the problems in this book resonate with you, you are definitely headed toward a Web3 future.\\n\\n> Reading Time: 8 hours, 368 pages in book\\n\\n[Summary](https://vitalik.ca/general/2018/04/20/radical_markets.html)\\n\\n## The Starfish and the Spider\\n\\nWritten by Ori Brafman and [Rod Beckstrom](https://en.wikipedia.org/wiki/Rod_Beckstrom)\\n\\n<img src=\\"/assets/images/the-starfish-and-the-spider.jpg\\" height=\\"300px\\" />\\n\\nThis book paints an awesome picture as to the unstoppability of decentralized organizations, and how effective decentralized organizations can form.\\n\\nWhen I think about what Parity wants to be, I see a starfish, and I think that we should learn from other organizations which have done this in the past (as covered in this book).\\n\\n> Reading Time: 5 hours, 240 pages in book\\n\\n[Summary](https://wikileaks.org/gifiles/attach/23/23016_Starfish%20and%20the%20Spider.pdf)\\n\\n## The Internet of Money\\n\\n[Written by Andreas M. Antonopoulos](https://en.wikipedia.org/wiki/Andreas_Antonopoulos)\\n\\n<img src=\\"/assets/images/the-internet-of-money.jpg\\" height=\\"300px\\" />\\n\\nAndreas is famous in the Bitcoin world, and has even published the book [Mastering Ethereum](https://github.com/ethereumbook/ethereumbook) with my boss Gavin Wood.\\n\\nThis book is a collection of talks he has given around the world, describing the problems which Bitcoin and blockchain can solve. This is the book that I send to anyone who asks \\"what is blockchain and why should I care?\\".\\n\\n> Reading Time: 3 hours, 150 pages in book\\n\\nYou can find the book free online [here](https://github.com/erangadbw/IoMv1)."},{"id":"/substrate/transparent-keys-in-substrate/","metadata":{"permalink":"/blog/substrate/transparent-keys-in-substrate/","source":"@site/blog/2020-03-29-transparent-keys-in-substrate.md","title":"Transparent Keys in Substrate","description":"In this post, we will take a look at the new Substrate storage hashers that allow you to transparently extract the keys for any given value in Substrate.","date":"2020-03-29T00:00:00.000Z","tags":[{"inline":true,"label":"storage","permalink":"/blog/tags/storage"},{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"module","permalink":"/blog/tags/module"},{"inline":true,"label":"keys","permalink":"/blog/tags/keys"},{"inline":true,"label":"rpc","permalink":"/blog/tags/rpc"}],"readingTime":12.51,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Transparent Keys in Substrate","date":"2020-03-29T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/transparent-keys-in-substrate/","categories":["Substrate"],"tags":["storage","runtime","module","keys","rpc"]},"unlisted":false,"prevItem":{"title":"Web3 Reading List","permalink":"/blog/personal/web3-reading-list/"},"nextItem":{"title":"Substrate Weights and Fees","permalink":"/blog/substrate/substrate-weight-and-fees/"}},"content":"##### In this post, we will take a look at the new Substrate storage hashers that allow you to transparently extract the keys for any given value in Substrate.\\n\\nThe more I learn about Substrate (and blockchain development in general), the more I come to understand the importance of storage design.\\n\\nIn retrospect, I guess this is obvious since the blockchain is all about coming to consensus about an underlying database, but I cannot overemphasize the fact that once you begin to fully understand the storage layers of Substrate, design decisions across the entire platform start to make more sense.\\n\\nThis blog post will be an extension of a previous post I wrote [Interacting with the Substrate RPC Endpoint](./2019-07-28-interacting-with-the-substrate-rpc-endpoint.md), where we investigated a little about how Substrate storage works, and how you can use basic RPC requests, along with the Substrate metadata, to retrieve information from the chain about the current state of the runtime.\\n\\nThis blog post will dive into the changes introduced to the Substrate storage keys since my last post by this PR: [Refactor away from opaque hashes #5226](https://github.com/paritytech/substrate/pull/5226)\\n\\nUltimately, I hope to show you how a simple design decision about the key format for Substrate storage maps has made the platform infinitely more friendly to external APIs.\\n\\n## Opaque Storage Keys\\n\\nJust do to a quick recap, in [my last post about interacting with Substrate storage via RPC](./2019-07-28-interacting-with-the-substrate-rpc-endpoint.md), we showed an example of how you can get all the balances in your Substrate blockchain using the `getKeys` query and knowledge about the underlying prefix tries used in runtime storage.\\n\\n> This means you could actually use the `state_getKeys` API to get all the storage keys for all the free balances in your system!\\n>\\n> ```bash\\n> curl -H \\"Content-Type: application/json\\" -d \'{\\"id\\":1, \\"jsonrpc\\":\\"2.0\\", \\"method\\": \\"state_getKeys\\", \\"params\\": [\\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4\\"]}\' http://localhost:9933\\n>\\n> {\\"jsonrpc\\":\\"2.0\\",\\"result\\":[\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4024cd62ab7726e039438193d4bbd915427f2d7de85afbcf00bd16fadbcad6aed\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b42e3fb4c297a84c5cebc0e78257d213d0927ccc7596044c6ba013dd05522aacba\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b44724e5390fcf0d08afc9608ff4c45df257266ae599ac7a32baba26155dcf4402\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b454b75224d766c852ac60eb44e1329aec5058574ae8daf703d43bc2fbd9f33d6c\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b465d0de2c1f75d898c078307a00486016783280c8f3407db41dc9547d3e3d651e\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b46b1ab1274bcbe3a4176e17eb2917654904f19b3261911ec3f7a30a473a04dcc8\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b477d14a2289dda9bbb32dd9313db096ef628101ac5bbb3b19301ede2c61915b89\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4927407fbcfe5afa14bcfb44714a843c532f291a9c33612677cb9e0ae5e2bd5de\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b494772f97f5f6b539aac74e798bc395119f39603402d0c85bc9eda5dfc5ae2160\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b49a9304efeee429067b2e8dfbcfd8a22d96f9d996a5d6daa02899b96bd7a667b1\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b49ea52149af6b15f4d523ad4342f63089646e29232a1777737159c7bc84173597\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4a315ee9e56d2f3bb24992a1cff6617b0f7510628a15722b680c42c2be8bb7452\\",\\n>  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4c4a80eb5e32005323fb878ca749473d7e5f40d60ed5e74e887bc125a3659f258\\"],\\"id\\":1}\\n> ```\\n\\nHowever, I also mentioned that this would only allow you to calculate the total balance in your system. It would _not_ necessarily allow you to know the individual balances of all the accounts because those Account IDs are cryptographically hashed and placed into the key. To figure out which account corresponds to each of the keys above, you would need to know the account ahead of time and then verify that the hash matches the bytes at the end of one of these keys.\\n\\nIn general, it is important that we use hashes in the construction of the storage key to [avoid unbalanced tries](./2019-12-09-substrate-storage-deep-dive.md). Imagine instead we use the raw Account ID rather than the hash of it in constructing these storage keys. I could attack a Substrate chain by transferring a very small balance to all accounts whose hexadecimal representation start with `0x69`. This means that any _real_ account that also shares that first byte would be more heavy to access since we will need to traverse past all of the \\"dust\\" accounts in the trie.\\n\\n![](/assets/images/unbalanced-trie.png)\\n\\nYou can see in this illustration, most accounts take only 4 hops to get access to the final value, but some in the middle can take up to 6 hops. This is a very tame example of an \\"unbalanced trie\\", but when actually attacking a system, you can imagine introducing many extra additional hops to access some user accounts.\\n\\nSo we can\'t use the Account ID directly because it is not safe for the chain, and using hashes is completely opaque to the user... Is there maybe some middle ground?\\n\\n## Customizable Storage Keys\\n\\nThis whole blog is about customizable storage keys in Substrate, so before we jump into it, lets talk about what this means.\\n\\nIn Substrate, each Pallet you design can introduce new storage items that will become part of your blockchain\'s state. These storage items can be simple single value items, or more complex storage maps. When you define storage maps in substrate, you must also specify a `hasher` that you want to use:\\n\\n```rust\\nFoo get(fn foo): map hasher(blake2_128_concat) T::AccountId => T::Balance;\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\\n```\\n\\nYou actually have the flexibility in Substrate to change which `hasher` gets used for each different storage map, and as a result, change the way your underlying storage keys are saved in the Substrate database. The `hasher` you select will be a part of blockchain\'s metadata, so any external UI will know what to do in order to correctly access your Substrate storage.\\n\\nBut you might be asking, what do these `hasher`s do?\\n\\n## Concatenating Hashers\\n\\nIn [PR #5226](https://github.com/paritytech/substrate/pull/5226) a new set of \\"hashers\\" were introduced into the Substrate runtime storage system:\\n\\n- `blake2_128_concat`\\n- `twox_64_concat`\\n\\nThese hashers are precisely a middle ground between using an opaque hash and using the raw key value, like an Account ID.\\n\\nWhen we use these hashers, we take a key, find the hash, and then append to the end the hash the raw key itself. For example:\\n\\n**Account ID**\\n\\n```\\n5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY\\n```\\n\\n**Account Bytes in Hex**\\n\\n```\\n0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d\\n```\\n\\n**Blake2 128 of Account Hex**\\n\\n```\\n0xde1e86a9a8c739864cf3cc5ec2bea59f\\n```\\n\\n**Blake2 128 Concat of Account Hex**\\n\\n```\\n0xde1e86a9a8c739864cf3cc5ec2bea59fd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d\\n-------------- hash --------------++++++++++++++++++++++++++ raw key +++++++++++++++++++++++++++++\\n```\\n\\nSo if you have this `blake2_128_concat` formed key, you will directly be able to find the underlying Account ID used to generate the key, you just need to look at the last 32 bytes. But since it is prefixed with a hash, it is also safe to use in the underlying storage trie since it cannot be trivially manipulated to attack the system as described in the previous section.\\n\\nYou can see these concatenating hashers comes in two flavors: `twox_64` and `blake2_128`. The only difference here is the security provided by the underlying hashing algorithm. `twox`/`xxhash` is not a cryptographically secure hashing algorithm, but it is significantly faster to execute. `blake2` is cryptographically secure, but also more resource heavy. So, when should we choose to use each of these?\\n\\nThat ultimately depends on how much control the users have over the value being hashed. Remember, we must always assume that the users of our blockchain are malicious, and build our chain to be resistent to that malicious activity.\\n\\nSo let\'s talk about when it would be appropriate to use each hasher and also show some examples of they they are being used in Substrate today.\\n\\n### Blake2 128 Concat\\n\\n`blake2_128_concat` should be the default choice for any storage key. Because the prefix of the final key uses Blake2, a cryptographically secure hashing algorithm, we need not worry about the content of the starting key. This `hasher` will work for anything.\\n\\nSo where is it used?\\n\\n```rust\\n/// The full account information for a particular account ID.\\npub Account get(fn account):\\n  map hasher(blake2_128_concat) T::AccountId => AccountInfo<T::Index, T::AccountData>;\\n```\\n\\nThis is the `Account` storage item in the FRAME System. Your default substrate configuration will put all user balances in here.\\n\\nSo why must we use `blake2_128_concat` for this storage item?\\n\\nWell, this storage item can be _completely_ controlled by external users. An Account ID (as implemented in a Substrate node) is any valid 32 bytes. When you make a transfer, you can specify any 32 bytes as the recipient of that balance transfer. That account need not have an existing balance or even a private key for someone to access it. So the attack we described earlier in the article where a malicious user could overpopulate parts of the trie still applies. Unfortunately, a non-cryptographic hashing algorithm like `twox` would not really help stop this attack. A malicious user would simply \\"mine\\" for an arbitrary account that generates a `twox` hash starting with some prefix, and perform the same attack. Because the `twox` hasher is not cryptographically secure, this kind of mining attack is not hard to the attacker.\\n\\nSo given that the starting key of this Storage Map is in complete and full control of the users in your network, we must use `blake2`.\\n\\n### TwoX 64 Concat\\n\\n`twox_64_concat` should only be used as an optimization to `blake2_128_concat` in situations where you know that starting key cannot be chosen arbitrarily by your users.\\n\\nLet\'s look at a few ways it is used in Substrate:\\n\\n#### Incrementing Index\\n\\n```rust\\n/// The next free referendum index, aka the number of referenda started so far.\\npub ReferendumCount get(fn referendum_count) build(|_| 0 as ReferendumIndex): ReferendumIndex;\\n\\n/// Information concerning any given referendum.\\npub ReferendumInfoOf get(fn referendum_info):\\n  map hasher(twox_64_concat) ReferendumIndex\\n  => Option<ReferendumInfo<T::BlockNumber, T::Hash, BalanceOf<T>>>;\\n```\\n\\nReferendums are a part of Substrate\'s Democracy Pallet, allowing people to execute on-chain governance which can evolve your Substrate chain. Here you will see that we place information about an ongoing referendum into a map where the key uses `twox_64_concat`.\\n\\nThe starting key in this situation is a `ReferendumIndex`, which is simply a value that will be incremented for each new referendum. I have included the `ReferendumCount` storage item above to show you were the current `ReferendumIndex` is being tracked. Technically speaking, the starting key of a referendum is allowed to be any `ReferendumIndex` value. However, practically speaking, this value is not really in control of the end user. If a malicious user wants to insert some data where `ReferendumIndex = 420`, they will need to open up 419 other referendums, all of which has some underlying economic cost to the user (a deposit, a transaction fee, etc...).\\n\\nAnd that is just to populate one key!\\n\\nSo a user can only manipulate the `ReferendumIndex` by creating more referendums, and ultimately, that is within the safety conditions of what the `twox` hasher can provide. Do note though, that if it was _much_ easier for a user to increment this referendum count, for example by submitting a low cost transaction or a transaction which allows them to increment the index multiple spots at a time, then it is possible that `twox_64_concat` would not be good enough again. You will need to justify the use on a case by base basis.\\n\\n#### Real Accounts\\n\\nLet\'s stay in the Democracy Pallet.\\n\\n```rust\\n/// All votes for a particular voter. We store the balance for the number of votes that we\\n/// have recorded. The second item is the total amount of delegations, that will be added.\\npub VotingOf: map hasher(twox_64_concat) T::AccountId => Voting<BalanceOf<T>, T::AccountId, T::BlockNumber>;\\n```\\n\\nHere we have all of the votes of users in our system for various referendums or proposals. You can see that this storage map is very similar to the `Account` storage item that we used `blake2_128_concat` for, as it maps from Account ID to some value.\\n\\nSo why can we use `twox_64_concat` here and not there?\\n\\nWell the way this storage item is populated is very different than in the case of managing user balances. Only when a user submits a vote onto the blockchain will this storage item be populated. Specifically, this implies the user has the private key to the account in question. If an attacker wanted to put onto the blockchain some data under an arbitrary public key, they would need to generate private keys until one had the corresponding public key they wanted. But at that point, it is as difficult to attack as a regular cryptographic hash.\\n\\n### Identity\\n\\nOne last option you have which I did not mention yet is the `identity` `hasher`. As its name implies, it does not do any hashing at all, and instead directly uses the starting key as the final storage key for the storage item. An example of this in Substrate can also be found in the Democracry Pallet:\\n\\n```rust\\n/// Map of hashes to the proposal preimage, along with who registered it and their deposit.\\n/// The block number is the block at which it was deposited.\\npub Preimages:\\n  map hasher(identity) T::Hash\\n  => Option<PreimageStatus<T::AccountId, BalanceOf<T>, T::BlockNumber>>;\\n```\\n\\nThis should only be used when the starting key is already a cryptographic hash. In the example above, a user can submit a call that will be later executed by a democracy proposal, and we track that call on-chain using the hashing algorithm defined by the runtime, in this case Blake2. So there is no need to append any additional data to the key, we can just use it directly!\\n\\n## Listing All Users\\n\\nOkay, so now that you understand the new hashers introduced to Substrate, lets go through a clear example why this is so great from a UX perspective.\\n\\nIn my old RPC blog post, I was able to list all the user balances, and calculate with it the total balance in my Substrate chain, but I was not able to tell you how much each person has. Well, with our new transparent storage keys, we can do this!\\n\\nI will start a regular Substrate dev node and make a query to return all `System.Accounts`. They will all have a shared prefix of:\\n\\n```\\ntwox_128(\\"System\\")                 + twox_128(\\"Account\\")\\n0x26aa394eea5630e07c48ae0c9558cef7 + 0xb99d880ec681799c0cf30e8886371da9\\n\\n> 0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9\\n```\\n\\nSo then I call the `getKeys` rpc for my dev node:\\n\\n```bash\\ncurl -H \\"Content-Type: application/json\\" -d \'{\\"id\\":1, \\"jsonrpc\\":\\"2.0\\", \\"method\\": \\"state_getKeys\\", \\"params\\": [\\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9\\"]}\' http://localhost:9933\\n```\\n\\nWhich returns:\\n\\n```json\\n{\\n  \\"jsonrpc\\": \\"2.0\\",\\n  \\"result\\": [\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9007cbc1270b5b091758f9c42f5915b3e8ac59e11963af19174d0b94d5d78041c233f55d2e19324665bafdfb62925af2d\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da923a05cabf6d3bde7ca3ef0d11596b5611cbd2d43530a44705ad088af313e18f80b53ef16b36177cd4b77b846f2a5f07c\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da932a5935f6edc617ae178fef9eb1e211fbe5ddb1579b72e84524fc29e78609e3caf42e85aa118ebfe0b0ad404b5bdd25f\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da94f9aea1afa791265fae359272badc1cf8eaf04151687736326c9fea17e25fc5287613693c912909cb226aa4794f26a48\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da95ecffd7b6c0f78751baa9d281e0bfa3a6d6f646c70792f74727372790000000000000000000000000000000000000000\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da96f2e33376834a63c86a195bcf685aebbfe65717dad0447d715f660a0a58411de509b42e6efb8375f562f58a554d5860e\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da98578796c363c105114787203e4d93ca6101191192fc877c24d725b337120fa3edc63d227bbc92705db1e2cb65f56981a\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9b0edae20838083f2cde1c4080db8cf8090b5ab205c6974c9ea841be688864633dc9ca8a357843eeacf2314649965fe22\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9b321d16960ce1d9190b61e2421cc60131e07379407fecc4b89eb7dbd287c2c781cfb1907a96947a3eb18e4f8e7198625\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9de1e86a9a8c739864cf3cc5ec2bea59fd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9e5e802737cce3a54b0bc9e3d3e6be26e306721211d5404bd9da88e0204360a1a9ab8b87c66c1bc2fcdd37f3c2222cc20\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9edeaa42c2163f68084a988529a0e2ec5e659a7a1628cdd93febc04a4e0646ea20e9f5f0ce097d9a05290d4a9e054df4e\\",\\n    \\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da9f3f619a1c2956443880db9cc9a13d058e860f1b1c7227f7c22602f53f15af80747814dffd839719731ee3bba6edc126c\\"\\n  ],\\n  \\"id\\": 1\\n}\\n```\\n\\nNow, I know that all these storage keys use `blake2_128_concat` with the Account ID as the starting key, so I know that there should be an additional 48 bytes added to the end of their shard prefix, and the last 32 bytes should be the raw Account ID information!\\n\\nLet\'s break one down manually:\\n\\n```\\n0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da932a5935f6edc617ae178fef9eb1e211fbe5ddb1579b72e84524fc29e78609e3caf42e85aa118ebfe0b0ad404b5bdd25f\\n---------------------- storage prefix key ------------------------+++++++ blake2 128 hash ++++++++------------------------ account id ----------------------------\\n```\\n\\nTaking a closer look at the last 32 bytes:\\n\\n```\\nAccount ID:      0xbe5ddb1579b72e84524fc29e78609e3caf42e85aa118ebfe0b0ad404b5bdd25f\\nBlake2 128 Hash: 0x32a5935f6edc617ae178fef9eb1e211f\\nAddress:         5GNJqTPyNqANBkUVMN1LPPrxXnFouWXoe2wNSmmEoLctxiZY\\n```\\n\\nAnd this address corresponds to \\"Alice\'s Stash\\" account:\\n\\n![](/assets/images/alice-stash-account.png)\\n\\nNice! So we could follow this same process for every key that is returned under `System.Account` and get a full list of all the accounts in our Substrate dev node!\\n\\n## Final Thoughts\\n\\nAt this point, it should be obvious why this storage key design is really an amazing feature for Substrate and its users. It provides safety, flexibility, and usability to everyone on the platform. I am not sure you can ask for much more.\\n\\nOne thing to note is that Substrate is able to quickly change fundamental design decisions like these storage key patterns due to its modular nature and the minimal number underlying assumptions it makes. If you are a blockchain developer who wants to introduce another storage key system, it really would not be that hard to do it!\\n\\nI have created some tools at [https://www.shawntabrizi.com/substrate-js-utilities/](https://www.shawntabrizi.com/substrate-js-utilities/) that allow people to quickly calculate the `blake2_128_concat` or `twox_64_concat` of some arbitrary string or hex bytes. Try out these examples on your own and see for yourself the advantages that these new transparent storage keys bring to you.\\n\\nAs always, if you enjoy the content I write, you can send me a friendly tip on my [donations page](https://www.shawntabrizi.com/donate/)."},{"id":"/substrate/substrate-weight-and-fees/","metadata":{"permalink":"/blog/substrate/substrate-weight-and-fees/","source":"@site/blog/2020-02-27-substrate-weights-and-fees.md","title":"Substrate Weights and Fees","description":"This is the first in a series of posts explaining our philosophy toward benchmarking and assigning weights to Substrate pallets for the imminent launch of the Polkadot network.","date":"2020-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"weight","permalink":"/blog/tags/weight"},{"inline":true,"label":"transaction","permalink":"/blog/tags/transaction"},{"inline":true,"label":"fee","permalink":"/blog/tags/fee"},{"inline":true,"label":"benchmark","permalink":"/blog/tags/benchmark"},{"inline":true,"label":"substrate","permalink":"/blog/tags/substrate"}],"readingTime":7.275,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Substrate Weights and Fees","date":"2020-02-27T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/substrate-weight-and-fees/","categories":["Substrate"],"tags":["weight","transaction","fee","benchmark","substrate"]},"unlisted":false,"prevItem":{"title":"Transparent Keys in Substrate","permalink":"/blog/substrate/transparent-keys-in-substrate/"},"nextItem":{"title":"Porting Web3.js to Polkadot.js","permalink":"/blog/substrate/porting-web3-js-to-polkadot-js/"}},"content":"##### This is the first in a series of posts explaining our philosophy toward benchmarking and assigning weights to Substrate pallets for the imminent launch of the Polkadot network.\\n\\nFor the past couple of weeks, I have been working hard creating and executing a plan around weighing extrinsic functions across our different FRAME pallets so that we can launch the Polkadot network.\\n\\nThere are a few overall goals that we want to accomplish with this task:\\n\\n1. Provide computational limits and thresholds for block producers on the network.\\n2. Provide economic security such that malicious actors would not be able to profitably attack the network.\\n3. Identify and redesign extrinsics which have non-linear complexity.\\n4. Improve the runtime architecture to reduce overall complexity.\\n\\nTo follow along this journey, you must first understand what \\"weights\\" are in Substrate, and how they are related to the more commonly understood fee system.\\n\\n## Weights\\n\\nIf you are unfamiliar with weights, the TL;DR is that a Substrate blockchains have limited resources when it comes to producing new blocks. Most notably, there is a limited window for block producers to create a block, limited amount of data that can be included per block ([`MaximumBlockLength`](https://substrate.dev/rustdocs/master/frame_system/trait.Trait.html#associatedtype.MaximumBlockLength)), and an overall practical limit to the storage footprint of the blockchain.\\n\\nSubstrate has introduced a Weight system that allows the runtime developer to tell the block production process how \\"heavy\\" an extrinsic is. Given some [`MaximumBlockWeight`](https://substrate.dev/rustdocs/master/frame_system/trait.Trait.html#associatedtype.MaximumBlockWeight), and the weight of the individual extrinsics in a transaction pool, we can select the set of extrinsics that allow us to saturate our block, while not going over the limits.\\n\\nOn top of this basic idea, Substrate has additionally introduced a configurable [`AvailableBlockRatio`](https://substrate.dev/rustdocs/master/frame_system/trait.Trait.html#associatedtype.AvailableBlockRatio) which ensures that only a portion of the total `MaximumBlockWeight` is used for regular transactions. This also introduces the concept of _operational transactions_ which are system critical operations that can use the rest of the available block weight.\\n\\n### Example\\n\\nLet\'s say a `balance_transfer` has weight 1,000, and our Substrate chain is configured to a maximum block weight of 1,000,000, with an available block ratio of 20%.\\n\\nThis means we would be able to include at most:\\n\\n    1,000,000 * .20 / 1,000 = 200 transfers per block\\n\\nFor more details on weights, read our doc: [https://substrate.dev/docs/en/conceptual/runtime/weight](https://substrate.dev/docs/en/conceptual/runtime/weight)\\n\\n## Fees\\n\\nTo bring the weight system to the users of our blockchain, Substrate introduces a tightly coupled fee system. In short, users will pay a transaction fee proportional to the weight of the call they are making.\\n\\n    total_fee = base_fee + length_fee + weight_fee\\n\\n> Note: There is also a length_fee which takes into account the amount of data included in an extrinsic.\\n\\nAs a pallet developer writing new dispatchable functions, the fee system should mostly be abstract to you, and instead you should primarily think in terms of weights.\\n\\nFor more details on fees, read our doc: [https://substrate.dev/docs/en/development/module/fees](https://substrate.dev/docs/en/development/module/fees)\\n\\n## Runtime Development\\n\\nAs a runtime developer, it is your goal to:\\n\\n- Minimize the computational and resource complexity of runtime functions.\\n- Accurately calculate the relative weight of your runtime functions.\\n\\nWe can accomplish this in three steps:\\n\\n1. Follow best practices when writing a runtime.\\n2. Accurately document the computational complexity introduced by runtime functions.\\n3. Empirically measure the real world cost of running these functions, and associate those measurements back to our computational complexity.\\n\\nIt is beyond the scope of any single blog post to explain all the best practices when it comes to runtime development, but we can start to touch on (2) and follow up and talk more about how to approach (3).\\n\\n### Documentation of Weights\\n\\nDispatchable functions within a FRAME pallet should contain documentation about the computational and resource complexity of the function. The result of weight documentation is to arrive at a final [order of a function](https://en.wikipedia.org/wiki/Big_O_notation). Such as:\\n\\n```\\nO(A + logA + BlogC)\\n```\\n\\nThis should serve as a resource to accurately measure the weight of different functions across all possible inputs, something we would not reasonably able to measure otherwise.\\n\\n#### What to Document\\n\\nYour weight documentation should include information about your runtime function which has notable execution costs. For example:\\n\\n- Storage Operations (read, write, mutate, etc...)\\n- Codec Operations (serializing/deserializing vecs or large structs)\\n- Search / Sort / Notable Computation\\n- Calls to other pallet functions (i.e. reserving some balance through the Currency trait)\\n\\nWe will work off the following example function:\\n\\n```rust\\n// Join a group of members.\\nfn join(origin) {\\n\\tlet who = ensure_signed(origin)?;\\n\\tlet deposit = T::Deposit::get(); // configuration constant\\n\\tlet sorted_members: Vec<T::AccountId> = Self::members();\\n\\tensure!(sorted_members.len() <= 100, \\"Membership Full\\");\\n\\tmatch sorted_members.binary_search(&who) {\\n\\t\\t// User is not a member.\\n\\t\\tErr(i) => {\\n\\t\\t\\tT::Currency::reserve(&who, deposit)?;\\n\\t\\t\\tmembers.insert(i, who.clone());\\n\\t\\t\\t<Members<T>>::put(sorted_members);\\n\\t\\t\\tOk(())\\n\\t\\t},\\n\\t\\t// User is already a member, do nothing.\\n\\t\\tOk(_) => Ok(()),\\n\\t}\\n\\tSelf::deposit_event(RawEvent::Joined(who));\\n}\\n```\\n\\n#### Storage and Codec Operations\\n\\nAccessing storage is a heavy operation, and one that should be well documented and optimized in favor writing \\"functional code\\". See [Best Practices](https://www.notion.so/paritytechnologies/Weights-8c916536949b47f299eed1302b6a2074?p=c5aafd34578f4be9ab8c8d7510e98314&showMoveTo=true#best-practices).\\n\\nThe each storage operation should be documented with the relative codec complexity of interacting with that storage.\\n\\nFor example, if you are reading a vector of members from a single value storage item, the weight documentation should read:\\n\\n    - One storage read to get the members of this pallet: `O(M)`.\\n\\nIn this case reading the vector from storage has a codec complexity of `O(M)` to deserialize the `M` member accounts in the vector.\\n\\nLater in your module, you might go ahead and write the data back into the runtime, which should also be documented:\\n\\n    - One storage write to update the members of this pallet: `O(M)`.\\n\\n#### Search, Sort, and Notable Computations\\n\\nIf you need to search or sort in your runtime module, it is also important to note the relative complexity of those operations.\\n\\nFor example, if you are searching for an item in a sorted list, a `binary_search` operation should take `O(logM)`, while an unsorted list, should take `O(M)`.\\n\\nSo the documentation may look like:\\n\\n    - Insert a new member into sorted list: O(logM).\\n\\nThis kind of documentation should be present for any sort of notable heavy computation present in your logic.\\n\\n#### Calls to Other Pallets and Traits\\n\\nThe computational complexity of your function may extend beyond your pallet. If you call other FRAME pallets either directly or through Trait configurations, you should take note of that, and assign these calls with their own variable.\\n\\nFor example, if you write a function which reserves some balance in the Balances pallet or emits an event through the System pallet, you should document:\\n\\n    - One balance reserve operation: O(B)\\n    - One event emitted: O(E)\\n\\n#### Combining the Data\\n\\nOnce you have good documentation for your runtime function, you need to consolidate it into a _single overall order of the function_.Lets combine the different example operations to create a full end to end example.\\n\\n```text\\n# <weight>\\nKey: M (len of members), B (reserve balance), E (event)\\n- One storage read to get the members of this pallet: `O(M)`.\\n- One balance reserve operation: O(B)\\n- Insert a new member into sorted list: O(logM).\\n- One storage write to update the members of this pallet: `O(M)`.\\n- One event emitted: O(E)\\n\\nTotal Complexity: O(M + logM + B + E)\\n# </weight>\\n```\\n\\n> Note: You may have introduced multiple different variables into your overall weight documentation, so be sure to document what these variables represent.\\n\\nIf you look at this example, you can see we had two operations that were O(M) (the storage read and write), but our overall order does not take this into account.\\n\\n**When doing empirical testing, we are unable to separate complexities which have the same order**. This means that there could be many many more operations added to this function, of order `O(M)`, `O(logM`), etc.. but it would not change our final formula as a function of `M`, `B`, and `E`:\\n\\n    weight(M, B, E) = K_1 + K_2 * M + K_3 * logM + B + E\\n\\nThe difference between two functions with the same order will be empirically measured through on-chain tests. The goal of this step is to simply derive the coefficients (`K`) that we will be searching for when we do the [next step](https://www.notion.so/paritytechnologies/Weights-8c916536949b47f299eed1302b6a2074?p=c5aafd34578f4be9ab8c8d7510e98314&showMoveTo=true#measuring-weights).\\n\\n## Summary\\n\\nSo hopefully now you can see how we can approach runtime code, and from it, derive a theoretical order of complexity. The next step after this is to actually run benchmarks for these different extrinsics, and start collecting data that we can map back to these derived formulas.\\n\\nIf you enjoy this content and want to see the next post where we dive deeper into actually benchmarking the runtime, consider taking a look at my [donations page](https://shawntabrizi.com/donate/) to see how you can support me."},{"id":"/substrate/porting-web3-js-to-polkadot-js/","metadata":{"permalink":"/blog/substrate/porting-web3-js-to-polkadot-js/","source":"@site/blog/2020-01-12-porting-web3-js-to-polkadot-js.md","title":"Porting Web3.js to Polkadot.js","description":"In this post, I will go over the changes I needed to make in order to port a Web3.js based Ethereum web app I had previously blogged about to use Polkadot.js and Substrate.","date":"2020-01-12T00:00:00.000Z","tags":[{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"front-end","permalink":"/blog/tags/front-end"},{"inline":true,"label":"balance","permalink":"/blog/tags/balance"},{"inline":true,"label":"graph","permalink":"/blog/tags/graph"},{"inline":true,"label":"plotly.js","permalink":"/blog/tags/plotly-js"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"substrate","permalink":"/blog/tags/substrate"}],"readingTime":9.265,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Porting Web3.js to Polkadot.js","date":"2020-01-12T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/porting-web3-js-to-polkadot-js/","categories":["Substrate"],"tags":["javascript","front-end","balance","graph","plotly.js","ethereum","substrate"],"github":"substrate-balance-graph","customjs":["https://www.shawntabrizi.com/substrate-balance-graph/polkadot.js"]},"unlisted":false,"prevItem":{"title":"Substrate Weights and Fees","permalink":"/blog/substrate/substrate-weight-and-fees/"},"nextItem":{"title":"Substrate Storage Deep Dive","permalink":"/blog/substrate/substrate-storage-deep-dive/"}},"content":"##### In this post, I will go over the changes I needed to make in order to port a Web3.js based Ethereum web app I had [previously blogged about](./2018-03-11-graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js.md) to use Polkadot.js and Substrate.\\n\\nAlmost 2 years ago, I was still on my journey learning about Ethereum, when I built a simple web application using Web3.js. At the time, there was a spawn of viral \\"ponzi scheme\\" smart contracts, and I wanted to see how these dApps grew and eventually crashed over time.\\n\\nCheck out my previous blog post about [Graphing ETH Balance History of an Ethereum Address using Parallel Asynchronous Requests in Web3.js](./2018-03-11-graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js.md) to learn more.\\n\\nSince the launch of [Kusama](https://kusama.network/), there has been a lot more activity around actually _using_ Substrate, specifically among the validator/nominator community. I wanted to take a look at the my nomination rewards over time, and to do that, I basically needed to rebuild this same application, but using Polkadot.js... ([sneak peek](https://www.shawntabrizi.com/substrate-balance-graph/))\\n\\n![Before and after screenshot of Web3 to Polkadot port](/assets/images/substrate-balance-graph-hero.png)\\n\\nHere is **that** journey.\\n\\n## Creating a Polkadot.js Bundle\\n\\nThe first issue I ran into when trying to migrate from Web3.js to Polkadot.js was generating a standalone JavaScript bundle so I can simply include the dependencies into my barebones project. At the moment, Polkadot.js does not provide an official bundle, but it is easy enough to create with [browserify](http://browserify.org/).\\n\\nAssuming you already have `npm`, here are those steps:\\n\\n1. Install browserify:\\n\\n   ```bash\\n   npm install -g browserify\\n   ```\\n\\n2. Create a new NodeJS project:\\n\\n   ```bash\\n   mkdir temp\\n   cd temp\\n   npm init\\n   # lots of interaction here, doesn\'t matter what you select\\n   ```\\n\\n3. Add the Polkadot.js dependencies (I use `@beta`, but the exact versions to use may change over time):\\n\\n   ```bash\\n   npm install @polkadot/api@beta\\n   npm install @polkadot/util@beta\\n   npm install @polkadot/util-crypto@beta\\n   npm install @polkadot/keyring@beta\\n   ```\\n\\n   You should have a `package.json` that looks like:\\n\\n   ```json\\n   \\"dependencies\\": {\\n     \\"@polkadot/api\\": \\"^1.0.0-beta.7\\",\\n     \\"@polkadot/keyring\\": \\"^2.0.0-beta.4\\",\\n     \\"@polkadot/util\\": \\"^2.0.0-beta.4\\",\\n     \\"@polkadot/util-crypto\\": \\"^2.0.0-beta.4\\"\\n   }\\n   ```\\n\\n4. Create a simple file which exports these libraries into the `window` object:\\n\\n   ```javascript\\n   // In a file named `dependencies.js`\\n   let api = require(\\"@polkadot/api\\");\\n   let util = require(\\"@polkadot/util\\");\\n   let util_crypto = require(\\"@polkadot/util-crypto\\");\\n   let keyring = require(\\"@polkadot/keyring\\");\\n\\n   window.api = api;\\n   window.util = util;\\n   window.util_crypto = util_crypto;\\n   window.keyring = keyring;\\n   ```\\n\\n5. Create the `polkadot.js` bundle:\\n\\n   ```bash\\n   browserify dependencies.js > polkadot.js\\n   ```\\n\\nYou should now have a `polkadot.js` file that you can include into any HTML page and will export `api`, `util`, `util_crypto`, and `keyring` commands.\\n\\n```html\\n<script src=\\"polkadot.js\\"><\/script>\\n```\\n\\nActually, you can find it on this page too! Just open your browser console and try any of these commands.\\n\\n```javascript\\nutil_crypto.blake2AsHex(\\"Hello, World!\\") >\\n  \\"0x511bc81dde11180838c562c82bb35f3223f46061ebde4a955c27b3f489cf1e03\\";\\n```\\n\\nIf you don\'t want to follow these steps, feel free to grab the `polkadot.js` bundle I created at: [shawntabrizi/substrate-balance-graph](https://github.com/shawntabrizi/substrate-balance-graph).\\n\\n## Connecting to a Node\\n\\nAs a front-end developer, I am not so interested in setting up a local node, to get my app to work. In the Web3.js world, I would use Metamask + a dedicated infura node. From my Ethereum web app [(ethgraph)](https://github.com/shawntabrizi/ethgraph):\\n\\n```js\\n// Check for MetaMask, otherwise use an HTTP Provider\\nwindow.addEventListener(\\"load\\", function () {\\n  if (typeof web3 !== \\"undefined\\") {\\n    console.log(\\"Web3 Detected! \\" + web3.currentProvider.constructor.name);\\n    window.web3 = new Web3(web3.currentProvider);\\n  } else {\\n    console.log(\\"No Web3 Detected... using HTTP Provider\\");\\n    window.web3 = new Web3(\\n      new Web3.providers.HttpProvider(\\"https://mainnet.infura.io/<APIKEY>\\")\\n    );\\n  }\\n});\\n```\\n\\nThe [polkadot-js/extension](https://github.com/polkadot-js/extension) does not inject a WebSocket provider automatically, so we can skip the \\"detected\\" step, and just connect when we know we are not connected. Substrate is also not just a platform for _one_ chain, but many chains, so I wanted to also support the user user customizable endpoints.\\n\\nI created a `connect` function which looks like this:\\n\\n```js\\n// Connect to Substrate endpoint\\nasync function connect() {\\n  let endpoint = document.getElementById(\\"endpoint\\").value;\\n  if (!window.substrate || global.endpoint != endpoint) {\\n    const provider = new api.WsProvider(endpoint);\\n    document.getElementById(\\"output\\").innerHTML = \\"Connecting to Endpoint...\\";\\n    window.substrate = await api.ApiPromise.create({ provider });\\n    global.endpoint = endpoint;\\n    document.getElementById(\\"output\\").innerHTML = \\"Connected\\";\\n  }\\n}\\n```\\n\\nYou can see I keep track of two global properties:\\n\\n1. `window.substrate` - This will be my WebSocket provider and how I access the Polkadot.js APIs. If it already exists, I am already connected!\\n2. `window.global.endpoint` - This is a global variable I created to keep track of the current endpoint I am connected to.\\n\\nWhen I call `connect`, it will make sure I am connected to the endpoint I want based on the input of the `endpoint` element on the HTML page. For a network like Kusama, this endpoint would be something like:\\n\\n```\\nwss://kusama-rpc.polkadot.io/\\n```\\n\\n## Querying the Node\\n\\nAt the time of creating `ethgraph`, Web3.js did not support `async`/`await`. Instead, [I wrapped everything in a \\"promisify\\" wrapper](./2017-11-24-making-web3-js-work-asynchronously-javascript-promises-await.md). Fortunately, Polkadot.js supports this natively, so you can query every API easily and ergonomically with a promise.\\n\\nFor example, here is how we can get the balance of a user:\\n\\n```javascript\\nlet balance = await substrate.query.balances.freeBalance(\\n  \\"EGVQCe73TpFyAZx5uKfE1222XfkT3BSKozjgcqzLBnc5eYo\\"\\n);\\nbalance.toNumber() > 2116624633061757;\\n```\\n\\nTo provide all the functionality of the Ethereum version of this app, I also need to query the timestamp of a block. Ethereum would include this in the block header, but we know that Substrate has no such requirements, and instead provides this through another runtime module:\\n\\n```javascript\\nlet timestamp = await substrate.query.timestamp.now();\\nDate(timestamp) >\\n  \\"Wed Jan 15 2020 22:42:37 GMT+0100 (Central European Standard Time)\\";\\n```\\n\\nGreat! But how do we get the _historical_ information?\\n\\nIn Ethereum, we could just provide the block number directly into the query:\\n\\n```javascript\\nweb3.eth.getBalance(address, blockNumber, function () {\\n  /*callback*/\\n});\\n```\\n\\nIn Polkadot.js we need to use the `.at(hash, <PARAMS>)` API, which extends all the Substrate queries. `hash` here is the block hash of the block that I want to get the information for. To get the block hash, I need to make an RPC call through the Polkadot.js API:\\n\\n```javascript\\nlet blockHash = await substrate.rpc.chain.getBlockHash(100);\\nblockHash.toString() >\\n  \\"0x46781d9a3350a0e02dbea4b5e7aee7c139331a65b2cd736bb45a824c2f3ffd1a\\";\\n```\\n\\nSo all together now:\\n\\n```javascript\\nlet balance_100 = await substrate.query.balances.freeBalance.at(\\n  \\"0x46781d9a3350a0e02dbea4b5e7aee7c139331a65b2cd736bb45a824c2f3ffd1a\\",\\n  \\"EGVQCe73TpFyAZx5uKfE1222XfkT3BSKozjgcqzLBnc5eYo\\"\\n);\\nbalance_100.toNumber() > 10000000000;\\n```\\n\\nYou can see I gained quite a bit of free balance since block 0! :)\\n\\n## Keeping it Async\\n\\nSo we have all the pieces to be able convert our old queries into the new ones. However, if we do things naively, we will run into a trap which was warned about in my last blog post.\\n\\nCan you guess?\\n\\nLet\'s take a look how a naive conversion between Web3.js to Polkadot.js would look like:\\n\\n- Original Web3.js Code\\n\\n  ```javascript\\n  // Loop over the blocks, using the step value\\n  for (let i = startBlock; i < endBlock; i = i + step) {\\n    // If we already have data about that block, skip it\\n    if (!global.balances.find((x) => x.block == i)) {\\n      // Create a promise to query the ETH balance for that block\\n      let balancePromise = promisify((cb) =>\\n        web3.eth.getBalance(address, i, cb)\\n      );\\n      // Create a promise to get the timestamp for that block\\n      let timePromise = promisify((cb) => web3.eth.getBlock(i, cb));\\n      // Push data to a linear array of promises to run in parellel.\\n      promises.push(i, balancePromise, timePromise);\\n    }\\n  }\\n  ```\\n\\n- Naive Polkadot.js Code\\n\\n  ```javascript\\n  // Loop over the blocks, using the step value\\n  for (let i = startBlock; i < endBlock; i = i + step) {\\n    // If we already have data about that block, skip it\\n    if (!global.balances.find((x) => x.block == i)) {\\n      // Get the block hash\\n      let blockHash = await substrate.rpc.chain.getBlockHash(i);\\n      // Create a promise to query the balance for that block\\n      let freeBalancePromise = substrate.query.balances.freeBalance.at(\\n        blockHash,\\n        address\\n      );\\n      // Create a promise to get the timestamp for that block\\n      let timePromise = substrate.query.timestamp.now.at(blockHash);\\n      // Push data to a linear array of promises to run in parellel.\\n      promises.push(i, freeBalancePromise, timePromise);\\n    }\\n  }\\n  ```\\n\\nFirst, we should call out how incredibly similar the two code blocks look. The naive update is _totally_ working, and really we did not have to change our app at all! But if you are trying this at home, you might notice the app is running pretty slow... over 30 seconds to fetch the data needed to create the graph!\\n\\n![Image before parallel async](/assets/images/substrate-balance-graph-before.png)\\n\\nThe point of this loop was to collect all the queries and run them asynchronously. As mentioned in the last blog post, this provides a huge boost in performance since we are not waiting for each response to move onto the next one. However, this naive conversion sticks an `await` right in the middle of the loop, and this causes us to serialize querying for all the blocks, and slow down the entire processes.\\n\\nTo solve this, we want to also query all the block hashes for the blocks we need in parallel, but in a separate loop, because we need to know the hash before we can make the next query.\\n\\nThe improved solution looks like:\\n\\n```javascript\\nvar promises = [];\\n\\n// Get all block hashes\\nfor (let i = startBlock; i < endBlock; i = i + step) {\\n  // If we already have data about that block, skip it.\\n  if (!global.blockHashes.find((x) => x.block == i)) {\\n    let blockHashPromise = substrate.rpc.chain.getBlockHash(i);\\n    promises.push(i, blockHashPromise);\\n  }\\n}\\n\\n// Call all promises in parallel for speed\\nvar results = await Promise.all(promises);\\n\\n// Save block hashes globally so we don\'t query them again if we don\'t need to.\\nfor (let i = 0; i < results.length; i = i + 2) {\\n  global.blockHashes.push({\\n    block: results[i],\\n    hash: results[i + 1],\\n  });\\n}\\n\\nvar promises = [];\\n\\n// Loop over the blocks, using the step value\\nfor (let i = startBlock; i < endBlock; i = i + step) {\\n  // If we already have data about that block, skip it\\n  if (!global.balances.find((x) => x.block == i)) {\\n    // Get the block hash\\n    let blockHash = global.blockHashes.find((x) => x.block == i).hash;\\n    // Create a promise to query the balance for that block\\n    let freeBalancePromise = substrate.query.balances.freeBalance.at(\\n      blockHash,\\n      address\\n    );\\n    // Create a promise to get the timestamp for that block\\n    let timePromise = substrate.query.timestamp.now.at(blockHash);\\n    // Push data to a linear array of promises to run in parellel.\\n    promises.push(i, freeBalancePromise, timePromise);\\n  }\\n}\\n\\n// Call all promises in parallel for speed\\nvar results = await Promise.all(promises);\\n\\nconsole.log(\\"Results:\\", results);\\n```\\n\\nThis generates a graph for us in under 2 seconds!\\n\\n![Image after parallel async](/assets/images/substrate-balance-graph-after.png)\\n\\nMuch better, and what you would expect from a modern web application! But here we don\'t have a traditional database, just a blockchain.\\n\\n## Final Thoughts\\n\\nYou can play with the final application here: [https://www.shawntabrizi.com/substrate-balance-graph/](https://www.shawntabrizi.com/substrate-balance-graph/)\\n\\nAfter this exercise it has become clear to me that porting existing web applications built with Web3.js to Polkadot.js is trivial. Additionally, I already have a ton of experience with Substrate runtime development, so I already know how easy it will be to take existing smart contracts and build them on Substrate, maybe even better than before.\\n\\nWith that in mind, it won\'t be long until we see a wave of existing dApps joining Substrate/Polkadot, taking advantage of all the next generation features without making any compromises toward their existing functionality. The future seems bright overall, and I am excited to be at the forefront.\\n\\nAs always, if you like the content I create, stop by my [donations page](https://shawntabrizi.com/donate/) and say thanks!"},{"id":"/substrate/substrate-storage-deep-dive/","metadata":{"permalink":"/blog/substrate/substrate-storage-deep-dive/","source":"@site/blog/2019-12-09-substrate-storage-deep-dive.md","title":"Substrate Storage Deep Dive","description":"In this post, I will share the video and slides I presented at the Substrate Developer Conference (Sub0) which gives a deep dive into the storage layers of the Substrate blockchain development framework.","date":"2019-12-09T00:00:00.000Z","tags":[{"inline":true,"label":"storage","permalink":"/blog/tags/storage"},{"inline":true,"label":"database","permalink":"/blog/tags/database"},{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"sub0","permalink":"/blog/tags/sub-0"},{"inline":true,"label":"merkle","permalink":"/blog/tags/merkle"},{"inline":true,"label":"patricia","permalink":"/blog/tags/patricia"},{"inline":true,"label":"trie","permalink":"/blog/tags/trie"}],"readingTime":0.945,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Substrate Storage Deep Dive","date":"2019-12-09T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/substrate-storage-deep-dive/","categories":["Substrate"],"tags":["storage","database","runtime","sub0","merkle","patricia","trie"]},"unlisted":false,"prevItem":{"title":"Porting Web3.js to Polkadot.js","permalink":"/blog/substrate/porting-web3-js-to-polkadot-js/"},"nextItem":{"title":"What is Substrate?","permalink":"/blog/substrate/what-is-substrate/"}},"content":"##### In this post, I will share the video and slides I presented at the Substrate Developer Conference (Sub0) which gives a deep dive into the storage layers of the Substrate blockchain development framework.\\n\\nThis last week I was given the opportunity to present at the official Substrate Developer Conference where I gave a deep dive into the inner workings of Substrate storage.\\n\\nAs a runtime developer, you should know that storage is one of the most important things to keep in mind when designing and developing your new runtime modules. Having a deep understanding of how the different storage layers work and interact will be critical for you to make correct decisions moving forward.\\n\\nHere is a video of my presentation, and the presentation itself:\\n\\n## Video\\n\\n<iframe width=\\"720px\\" height=\\"480px\\" src=\\"https://www.youtube.com/embed/9S8rmW8LD5o\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n## Presentation\\n\\n<iframe src=\\"/assets/presentations/substrate-storage-deep-dive.pdf\\" width=\\"720px\\" height=\\"480px\\"></iframe>\\n\\n## Questions?\\n\\nFeel free to contact me with any questions you might have or any details you think I may have gotten wrong. As always, if you enjoy this content and want to continue to support me, take a look at my [donations page](https://shawntabrizi.com/donate/)."},{"id":"/substrate/what-is-substrate/","metadata":{"permalink":"/blog/substrate/what-is-substrate/","source":"@site/blog/2019-09-29-what-is-substrate.md","title":"What is Substrate?","description":"In this post, I will try to explain the Substrate blockchain framework in a way that anyone with a bit of technical experience could understand.","date":"2019-09-29T00:00:00.000Z","tags":[{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"module","permalink":"/blog/tags/module"},{"inline":true,"label":"block","permalink":"/blog/tags/block"},{"inline":true,"label":"database","permalink":"/blog/tags/database"},{"inline":true,"label":"networking","permalink":"/blog/tags/networking"},{"inline":true,"label":"transaction queue","permalink":"/blog/tags/transaction-queue"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"}],"readingTime":10.95,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"What is Substrate?","date":"2019-09-29T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/what-is-substrate/","categories":["Substrate"],"tags":["runtime","module","block","database","networking","transaction queue","consensus"]},"unlisted":false,"prevItem":{"title":"Substrate Storage Deep Dive","permalink":"/blog/substrate/substrate-storage-deep-dive/"},"nextItem":{"title":"Substrate Feeless Token Factory","permalink":"/blog/substrate/substrate-feeless-token-factory/"}},"content":"##### In this post, I will try to explain the Substrate blockchain framework in a way that anyone with a bit of technical experience could understand.\\n\\nYou may have heard before that Substrate is an **extensible**, **modular**, and **open-source** framework for building blockchains. But what does that mean?\\n\\nSubstrate provides you with all the core components needed to build a distributed blockchain network:\\n\\n- [Database](#database)\\n- [Networking](#networking)\\n- [Transaction Queue](#transaction-queue)\\n- [Consensus](#consensus)\\n\\nWhile these layers are extensible, Substrate mostly assumes the average blockchain developer should not care about the specific implementation details of these core components. Instead, Substrate\'s core philosophy is to make development of a blockchain\'s state transition function as flexible and easy as possible. This layer is called the [Substrate runtime](#substrate-runtime).\\n\\nBut before we dive into all these details, first we need to establish a common understanding of what a blockchain is...\\n\\n## What is a Blockchain?\\n\\nIn its most basic form, a blockchain is a simple data structure where _blocks_ of data are linked together forming an ordered _chain_. The specific details of a blockchain can vary depending on the functionality of that chain. However, at a high level, all blockchains should share some common properties.\\n\\n### Blocks\\n\\nEach block in a blockchain has some data that can be used to generate a unique identifier for that block. One part of this data is the unique identifier for the preceding block, known as the \\"parent block\\". Since each block has a pointer to its parent block, the blocks can be ordered in a deterministic way.\\n\\n![Blockchain Blocks](/assets/images/wis-blocks.png)\\n\\nAny small changes to the data in a block will change its unique ID. Since this block\'s ID changed, the block that comes after it (the \\"child block\\") will also change. Same with the next child, and the next one, and the next... In fact, all the blocks that come after the originally modified block will have to change their unique ID in order to maintain the chain! This means that it is easy to verify that two blockchains have the exact same data by simply checking the unique identifier of the last block on the chain.\\n\\n> To learn more about these basics of a blockchain, visit the demo/videos found here: [https://anders.com/blockchain/](https://anders.com/blockchain/)\\n\\n### Block Production\\n\\nDue to these properties, blockchain systems are commonly used to keep track of a shared [ledger](https://en.wikipedia.org/wiki/Ledger). The contents of the ledger are changed not by changing an existing block, but by adding new blocks to the blockchain with instructions about how the state of the ledger should change from block to block. These instructions are commonly referred to as _transactions_.\\n\\n![Block Production](/assets/images/wis-block-production.png)\\n\\nThere are usually rules associated with how the ledger can change, which are defined by a [state transition function](https://en.wikipedia.org/wiki/Transition_system). For cryptocurrency systems, these rules can be quite simple; for example:\\n\\n> **Rule:** Users can only spend funds that they own.\\n\\nThese rules can also be more complex, even allowing for blockchain systems to act as a [Turing complete](https://simple.wikipedia.org/wiki/Turing_complete) computer, and the ledger acting as that computer\'s storage.\\n\\nOnce a valid set of transactions are collected, they are put into the content of a block, and then this block is placed at the end of the chain. This block production process allows the underlying state of the blockchain to change over time.\\n\\n### Block Finalization\\n\\nNow that a new block has been produced, it can be shared with others who want to construct the same shared ledger. However, since blockchains are decentralized in nature, it could be that two different, yet still valid blocks compete for the same position at the end of a chain. Different block finalization mechanisms can be used to determine which \\"chain of blocks\\" is the _canonical_ blockchain. For any given blockchain, there should only be one true final state of the shared ledger. Any alternative states of the blockchain are known as \\"forks\\".\\n\\n![Blockchain Fork](/assets/images/wis-forking.png)\\n\\nForks are normal, expected, and generally not a problem. The block finalization process is in place to help non-canonical chains get back in sync. We will return to forking later in this post.\\n\\n### Nodes\\n\\nAt this point, you should be able to see that blockchains are designed to be distributed and decentralized. You want multiple users around the world to be able to keep track of this shared ledger without the need of intermediary third parties. By following the rules above, each participant of this shared ledger can run a _node_, which is a computer program that follows the rules of the blockchain network and connects to other nodes that do the same, all without the need for a centralized service. Blockchain system are often \\"open\\" systems, which mean that anyone can participate. To prevent against malicious actors, mechanisms are put in place to incentivize good behavior while punishing bad behavior. With all of these mechanisms in place, a blockchain system can become an unstoppable machine.\\n\\n## Substrate Components\\n\\nNow that you have a high level understanding of what a blockchain is, we can now start to understand how Substrate is a framework for building them. The first claim about the Substrate framework is that it is **extensible**. This means that it makes as few assumptions about how you design your blockchain and attempts to be as _generic_ as possible.\\n\\n### Database\\n\\nAs we illustrated, the heart of a blockchain is its shared ledger that must be maintained and stored. Substrate makes no assumptions about the content or structure of the data in your blockchain. The underlying database layer uses simple key-value storage, on top of which a modified Patricia Merkle tree ([trie](https://github.com/paritytech/trie)) is implemented. This special storage structure allows us to easily verify if an item is or is not in that storage. This is particularly important to support light clients, who will depend on these storage proofs to provide light-weight, yet trustless interactions with the blockchain network.\\n\\n![Trie Structure](/assets/images/wis-trie.png)\\n\\n### Networking\\n\\nIn order for a decentralized blockchain system to communicate, it needs to establish a peer-to-peer networking protocol. Substrate uses [libp2p](https://github.com/libp2p) as a modular peer-to-peer networking stack. Through this networking layer, Substrate-based blockchains are able to share transactions, blocks, peers, and other system critical details without the need for centralized servers. In line with Substrate\'s philosophy, libp2p is unique in the fact that it makes no assumptions about your specific networking protocol. As a result, you are able to implement and use different transports on top of a Substrate-based blockchain.\\n\\n![Peer-to-Peer Networking](/assets/images/wis-p2p.png)\\n\\n### Transaction Queue\\n\\nAs mentioned above, transactions are collected and formed into blocks that ultimately define how the state of the blockchain changes. However, the order of these transactions can impact the final state of the ledger. Substrate allows you full control over the dependency and queue management of transactions on your network. Substrate only assumes that a transaction has a _weight_ and a set of prerequisite _tags_ that are used to create dependency graphs. These dependency graphs are linear in the simplest case, but they can become more complex. Substrate handles those complexities for you automatically.\\n\\n![](/assets/images/wis-txq.png)\\n\\n### Consensus\\n\\nRecall that there are different ways that a blockchain network can come to consensus about changes to the chain. Traditionally, these consensus engines are tightly coupled to the other blockchain components. However, Substrate has spent extra effort designing a consensus layer that can be easily changed during development. In fact, it was made such that consensus could even be hot-swapped after the chain goes live! Built into Substrate are multiple different consensus engines such as traditional [Proof of Work (PoW)](https://en.wikipedia.org/wiki/Proof_of_work), [Aura (Authority Round)](https://github.com/poanetwork/wiki/wiki/Aura-Consensus-Protocol-Audit), and Polkadot consensus, which is unique in the fact that it separates the block production process ([BABE](https://research.web3.foundation/en/latest/polkadot/BABE/Babe/)) from the block finalization process ([GRANDPA](https://research.web3.foundation/en/latest/polkadot/GRANDPA/)).\\n\\n![GRANDPA Consensus](/assets/images/wis-grandpa.png)\\n\\n## Substrate Runtime\\n\\nSo far, we have touched on all the core blockchain components that Substrate provides to you. As you have read, Substrate has made every effort to stay as generic and extensible as possible. However, arguably the most customizable part of Substrate is its **modular** runtime. The runtime is Substrate\'s _state transition function_ that we mentioned earlier.\\n\\nSubstrate believes that the average blockchain developer does not need to care so much about the blockchain components listed above. As long as the components are battle-tested and production-ready, the implementation details are often of little importance. However, the core blockchain logic that determines what is and is not valid for a network is often of critical importance for any chain.\\n\\n**Thus, Substrate\'s core philosophy is to make blockchain runtime development as flexible and easy as possible.**\\n\\n### Substrate Runtime Module Library (SRML)\\n\\nA Substrate runtime is divided into separate logical components that are known as _runtime modules_. These modules will control some aspect of the on-chain logic managed by that blockchain. You can think of these modules like \\"plug-ins\\" for your system. As a Substrate developer, you are able to pick and choose the modules and functionality that you want to include in your chain.\\n\\nFor example, there is a module called \\"Balances\\" that manages the currency of the chain. There are also a set of modules like \\"Collective\\", \\"Democracy\\", and \\"Elections\\" that handle decision making and governance for the chain. There is even a module called \\"Contracts\\" that can turn any Substrate-based chain into a [smart contract](https://en.wikipedia.org/wiki/Smart_contract) platform. These kinds of modules are provided to you automatically when you build on Substrate.\\n\\nHowever, you are not limited to only the modules provided by Substrate. In fact, developers can easily build their own runtime modules, either as independent logical components, or even directly interacting with other runtime modules to build more complicated logic. I believe that long term, the module system in Substrate will act much like an \\"app store\\", where users can simply pick and choose the functionality they want to include, and with minimal technical expertise, deploy a distributed blockchain network!\\n\\n![Modern App Store](/assets/images/wis-app-store.png)\\n\\n### Forkless Runtime Upgrades\\n\\nIf we follow the analogy of the Substrate module ecosystem acting like an app store, then we must also address how we update our runtime. Whether it be bug fixes, general improvements to existing modules, or even new features that you want to add to your blockchain, the ability to change your runtime is something that Substrate has made a first class process.\\n\\nHowever, changes to the state transition function of your chain also impact consensus of the network. If one node running on your network has one version of your runtime logic, while another node has a different one, these two nodes will not be able to reach consensus with one another. They will fundamentally disagree on the true state of the ledger, resulting in what we defined earlier as a fork. These kinds of irreconcilable forks are bad because they reduce the security of your network, as only a subset of nodes will correctly create and verify new blocks.\\n\\n![Blockchain Hard Fork Upgrade](/assets/images/wis-upgrade.png)\\n\\nSubstrate has addressed this issue by having the network come to consensus about the runtime logic itself! Using the [Wasm](https://webassembly.org/) binary format, we are able to put the Substrate runtime code on the blockchain as part of the shared ledger. This means that anyone running a node is able to verify that their node has the latest logic. If it does not, then it will instead execute the on-chain Wasm directly! This means runtime upgrades to your blockchain can happen in real time, on a live network, without creating forks!\\n\\nIn the spirit of Substrate\'s flexibility, you do not need to enable this feature at all. If you want to disable on-chain upgrades, you can. Truly, Substrate provides you with all the tools needed to create a living, breathing blockchain.\\n\\n## Free, Open-Source, Production-Ready\\n\\n> **Note:** At the time of writing this post, Substrate is **not** production ready... but it almost is. Substrate is currently undergoing a security audit in preparation of a 2020 release of the Polkadot network.\\n\\nSubstrate is a completely free and [**open-source**](https://github.com/paritytech/substrate/blob/master/LICENSE) project. It is built using the [Rust programming language](https://www.rust-lang.org/), which is designed for creating fast and inherently safe software. Coordination and development of Substrate happens through public communities like [GitHub](https://github.com/paritytech/substrate) and [Riot](https://riot.im/app/#/room/!HzySYSaIhtyWrwiwEV:matrix.org), with over 100 individual contributors.\\n\\nSubstrate is a project born from [Polkadot](https://polkadot.network/), a larger vision of a world with many interoperable blockchains. Substrate powers the blockchain that connects this public network in addition to most of the chains that will be connected to it. You can feel secure that the technology that backs your blockchain is the same technology that powers multiple other production-level blockchains.\\n\\nSubstrate aims to be the absolute best platform for blockchain innovators, and the natural choice for anyone who is thinking about building a blockchain.\\n\\n## Summary\\n\\nAt this point, I hope that you can see why we say that Substrate an **extensible**, **modular**, and **open-source** platform for building blockchain systems. At every point in the Substrate development process, keeping things generic has remained a priority. As a result, Substrate can be used as a platform to build future technologies, even those that are not yet thought of.\\n\\nIf you enjoy this content and want to see more, consider taking a look at my [donations page](https://shawntabrizi.com/donate/) to see how you can support me."},{"id":"/substrate/substrate-feeless-token-factory/","metadata":{"permalink":"/blog/substrate/substrate-feeless-token-factory/","source":"@site/blog/2019-08-28-substrate-feeless-token-factory.md","title":"Substrate Feeless Token Factory","description":"In this post, I will share the Hackathon project I worked on for ETHBerlin 2019, where we built a Substrate blockchain that supports generating fungible tokens that can be transferred without end-users paying fees.","date":"2019-08-28T00:00:00.000Z","tags":[{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"module","permalink":"/blog/tags/module"},{"inline":true,"label":"erc20","permalink":"/blog/tags/erc-20"},{"inline":true,"label":"fee","permalink":"/blog/tags/fee"},{"inline":true,"label":"gas","permalink":"/blog/tags/gas"},{"inline":true,"label":"token","permalink":"/blog/tags/token"},{"inline":true,"label":"ethberlin","permalink":"/blog/tags/ethberlin"},{"inline":true,"label":"hackathon","permalink":"/blog/tags/hackathon"}],"readingTime":12.34,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Substrate Feeless Token Factory","date":"2019-08-28T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/substrate-feeless-token-factory/","categories":["Substrate"],"tags":["runtime","module","erc20","fee","gas","token","ethberlin","hackathon"],"github":"substrate-feeless-token-factory"},"unlisted":false,"prevItem":{"title":"What is Substrate?","permalink":"/blog/substrate/what-is-substrate/"},"nextItem":{"title":"Querying Substrate Storage via RPC","permalink":"/blog/substrate/querying-substrate-storage-via-rpc/"}},"content":"##### In this post, I will share the [Hackathon project](https://github.com/shawntabrizi/substrate-feeless-token-factory) I worked on for ETHBerlin 2019, where we built a Substrate blockchain that supports generating fungible tokens that can be transferred without end-users paying fees.\\n\\nEthereum has shown itself to be the ultimate platform for building token economies. Thousands of contracts have been created which support standards like ERC20 and ERC721.\\n\\nHowever, businesses using Ethereum have struggled adopting new users into the ecosystem due to the upfront costs of Gas to interact with these token contracts. Many newcomers do not understand why they need ETH to be able to interact with other tokens they actually are interested in.\\n\\nBusinesses have shown that they would be more than happy to fund the usage of their users. Some have done this by providing a faucet or ETH drop to their users, while others have implemented layer 2 solutions or have made compromises building centralized solutions.\\n\\n## A \\"Feeless\\" Token Factory\\n\\nThe [Substrate Feeless Token Factory](https://github.com/shawntabrizi/substrate-feeless-token-factory) (SFTF) removes the pains and costs of token transfer fees from end users, by offloading those costs onto the token creators and community contributors who want to support a particular token.\\n\\nTransaction fees are used as a mechanism to prevent denial-of-service attacks on public blockchain systems. SFTF is a blockchain level protocol, developed on top of the Substrate blockchain framework, which provides an alternative mechanism for transfer fees on tokens, while still preventing nominal attack vectors. In short, each token built on the network is backed by a fund of the native blockchain currency. This fund is ultimately used to pay for the transfers of that token, on behalf of the user.\\n\\nAny user can deposit funds into the pot for a token, but we think that most often, it will be the token creator who will be most incentivized to fund their users to transfer and spend their tokens. Users are enabled only a certain number of free transfers per token within a given time period. These transfers are enabled by an extension of the standard ERC20 token API, introducing a `try_free_transfer` function which can allow a user to make a free transfer if underlying conditions are met.\\n\\nLet\'s take a look at a hypothetical example: the \\"Better Energy Foundation\\" wants to issue a new token to be used as electricity credits. When they do this, they fund the token with an initial supply of the underlying blockchain currency (10,000,000 units), and specify that the users of their token have 10 free transactions every 1,000 blocks. They can sell their tokens and transfer them to the buyers just like a normal ICO. These buyers can then call the `try_free_transfer` function when trying to trade the token among their peers, and the fees are paid for using the fund. Assuming the underlying transfer fee being charged to the pot is 1 unit per transfer, the \\"Better Energy Foundation\\" has just supported millions of \\"free\\" transfers of their token. Anyone in the community can continue to add more funds, and allow the free transfers to continue when the funds start to dry up. If a user does not have any more \\"free\\" transactions left, or if the fund is empty, they can always make a transaction using the normal `transfer` function which will charge them a normal transaction fee directly from their own account.\\n\\n## Design\\n\\nFirst we designed a runtime module which acts as a normal ERC20 compatible token _factory_, this mean the module supports the creation of any number of different fungible tokens. This was mostly based on the [`srml-assets` module](https://github.com/paritytech/substrate/tree/master/frame/assets), but extended to expose an API which matches that of an ERC20 token: transfers on-behalf-of and allowances. We kept the token factory constructor simple by having the user mint all token up front into their own account, and controlling distribution manually.\\n\\nWithout touching any of the fee mechanisms, this module is basically a replacement for any number of ERC20 token contracts, but built at the Substrate runtime level, which should be more efficient and cost effective for everyone.\\n\\n### Removing Fees\\n\\nAt that point, we needed to remove fees from the runtime module. At the moment, Substrate runtime fees are controlled in two areas:\\n\\n1. The [Balances module](https://github.com/paritytech/substrate/blob/v1.0/srml/balances/src/lib.rs), which defines a `TransactionBaseFee` and `TransactionByteFee`.\\n2. The [weight annotation](https://github.com/paritytech/substrate/pull/3157), which allows you to control fees for an individual runtime function.\\n\\nWe configured our runtime such that both the `TransactionBaseFee` and the `TransactionByteFee` would be set to 0:\\n\\n```\\npub const TransactionBaseFee: u128 = 0;\\npub const TransactionByteFee: u128 = 0;\\n```\\n\\n> NOTE: I think in the long term, the fees within the Balances module will be completely removed in favor of weight annotations. For the time being, weight annotations do not support the concept of \\"per byte fee\\", which is why I think it is still around.\\n\\nOriginally, we thought this might be all that is needed to remove fees from our Runtime, however, if we do not specify a weight annotation for a runtime function, it is automatically assigned a default value:\\n\\n[**sr-primitives/weights**](https://github.com/paritytech/substrate/blob/master/frame/support/src/weights.rs)\\n\\n```rust\\nimpl Default for SimpleDispatchInfo {\\n  fn default() -> Self {\\n    // Default weight of all transactions.\\n    SimpleDispatchInfo::FixedNormal(10_000)\\n  }\\n}\\n```\\n\\nSo what we actually need to do is label our function(s) explicitly with a zero weight tag:\\n\\n```rust\\n#[weight = SimpleDispatchInfo::FixedNormal(0)]\\nfn my_free_function(origin, ...) { ... }\\n```\\n\\nWe really only wanted to remove fees from the `transfer` function, not necessarily the other parts of the ERC20 API, so we decided to create a new function `try_free_transfer` which would have a fee of zero, and the additional functionality needed to protect our chain from malicious attacks.\\n\\n### Protecting Our Chain\\n\\nThe whole reason there are transfer fees on the blockchain is to protect the network from attackers who would be able to perform a denial-of-service attacks and spam the network with transactions. So, if we choose to remove the base fees from our transfer function, we would need to implement a new solution to prevent attacks to our chain.\\n\\nHere is where you could get very clever, and someone with much more research and knowledge into game theory would probably come up with some really interesting solutions. But we were not those people, and we only had 1 day to build our solution, so we addressed this issue in the simplest way we could think of.\\n\\nWe know that businesses and organizations are the number one user of creating these tokenized assets. Usually they make a huge profit through an ICO and further development of their company. As a result, we predict in most cases they would happily eat any and all costs of fees related to using their token. So, we created an open fund for each token, where the standard token fee will be burned from that fund rather than the individuals who are transferring tokens. While this pot can obviously be funded by the token creators, we allowed open contributions to the pot in order to allow any community members who want to support a token to be able to do so.\\n\\nTo support this new functionality, we continued to develop our API to support an initial contribution when the token is created, and a `deposit` function allowing users to place funds in the pot for a token. If at any point the funds for a token is depleted, the fund can be replenished through new deposits. Futhermore, because we created a separate function for free transfers, we can still support the standard `transfer` function which has a standard fee and allows users to still use a token even when the fund is zero. Really, all these feeless transfer stuff is all extra functionality that is built on top of the token factory, which already works on its own!\\n\\nOne last point of friction we introduced was a limit per user, per token, per time period on how many transfers can be made. This prevents a malicious user from simply spending all of the pot by making frivolous transfers. This mechanism is still vulnerable to a [Sybil attack](https://en.wikipedia.org/wiki/Sybil_attack), where you can imagine an attacker generates millions of accounts and has account 1 send tokens to account 2, who sends tokens to account 3, etc...\\n\\nHowever, this is hopefully thwarted by the need for an existential deposit of the base blockchain currency to make an active account. This existential deposit limit could be adjusted in order to provide the needed friction to prevent a single user from having too many accounts. Again, this initial existential deposit could be provided by the companies that want to support their users to use their tokens, and since there are no fees, only the minimum amount is needed to be given to a user, and only one time per user.\\n\\nI think that the design here has lots of room for improvement, and there are likely a lot of different ways we could prevent malicious attacks on individual token funds, but for the hackathon, I felt that this was a reasonable first step.\\n\\n## Substrate Patterns\\n\\nHaving gone over all the details of how we designed the SFTF, I want to review a few of the specific implementation details which hopefully convey reusable design patterns for other Substrate runtime modules.\\n\\n### Module Funds\\n\\nThe main aspect of our feeless token is creating a fund for each token. Ultimately these funds will need to be controlled and managed by our module.\\n\\nTo do this, we create a unique identifier for our module, and use this to generate new accounts for the funds:\\n\\n```rust\\nconst MODULE_ID: ModuleId =ModuleId(*b\\"coinfund\\");\\n\\nimpl<T: Trait> Module<T> {\\n  pub fn fund_account_id(index: T::TokenId) -> T::AccountId {\\n    MODULE_ID.into_sub_account(index)\\n  }\\n}\\n```\\n\\nUsing the `into_sub_account` function, we can actually use the unique module id we created to generate any number of unique `AccountIds` which can then represent the funds for each of the tokens.\\n\\nTo then fund these accounts, we simply call the Balances module\'s `transfer` function, just like you would transfer to any other account:\\n\\n```rust\\nfn deposit(origin, #[compact] token_id: T::TokenId, #[compact] value: BalanceOf<T>) {\\n  let who = ensure_signed(origin)?;\\n  ensure!(Self::count() > token_id, \\"Non-existent token\\");\\n  T::Currency::transfer(&who, &Self::fund_account_id(token_id), value)?;\\n\\n  Self::deposit_event(RawEvent::Deposit(token_id, who, value));\\n}\\n```\\n\\nSince funds for different tokens are separated, we don\'t really need to do fancy tracking of the funds for each account. Instead, we can rely on the Balances module to do that for us! In our `try_free_transfer` function, we do the following:\\n\\n```rust\\n// Burn fees from funds\\nlet fund_account = Self::fund_account_id(id);\\nlet fund_fee = T::FundTransferFee::get();\\nlet _ = T::Currency::withdraw(&fund_account, fund_fee, WithdrawReason::Transfer, ExistenceRequirement::AllowDeath)?;\\n```\\n\\nIf the `withdraw` call fails, then the token does not have enough funds, and we simply fail to complete the `try_free_transfer`. Easy as pie.\\n\\n### Tracking Transfers Per Time Period\\n\\nOne of the more challenging tricks we had to implement for this runtime module was a storage structure which would allow us to track how many time each user transferred a particular token in a given time period. If we simply wanted to count the total number of transfers, we would be able to create a regular map like so:\\n\\n```rust\\nTotalTransferCount get(total_transfer_count): map (T::TokenId, T::AccountId) => u64;\\n```\\n\\nHowever, since we want to reset this count after a certain time period, this is not good enough. To use this regular map in this way, we would need to track each user which spent tokens in a time period, and which tokens they spent, and then we would need to do some unbounded loop over this mapping in order to clear all the entries. This is a big no-no.\\n\\nThe trick here is to take advantage of the `StorageDoubleMap`, which is just a map nested within a map. Most importantly, the `StorageDoubleMap` API provides the ability to clear all entries under a key in the top level map through `remove_prefix`. This means that I simply need to create a `double_map` where the first key is \\"fixed\\" (essentially treating the `double_map` as a regular `map`), and then call `remove_prefix` on that fixed first key when I want to clear all entries. This will clean up all of the data in our map without having to do a loop, which we know is generally a runtime sin.\\n\\nHere is what the double map declaration looks like:\\n\\n```rust\\nFreeTransferCount get(free_transfer_count): double_map (), blake2_128((T::TokenId, T::AccountId)) => T::TokenFreeTransfers;\\n```\\n\\nWhen we want to keep track of the user\'s free transfers, we simply update the storage item like so:\\n\\n```rust\\nlet free_transfer_count = Self::free_transfer_count(&(), &(id, sender.clone()));\\nlet new_free_transfer_count = free_transfer_count\\n  .checked_add(&One::one()).ok_or(\\"overflow when counting new transfer\\")?;\\n\\n...\\n\\n<FreeTransferCount<T>>::insert(&(), &(id, sender), &new_free_transfer_count);\\n```\\n\\nFinally, when we want to clean up all the tracking and start fresh, we simply call the magical `remove_prefix` API:\\n\\n```rust\\n// This function is called at the beginning of every block\\nfn on_initialize(n: T::BlockNumber) {\\n  // Check is `true` every `FreeTransferPeriod` number of blocks\\n  if n % T::FreeTransferPeriod::get() == Zero::zero() {\\n    // Reset everyone\'s free transfer count\\n    <FreeTransferCount<T>>::remove_prefix(&());\\n  }\\n}\\n```\\n\\nI would hope in the future, the `StorageMap` and `StorageDoubleMap` will implement a `kill` function like the `StorageValue` item has, which would allow a user to easily clear all entries of the mapping. It could even use this trick under the hood! However, it is unclear to me if there are significant costs to doing things this way. \xaf\\\\\\\\_(\u30c4)_/\xaf\\n\\n## Next Steps\\n\\nBecause the hackathon is so short, and we were teaching new developers to start building on Substrate, the full potential of this idea was not created, nor was it even conceived. There is so much more potential for exploring how Substrate can enable \\"feeless\\" token transfers given that you have full control at the runtime level of how your blockchain operates. This is not feature that you would get with any open smart contract platform.\\n\\nA few ideas which could further this project are:\\n\\n- Upgrade the token API to support ERC1155, thus also supporting non-fungible tokens.\\n- Allow payment of token transfers with the custom token rather than the underlying blockchain currency.\\n\\n  > We actually mostly did this in the SFTF by implementing `SignedExtension` for a custom `TakeTokenFees` struct. This has logic which transfers a token from the user to the block author, and increases the priority of the transaction. However, limitations of easily generating the correct extrinsic format meant that it would not work in time for the hackathon.\\n\\n- Allow a small proof of work to replace the cost of transferring the token.\\n- Create a ban list of users who are not allowed to use any free transfer funds.\\n- Build a decentralized token exchange module which supports tokens generated from the factory.\\n\\nDo you have good ideas? Open an issue on the [Substrate Feeless Token Factory](https://github.com/shawntabrizi/substrate-feeless-token-factory/) repository!\\n\\nAs always, if you enjoy this content and want to support me in continuing to write new posts, check out my [donations page](https://shawntabrizi.com/donate/)."},{"id":"/substrate/querying-substrate-storage-via-rpc/","metadata":{"permalink":"/blog/substrate/querying-substrate-storage-via-rpc/","source":"@site/blog/2019-07-28-interacting-with-the-substrate-rpc-endpoint.md","title":"Querying Substrate Storage via RPC","description":"In this post, we will investigate how you can interact with the Substrate RPC endpoint in order to read storage items from your Substrate runtime.","date":"2019-07-28T00:00:00.000Z","tags":[{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"module","permalink":"/blog/tags/module"},{"inline":true,"label":"rpc","permalink":"/blog/tags/rpc"},{"inline":true,"label":"metadata","permalink":"/blog/tags/metadata"},{"inline":true,"label":"storage","permalink":"/blog/tags/storage"},{"inline":true,"label":"scale","permalink":"/blog/tags/scale"},{"inline":true,"label":"hash","permalink":"/blog/tags/hash"}],"readingTime":9.435,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Querying Substrate Storage via RPC","date":"2019-07-28T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/querying-substrate-storage-via-rpc/","categories":["Substrate"],"tags":["runtime","module","rpc","metadata","storage","scale","hash"],"github":"substrate-rpc-examples","customjs":["https://www.shawntabrizi.com/substrate-rpc-examples/bundle.js"]},"unlisted":false,"prevItem":{"title":"Substrate Feeless Token Factory","permalink":"/blog/substrate/substrate-feeless-token-factory/"},"nextItem":{"title":"Extending Substrate Runtime Modules","permalink":"/blog/substrate/extending-substrate-runtime-modules/"}},"content":"##### In this post, we will investigate how you can interact with the Substrate RPC endpoint in order to read storage items from your Substrate runtime.\\n\\n> This post was updated to the latest changes in Substrate as of January 2020.\\n\\nMost of the posts I have written about Substrate so far have showed you how easy it is to build custom blockchains with this next generation framework. However, there is an entire set of parallel development and tools needed to enable users to easily interact with these new blockchain systems.\\n\\nOur ultimate goal in this post is to **query the balance of a Substrate user using the Substrate RPC**. Along the way, we will paint a better picture of how Substrate interacts with the outside world by investigating storage structures, hashing algorithms, encoding schemes, public endpoints, metadata, and more!\\n\\n## Substrate RPC Methods\\n\\nSubstrate provides a set of RPC methods by default which allow you to interact, query, and submit to the actual node. The available RPC methods that Substrate exposes are documented as part of the [Polkadot-JS docs](https://polkadot.js.org/api/substrate/rpc.html). Your node actually exposes this information behind another RPC endpoint: `rpc_methods`.\\n\\nTo query the balance of a Substrate user, we will need to read into the runtime storage of the Balances module. This is done by calling the `getStorage` method in `state`:\\n\\n```\\ngetStorage(key: StorageKey, block?: Hash): StorageData\\n\\nsummary: Retrieves the storage for a key\\n```\\n\\n> Note that specifying a `block` here is optional. By default, it will query the latest block.\\n\\nThe actual RPC method name is generated by combining the category with the documented function name, like so:\\n\\n- `state_getStorage`\\n\\nHowever, to start simple, we will first query the Metadata endpoint for our Substrate node, which requires only knowledge of the method name: `state_getMetadata`.\\n\\n## Substrate RPC Endpoint\\n\\nTo actually call these methods, you need access to a Substrate RPC endpoint. When you start a local Substrate node, two endpoints are made available to you:\\n\\n- HTTP Endpoint: http://localhost:9933/\\n- Websocket Endpoint: ws://localhost:9944/\\n\\nMost of the Substrate front-end libraries and tools use the more powerful WebSocket endpoint to interact with the blockchain. Through WebSockets, you can subscribe to various items, like events, and receive push notifications whenever changes in your blockchain occur.\\n\\nFor the purposes of this post, we will continue to keep things simple and use the HTTP endpoint to make JSON-RPC queries to our blockchain.\\n\\nLet\'s first use this endpoint to call the RPC methods endpoint:\\n\\n```bash\\n$ curl -H \\"Content-Type: application/json\\" -d \'{\\"id\\":1, \\"jsonrpc\\":\\"2.0\\", \\"method\\": \\"rpc_methods\\"}\' http://localhost:9933/\\n\\n> {\\"jsonrpc\\":\\"2.0\\",\\"result\\":{\\"methods\\":[\\"account_nextIndex\\",\\"author_insertKey\\",\\"author_pendingExtrinsics\\",\\"author_removeExtrinsic\\",\\"author_rotateKeys\\",\\"author_submitAndWatchExtrinsic\\",\\"author_submitExtrinsic\\",\\"author_unwatchExtrinsic\\",\\"chain_getBlock\\",\\"chain_getBlockHash\\",\\"chain_getFinalisedHead\\",\\"chain_getFinalizedHead\\",\\"chain_getHead\\",\\"chain_getHeader\\",\\"chain_getRuntimeVersion\\",\\"chain_subscribeFinalisedHeads\\",\\"chain_subscribeFinalizedHeads\\",\\"chain_subscribeNewHead\\",\\"chain_subscribeNewHeads\\",\\"chain_subscribeRuntimeVersion\\",\\"chain_unsubscribeFinalisedHeads\\",\\"chain_unsubscribeFinalizedHeads\\",\\"chain_unsubscribeNewHead\\",\\"chain_unsubscribeNewHeads\\",\\"chain_unsubscribeRuntimeVersion\\",\\"contracts_call\\",\\"state_call\\",\\"state_callAt\\",\\"state_getChildKeys\\",\\"state_getChildStorage\\",\\"state_getChildStorageHash\\",\\"state_getChildStorageSize\\",\\"state_getKeys\\",\\"state_getMetadata\\",\\"state_getRuntimeVersion\\",\\"state_getStorage\\",\\"state_getStorageAt\\",\\"state_getStorageHash\\",\\"state_getStorageHashAt\\",\\"state_getStorageSize\\",\\"state_getStorageSizeAt\\",\\"state_queryStorage\\",\\"state_subscribeRuntimeVersion\\",\\"state_subscribeStorage\\",\\"state_unsubscribeRuntimeVersion\\",\\"state_unsubscribeStorage\\",\\"subscribe_newHead\\",\\"system_accountNextIndex\\",\\"system_chain\\",\\"system_health\\",\\"system_name\\",\\"system_networkState\\",\\"system_nodeRoles\\",\\"system_peers\\",\\"system_properties\\",\\"system_version\\",\\"unsubscribe_newHead\\"],\\"version\\":1},\\"id\\":1}\\n```\\n\\nHere you see a full list of available RPC apis. We can try calling the Metadata endpoint:\\n\\n```bash\\n$ curl -H \\"Content-Type: application/json\\" -d \'{\\"id\\":1, \\"jsonrpc\\":\\"2.0\\", \\"method\\": \\"state_getMetadata\\"}\' http://localhost:9933/\\n\\n> {\\"jsonrpc\\":\\"2.0\\",\\"result\\":\\"0x6d65746107481853797374656d011853797374656d3c304163636f756e744e6f6e636501010130543a3a4163636f756e74496420543a3a496e64657800200000000000000000047c2045787472696e73696373206e6f6e636520666f72206163636f756e74732e3845787472696e736963436f756e...\\n```\\n\\nYay! A basic RPC call to get the metadata from Substrate is successful! However, you will notice the result is a _large_ hex value, which really isn\'t that helpful...\\n\\nThere is more to the story.\\n\\n## Substrate Encoding\\n\\nWhat we haven\'t touched on yet are the various encoding mechanisms used by Substrate to both optimize serialization of data, but also provide safeties to the blockchain system.\\n\\n### SCALE Codec\\n\\nIf we try to naively decode the hex returned from the metadata endpoint using JavaScript, we get something like:\\n\\n```javascript\\n// From StackOverflow question 3745666\\nfunction hex_to_string(metadata) {\\n  return metadata\\n    .match(/.{1,2}/g)\\n    .map(function (v) {\\n      return String.fromCharCode(parseInt(v, 16));\\n    })\\n    .join(\\"\\");\\n}\\n\\nhex_to_string(\\n  \\"0x6d65746107481853797374656d011853797374656d3c304163636f756e744e6f6e636501010130543a3a4163636f756e74496420543a3a496e64657800200000000000000000047c2045787472696e73696373206e6f6e636520666f72206163636f756e74732e3845787472696e736963436f756e...\\"\\n) >\\n  \\"\\\\u0000meta\\\\u0007H\\\\u0018System\\\\u0001\\\\u0018System<0AccountNonce\\\\u0001\\\\u0001\\\\u00010T::AccountId T::Index\\\\u0000 \\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0000\\\\u0004| Extrinsics nonce for accounts.8ExtrinsicCoun...\\";\\n```\\n\\nThere is real data in there! However, it is not well formed.\\n\\nTo correctly parse the metadata, you will need to become familiar with is [Parity\'s SCALE codec](https://github.com/paritytech/parity-scale-codec):\\n\\n> SCALE is a light-weight format which allows encoding (and decoding) which makes it highly suitable for resource-constrained execution environments like blockchain runtimes and low-power, low-memory devices.\\n\\nParity uses SCALE for a number of reasons. [Gav](https://github.com/gavofyork) mentioned that:\\n\\n- It does not use Rust STD, and thus can compile to Wasm.\\n- It is zero-copy and uses next to no memory on little-endian hardware for elementary numeric types.\\n- It is built to have great support in Rust for deriving codec logic for new types: just add `#[derive(Encode, Decode)]`.\\n- It is about as thin and lightweight as can be.\\n\\nUsing the SCALE codec and parsing the Substrate metadata could be it\'s own blog post, so I will not go much deeper here; I just wanted to point out the main encoding scheme used by Substrate, and which shows up in the examples we have done so far.\\n\\n### Storage Keys\\n\\nFor our goal, what we really want to learn is how to generate the storage keys for our various runtime storage items.\\n\\nSubstrate has a single key-value database for powering the entire blockchain framework. From this minimal data structure, additional abstractions can be constructed such as a [Merkle Patricia tree (\\"trie\\")](https://github.com/paritytech/trie) that is used throughout Substrate.\\n\\nAt a base level, to gain access to any runtime storage item, you simply need to know it\'s storage key for the core key-value database. To prevent key collisions, a special schema is used to generate keys for Runtime module storage items:\\n\\n- For storage values:\\n\\n  ```\\n  xxhash128(\\"ModuleName\\") + xxhash128(\\"StorageName\\")\\n  ```\\n\\n- For storage maps:\\n\\n  ```\\n  xxhash128(\\"ModuleName\\") + xxhash128(\\"StorageName\\") + blake256hash(\\"StorageItemKey\\")\\n  ```\\n\\n- For storage double maps:\\n\\n  ```\\n  xxhash128(\\"ModuleName\\") + xxhash128(\\"StorageName\\") + blake256hash(\\"FirstKey\\") + blake256hash(\\"SecondKey\\")\\n  ```\\n\\nThis constructs a \\"prefix trie\\", where all storage items for a module share a common prefix, where all storage keys under a storage item share a common prefix, and so on... This may not make a lot of sense right now, but we will do some practical examples below to hopefully clarify.\\n\\n> **Learn More:** Check out my deep-dive into Substrate storage [here](./2019-12-09-substrate-storage-deep-dive.md).\\n\\n## Querying Runtime Storage\\n\\nWe are almost to the finish line. Now that you know the different storage key encoding patterns, we can try to construct and query the runtime storage for a Substrate chain. Since you will need to use some cryptographic hash functions to try this yourself, I have loaded them for you on this blog post.\\n\\nOpen your browser console, and you will find utility functions under `util.*`, `util_crypto.*`, and `keyring.*`. These come from the [polkadot-js/common](https://polkadot.js.org/common/) and will give you access to the hash functions like `util_crypto.xxhashAsHex` or `util_crypto.blake2AsHex`.\\n\\n### Storage Value Query\\n\\nLet\'s start with a simple storage value, for instance getting the [Sudo user](https://substrate.dev/rustdocs/v1.0/srml_sudo/index.html) for a Substrate chain. The module name is `Sudo` and the storage item which holds the `AccountId` is named `Key`.\\n\\nThus we would do the following:\\n\\n```javascript\\nutil_crypto.xxhashAsHex(\\"Sudo\\", 128) > \\"0x5c0d1176a568c1f92944340dbfed9e9c\\";\\n\\nutil_crypto.xxhashAsHex(\\"Key\\", 128) > \\"0x530ebca703c85910e7164cb7d1c9e47b\\";\\n```\\n\\nSo the combined storage key would be:\\n\\n```\\n0x5c0d1176a568c1f92944340dbfed9e9c530ebca703c85910e7164cb7d1c9e47b\\n```\\n\\n> **Note:** Note that we use XXHash to output a 128 bit hash. However, XXHash only supports 32 bit and 64 bit outputs. To correctly generate the 128 bit hash, we need to hash the same phrase twice, with seed `0` and seed `1`, and concatenate them.\\n\\nNow we can form an RPC request using this value as the `params` when calling the `state_getStorage` endpoint:\\n\\n```bash\\n$ curl -H \\"Content-Type: application/json\\" -d \'{\\"id\\":1, \\"jsonrpc\\":\\"2.0\\", \\"method\\": \\"state_getStorage\\", \\"params\\": [\\"0x5c0d1176a568c1f92944340dbfed9e9c530ebca703c85910e7164cb7d1c9e47b\\"]}\' http://localhost:9933/\\n\\n> {\\"jsonrpc\\":\\"2.0\\",\\"result\\":\\"0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d\\",\\"id\\":1}\\n```\\n\\nSuccess! The result here is the SCALE encoded AccountID of the Sudo user:\\n\\n```javascript\\nkeyring.encodeAddress(\\n  \\"0xd43593c715fdd31c61141abd04a99fd6822c8558854ccde39a5684e7a56da27d\\"\\n) > \\"5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY\\";\\n```\\n\\nThis is the familiar `Alice` account which we would expect on a `--dev` chain, and also matches what we get using the [Polkadot-JS UI](https://polkadot.js.org/apps/#/chainstate):\\n\\n![Sudo Key for the Substrate `--dev` node](/assets/images/sudo-key-dev-node.png)\\n\\n### Storage Map Query\\n\\n> **Note:** The construction of storage keys for maps has slightly changed from when this was written. You can find an up to date version of that key construction in my post [\\"Transparent Keys in Substrate\\"](./2020-03-29-transparent-keys-in-substrate.md).\\n\\nAs a final challenge, we will look to query a storage map like the balance of an account. The module name is `Balances` and the storage item we are interested in is named `FreeBalance`. They mapping for this storage item is from `AccountId -> Balance`, so the storage item key we want to use is an `AccountId`.\\n\\nWe need to follow the same pattern as before, but append to the end the hash of the `AccountId`:\\n\\n```javascript\\nutil_crypto.xxhashAsHex(\\"Balances\\", 128) > \\"0xc2261276cc9d1f8598ea4b6a74b15c2f\\";\\n\\nutil_crypto.xxhashAsHex(\\"FreeBalance\\", 128) >\\n  \\"0x6482b9ade7bc6657aaca787ba1add3b4\\";\\n\\nutil_crypto.blake2AsHex(\\n  keyring.decodeAddress(\\"5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY\\"),\\n  256\\n) > \\"0x2e3fb4c297a84c5cebc0e78257d213d0927ccc7596044c6ba013dd05522aacba\\";\\n```\\n\\nSo the final storage key in this case is:\\n\\n```bash\\n0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b42e3fb4c297a84c5cebc0e78257d213d0927ccc7596044c6ba013dd05522aacba\\n```\\n\\nJust like before, we can form an RPC request using this value as the `params`:\\n\\n```bash\\n$ curl -H \\"Content-Type: application/json\\" -d \'{\\"id\\":1, \\"jsonrpc\\":\\"2.0\\", \\"method\\": \\"state_getStorage\\", \\"params\\": [\\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b42e3fb4c297a84c5cebc0e78257d213d0927ccc7596044c6ba013dd05522aacba\\"]}\' http://localhost:9933\\n\\n{\\"jsonrpc\\":\\"2.0\\",\\"result\\":\\"0x0000a0dec5adc9353600000000000000\\",\\"id\\":1}\\n```\\n\\nThe result here is now a SCALE encoded version of the `Balance` type, which is a u64 and thus trivially decodable (now that you know it is little endian):\\n\\n```javascript\\nutil.hexToBn(\\"0x0000a0dec5adc9353600000000000000\\", { isLe: true }).toString() >\\n  \\"1000000000000000000000\\";\\n```\\n\\nWoohoo!\\n\\n## Prefix Tries\\n\\nHopefully, you should be able to see they this storage key generation forms \\"prefix tries\\".\\n\\nLet\'s say you wanted to query another user\'s balance. Well the construction of the key would make the first 256 bits exactly the same!\\n\\n```bash\\n# All Balances -> FreeBalance storage keys start with\\n0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4\\n```\\n\\nThis means you could actually use the `state_getKeys` API to get all the storage keys for all the free balances in your system!\\n\\n```bash\\ncurl -H \\"Content-Type: application/json\\" -d \'{\\"id\\":1, \\"jsonrpc\\":\\"2.0\\", \\"method\\": \\"state_getKeys\\", \\"params\\": [\\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4\\"]}\' http://localhost:9933\\n\\n> {\\"jsonrpc\\":\\"2.0\\",\\"result\\":[\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4024cd62ab7726e039438193d4bbd915427f2d7de85afbcf00bd16fadbcad6aed\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b42e3fb4c297a84c5cebc0e78257d213d0927ccc7596044c6ba013dd05522aacba\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b44724e5390fcf0d08afc9608ff4c45df257266ae599ac7a32baba26155dcf4402\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b454b75224d766c852ac60eb44e1329aec5058574ae8daf703d43bc2fbd9f33d6c\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b465d0de2c1f75d898c078307a00486016783280c8f3407db41dc9547d3e3d651e\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b46b1ab1274bcbe3a4176e17eb2917654904f19b3261911ec3f7a30a473a04dcc8\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b477d14a2289dda9bbb32dd9313db096ef628101ac5bbb3b19301ede2c61915b89\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4927407fbcfe5afa14bcfb44714a843c532f291a9c33612677cb9e0ae5e2bd5de\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b494772f97f5f6b539aac74e798bc395119f39603402d0c85bc9eda5dfc5ae2160\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b49a9304efeee429067b2e8dfbcfd8a22d96f9d996a5d6daa02899b96bd7a667b1\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b49ea52149af6b15f4d523ad4342f63089646e29232a1777737159c7bc84173597\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4a315ee9e56d2f3bb24992a1cff6617b0f7510628a15722b680c42c2be8bb7452\\",\\n  \\"0xc2261276cc9d1f8598ea4b6a74b15c2f6482b9ade7bc6657aaca787ba1add3b4c4a80eb5e32005323fb878ca749473d7e5f40d60ed5e74e887bc125a3659f258\\"],\\"id\\":1}\\n```\\n\\nThis basically allows you to enumerate across all the balances in a Substrate blockchain! Although, you would not necessarily know the `AccountId` for these balances...\\n\\n> **Note:** Now you CAN figure out the `AccountId` for all these balances! Learn how in my post [\\"Transparent Keys in Substrate\\"](./2020-03-29-transparent-keys-in-substrate.md).\\n\\n## Next Steps\\n\\nIf you made it this far, you probably have come to the same conclusion as me, which is that interacting with the Substrate RPC is not trivial. Substrate is optimized for performance, bandwidth, and execution, which leaves tasks like encoding and decoding of transactions, storage, metadata, etc... to the outside world.\\n\\nThat being said, once you are able to walk through these examples step by step, I think it becomes easier to understand what is going on, and even reproduce this logic on other platforms and languages. Certainly this is needed for the future Substrate ecosystem.\\n\\nI have started a project called Substrate RPC Examples:\\n\\n[https://github.com/shawntabrizi/substrate-rpc-examples](https://github.com/shawntabrizi/substrate-rpc-examples)\\n\\nThe idea of this project is to provide some easy to read, \\"minimal library magic\\" examples of interacting with the Substrate RPC. So far, I have only used the tools available in `util`, `util_crypto`, and `keyring`, and ideally this can be reduced by introducing a few hand written functions.\\n\\nThe two samples I have described in this blog post (getting metadata, querying storage) are implemented. I hope to also add to it an example of a balance transfer, which will show how to sign a message. If you have any good ideas or examples that you would want to share with the world, feel free to open a [PR](https://github.com/shawntabrizi/substrate-rpc-examples/pulls).\\n\\nI think the next follow up from this post should be a deep dive into the SCALE codec and how you can turn the Metadata you receive from a Substrate node into valid JSON.\\n\\nAs always, if you are enjoying the content I have produced, take a look at my [donations page](https://shawntabrizi.com/donate/) to see how you can continue to support me."},{"id":"/substrate/extending-substrate-runtime-modules/","metadata":{"permalink":"/blog/substrate/extending-substrate-runtime-modules/","source":"@site/blog/2019-06-27-extending-substrate-runtime-modules.md","title":"Extending Substrate Runtime Modules","description":"In this post, I will show you how you can extend the SRML Contracts module to add additional authorization layers to your smart contract blockchain.","date":"2019-06-27T00:00:00.000Z","tags":[{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"module","permalink":"/blog/tags/module"},{"inline":true,"label":"rust","permalink":"/blog/tags/rust"},{"inline":true,"label":"sudo","permalink":"/blog/tags/sudo"},{"inline":true,"label":"contracts","permalink":"/blog/tags/contracts"},{"inline":true,"label":"ink","permalink":"/blog/tags/ink"}],"readingTime":15.505,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Extending Substrate Runtime Modules","date":"2019-06-27T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/extending-substrate-runtime-modules/","categories":["Substrate"],"tags":["runtime","module","rust","sudo","contracts","ink"],"github":"sudo-contract"},"unlisted":false,"prevItem":{"title":"Querying Substrate Storage via RPC","permalink":"/blog/substrate/querying-substrate-storage-via-rpc/"},"nextItem":{"title":"Adding Fees to Your Substrate Runtime Module","permalink":"/blog/substrate/adding-fees-to-your-substrate-runtime-module/"}},"content":"##### In this post, I will show you how you can extend the SRML Contracts module to add additional authorization layers to your smart contract blockchain.\\n\\nOne of the best things about Substrate is the ability to easily execute on your ideas when developing blockchain systems. I want to show you a question from the first Substrate Developer Conference (Sub0) that lead to me investigating how one might extend the functionality of a runtime module with a \\"wrapper module\\".\\n\\nBefore we can jump in though, you will need to know a little about how the Substrate Contract module works.\\n\\n## Background\\n\\nThe [Contract module](https://substrate.dev/rustdocs/v1.0/srml_contract/index.html) is included in the Substrate Runtime Module Library (SRML) and provides your blockchain with the ability to execute Wasm smart contracts.\\n\\nThere is a two step process for deploying a smart contract using the Contract module:\\n\\n1. Putting the WebAssembly smart contract code on the blockchain.\\n2. Creating an instance of a smart contract with a new Contract account.\\n\\nThis has a major advantage over existing smart contract platforms since you are able to create multiple instances of the same smart contract without needing to waste additional space with multiple instances of the contract code. For example, on Ethereum, each and every ERC-20 token uploads their own version of the ERC-20 contract. In Substrate, and with the Contracts module, a single ERC-20 Wasm smart contract can be uploaded, and many people can deploy their own tokens using customizable deployment parameters like initial balance, token name, etc...\\n\\n## Permissioned Access\\n\\nSo now that you are familiar with how to deploy contracts using the Contract module, let\'s hear the question that was asked at Sub0:\\n\\n<iframe width=\\"720px\\" height=\\"480px\\" src=\\"https://www.youtube.com/embed/-EJHu0u6hT8?start=6405&end=6573\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n> Question: \\"Can you restrict which accounts can add code to the blockchain?\\"\\n\\n> Sergei: \\"Maybe you can write another module which just wraps the smart contract module, and just does this additional check...\\"\\n\\nWhile there are a few different ways you could approach solving this problem, it turns out [Sergei](https://github.com/pepyakin), as always, was absolutely correct about the best approach.\\n\\nOne _could_ copy the entire Contract module and make changes directly to the source code (as I had originally suggested), but that means that any future updates and improvement to the Contract module would need to get added back into your fork of the module manually. This is definitely not a recommended approach, and one that I believe most users will naturally avoid anyway.\\n\\nRather, creating a \\"wrapper module\\" which somehow applies itself on top of the existing SRML Contract module, but allows for additional logic to be added, would be the best here. It would create clear separation between the vanilla module and the changes made by the end user, and would allow for the module to automatically stay up to date with the latest changes to Substrate.\\n\\nSo how would we do this?\\n\\n## Creating Sudo Contract\\n\\nI have created a Substrate runtime module called [`sudo-contract`](https://github.com/shawntabrizi/sudo-contract) which, as suggested, wraps the SRML Contract module, and provides a simple example on how you might execute similar wrapper modules.\\n\\nAs the name implies, `sudo-contract` uses both the SRML Sudo module and the SRML Contract module to make it so that only the \\"Sudo key\\" can put contract code on the blockchain. We did not change any other logic though, so there are no limits on who can deploy or call an instance of this smart contract. This combination of authorization to `put_code`, with open access to `create` and `call` enables for some practical use cases.\\n\\nFor example, imagine a DeFi (decentralized finance) platform controlled by a trusted smart contract development team like [OpenZeppelin](https://github.com/OpenZeppelin). This team would be able to provide a number of standardized, audited, and safe contracts for their users like an ICO contract, ERC-20 Contract, Multi-Signature Contract, etc... Users of this platform can choose from any of the standard contracts provided by the authorized team, and make their own instance of it for their needs.\\n\\nWith `sudo-contract`, this team would be able to tell their users that all smart contracts on their blockchain have been created and audited by their team. Thus, users can feel safe that there is no malicious code, backdoors, or generally broken contracts when using this platform.\\n\\nThis could provide a safer, and more consistent experience to all parties trying to take part in a larger decentralized financial system. Hopefully, you can imagine that the same can be done for other classes of smart contracts too!\\n\\nSo now that you are convinced of the utility of such a module, let\'s show you how you can build it.\\n\\n### Wrapping a Module\\n\\nThe `sudo-contract` module needs to provide all the same functionalities of the SRML Contract module, but have additional authorization checks around just one of the functions: `put_code`.\\n\\nAs Sergei suggested, the best way to approach this is to write a \\"wrapper module\\", which basically means a module which exposes the same extrinsic calls as the Contract module and forwards those calls to the real Contract module.\\n\\nFor example, the Contract module exposes 4 dispatchable functions:\\n\\n- `update_schedule`\\n- `put_code`\\n- `create`\\n- `call`\\n\\n> **Note:** We do not include special functions like `on_initialize`, `on_finalize`, `deposit_event`, etc... Only the ones which can be called via an extrinsic.\\n\\nIn our `sudo-contract` module, one of these \\"wrapper functions\\" will look like this:\\n\\n```rust\\n/// Simply forwards to the `create` function in the Contract module.\\nfn create(\\n    origin,\\n    #[compact] endowment: BalanceOf<T>,\\n    #[compact] gas_limit: T::Gas,\\n    code_hash: CodeHash<T>,\\n    data: Vec<u8>\\n) -> Result {\\n    <contract::Module<T>>::create(origin, endowment, gas_limit, code_hash, data)\\n}\\n```\\n\\nAll I have done here is copy the function signature for the `create` function, and then passed those parameters to the real `<contract::Module<T>>::create` function. You would do the same thing with each function until you have essentially created a \\"wrapped\\" module!\\n\\n### Adding Authorization Checks\\n\\nCreating a wrapper module like we have done above is not very useful as is, but using this pattern, we now have the ability to execute some additional logic before or after the main SRML Contract functions!\\n\\nOur goal here is to add some additional authorization logic before the `put_code` function, where only the Sudo key can call this function. That is actually really easy and can be done like so:\\n\\n```rust\\n/// Checks that sender is the Sudo `key` before forwarding to `put_code` in the Contract module.\\nfn put_code(\\n    origin,\\n    #[compact] gas_limit: T::Gas,\\n    code: Vec<u8>\\n) -> Result {\\n    let sender = ensure_signed(origin)?;\\n    ensure!(sender == <sudo::Module<T>>::key(), \\"Sender must be the Sudo key to put_code\\");\\n    let new_origin = system::RawOrigin::Signed(sender).into();\\n    <contract::Module<T>>::put_code(new_origin, gas_limit, code)\\n}\\n```\\n\\nHere, we simply call into the Sudo module\'s storage to retrieve who the current Sudo key is, and check that the sender matches that key. If that check fails, we never make the downstream call to the SRML Contract module to actually put the code on the chain. Instead the module will return a runtime error:\\n\\n> Sender must be the Sudo key to put_code\\n\\nand nothing will happen. It really is that easy!\\n\\n### Other Details\\n\\nI won\'t go into a line by line instruction of creating the module, but there a few details I want to call out so that even new runtime developers can understand how some things work.\\n\\n#### Accessing Other Modules\\n\\nIn my wrapper module, I have dependencies on both the SRML Contract module and SRML Sudo module. You can see in my code, I reference both modules to either call their functions or to read their storage. I am able to do this because I have imported these dependencies in my module\'s `Cargo.toml` file, and I have inherited the modules\' traits into my own:\\n\\n```rust\\npub trait Trait: contract::Trait + sudo::Trait {}\\n```\\n\\nYou will need to do this for any modules you wrap, and in our case this also implies that your runtime must use exactly these two modules.\\n\\nWe may look to revisit a wrapper module which does not depend on any specific module, but just one that has the traits, functions, and types expected. You could imagine this would be useful if another Contract module was released with alternative implementation details, but ultimately the same API. We would want our wrapper module to work for that module too!\\n\\n#### Calling Private Dispatchable Functions\\n\\nA critical detail which makes this solution work so \\"easily\\" is that the dispatchable functions for the Contract module are marked `pub`, which means I can call them directly from my wrapper module. However, this is not a requirement for making a wrapper module since all dispatchable functions are made explicitly public through the `Call` type. What this really means is that your module can always \\"dispatch\\" a transaction to another module\'s function!\\n\\nHere is an example of what that might look like:\\n\\n```rust\\nlet result = contract::Call::<T>::update_schedule(schedule).dispatch(origin).is_ok();\\n```\\n\\nThis is exactly the same as calling the function directly, so no extra transactions will be recorded, no extra fees taken, etc...\\n\\n## Adding Sudo Contract\\n\\nSo I have already done the work for you to create the `sudo-contract` wrapper module. Now I want to share with you some of the nuances of adding it to your smart contract enabled runtime.\\n\\n> **Note:** If you want to add the `sudo-contract` module to your runtime, you should follow the [README](https://github.com/shawntabrizi/sudo-contract) included with the module, since the next couple of sections may leave out smaller details.\\n\\n### Substrate Dependencies\\n\\nI won\'t go into details about the challenges in creating a Substrate module as its own Rust library, but one thing you will need to be conscious of is the specific Substrate dependencies used by your runtime code.\\n\\nIn the `v1.0` branch of the `sudo-contract` module, I point all Substrate dependencies to the `v1.0` branch of Substrate. This means your runtime must **also** have all of its substrate dependencies point to the `v1.0` branch too. If your runtime is pointing to a specific git commit or a different branch, you will either need to update your runtime code or fork my wrapper module and update it to use exactly the same dependency.\\n\\nYou will also need to be sure to add this module to your runtime\'s `std` feature, so that it will use `std` features when building the native binaries for your runtime.\\n\\nMore details can be found in README for the `sudo-contract` module and the HOWTO of the [`substrate-module-template`](https://github.com/shawntabrizi/substrate-module-template), which was used to create the `sudo-contract` module.\\n\\n### Tricking the Polkadot UI\\n\\nThe next challenge we will overcome is how to trick the Polkadot UI into thinking our `sudo-contract` module, with the same public API, is the \\"real\\" Contract module of my chain.\\n\\nThe Polkadot UI is actually really simple when it comes to enabling and routing logic to module specific UI like the Contract module. All it does is check in the runtime metadata that the `name` of the module matches what it expects.\\n\\nThis metadata value is generated from the Rust dependency name chosen when importing the module into your project. So in the case of the SRML Contract module, you would normally import the module like this:\\n\\n```\\n[dependencies.contract]\\ndefault_features = false\\ngit = \'https://github.com/paritytech/substrate.git\'\\npackage = \'srml-contract\'\\nbranch = \'v1.0\'\\n```\\n\\nWhich would give it `\\"name\\": \\"contract\\"` in the metadata, which is exactly what the UI expects. So, we would be able to trick the UI by taking advantage of this and importing our module with the `contract` dependency name, and renaming the SRML Contract module:\\n\\n```\\n[dependencies.srml-contract]\\ndefault_features = false\\ngit = \'https://github.com/paritytech/substrate.git\'\\npackage = \'srml-contract\'\\nbranch = \'v1.0\'\\n\\n[dependencies.contract]\\ndefault_features = false\\ngit = \'https://github.com/shawntabrizi/sudo-contract.git\'\\npackage = \'sudo-contract\'\\nbranch = \'v1.0\'\\n```\\n\\n> **Note** that we imported our `sudo-contract` package from the `v1.0` branch, and our Substrate based depdencies are also coming from their `v1.0` branch, as described in the previous section.\\n\\nAfter making this change and updating our `std` feature appropriately, we will also need to update our runtime\'s `lib.rs` file.\\n\\nAny references to the `contract` dependency which is intended for `srml-contract` will need to be updated, like the trait implementations:\\n\\n```rust\\n// This is the srml-contract Trait\\nimpl srml_contract::Trait for Runtime {\\n\\ttype Currency = Balances;\\n\\ttype Call = Call;\\n\\ttype Event = Event;\\n\\ttype Gas = u64;\\n    // Note the updated names in these lines too!\\n\\ttype DetermineContractAddress = srml_contract::SimpleAddressDeterminator<Runtime>;\\n\\ttype ComputeDispatchFee = srml_contract::DefaultDispatchFeeComputor<Runtime>;\\n\\ttype TrieIdGenerator = srml_contract::TrieIdFromParentCounter<Runtime>;\\n\\ttype GasPayment = ();\\n}\\n\\n// This is the sudo-contract Trait\\nimpl contract::Trait for Runtime {}\\n```\\n\\nWith these changes, the Polkadot UI should think that the `sudo-contract` module is indeed the regular Contract module, and provide you with a great user experience for interacting with Smart Contracts, without any additional work from your side.\\n\\n### Making SRML Contract \\"Un-Callable\\"\\n\\nThe `sudo-contract` module would be pretty useless if it was possible to still call the contract module directly, bypassing this authorization check we just added. However, the whole point of this project is to keep the original module around so we aren\'t forking things.\\n\\nFortunately, it seems that Substrate was designed to be so modular and customizable that this scenario is already supported! [From Gav](https://github.com/paritytech/substrate/pull/2399#issuecomment-487597233):\\n\\n> You can introduce a module entry into the runtime without the `Call` functionality which will prevent it from being routed to in a transaction.\\n\\nWhat he is saying here is that whereas we would normally import our SRML Contract module into the `construct_runtime!` macro like so:\\n\\n```rust\\nContract: srml_contract::{Module, Call, Storage, Config<T>, Event<T>},\\n```\\n\\nWe can simply omit the `Call` type, and it will make our module \\"un-callable\\". This does not affect any of the other basic functionalities of our runtime like the module storage, module events, and even genesis configuration.\\n\\nThen, when adding our `sudo-contract` module, we _do_ include the `Call` type, but nothing else since our wrapper module does not have any storage or events of its own (but it could so don\'t think this is a limitation). Here is what our final `construct_runtime!` import would look like for these two modules:\\n\\n```\\nContract: srml_contract::{Module, Storage, Config<T>, Event<T>},\\nSudoContract: contract::{Module, Call},\\n```\\n\\n## Testing Sudo Contract\\n\\nSo now that we have successfully added the `sudo-contract` module to our runtime, let\'s take a look at what happens when we use it.\\n\\nI have a [`sudo-contract` branch](https://github.com/shawntabrizi/substrate-package/tree/sudo-contract) in the `substrate-package` which you can use to run this wrapper module yourself, or double check the steps [in this PR](https://github.com/shawntabrizi/substrate-package/commit/c0c1e4604db279c5940f528c378575fa2c5aaf7a) for adding it to your own runtime.\\n\\nWe build the Wasm runtime and the native binaries to start our node:\\n\\n```bash\\n./scripts/build.sh\\ncargo build --release\\n./target/release/node-template purge-chain --dev\\n./target/release/node-template --dev\\n```\\n\\nWhen we run the node, we can interact with it using the Polkadot UI. We can immediately see that the UI recognizes that we have the Contract module included in our runtime:\\n\\n![Screenshot of the Polkadot UI with Contract Tab](/assets/images/sudo-contract-polkadot-ui.png)\\n\\nIf we dig a little deeper into the details, we can see that the extrinsics section uses our `sudo-contract` version of the Contract module functions:\\n\\n![Screenshot of the Contract Extrinsics](/assets/images/sudo-contract-call.png)\\n\\n> Notice that the comments with each function are the ones that we wrote in the wrapper module, and there are no other \\"contract\\" modules which can be called.\\n\\nFinally, if we look at the chain state tab, we will see that the UI and our runtime still manages the full storage of the SRML Contract module and that our module has no storage itself:\\n\\n![Screenshot of the Contract Extrinsics](/assets/images/sudo-contract-chain-state.png)\\n\\nSo really we have set up our runtime exactly as we want.\\n\\nLet\'s now try to use the Contract UI to create a new contract on our blockchain!\\n\\n### Putting Code On the Chain\\n\\nIf our `sudo-contract` module is really working, then only the Sudo key will be able to put new contracts onto the blockchain. Since we are running a `--dev` chain, Alice is set as the Sudo key at the genesis of our blockchain.\\n\\nSo let\'s first try to put a new smart contract with _another_ account. Let\'s fund Bob with enough units to deploy a contract, and try to upload the standard \\"flipper\\" contract. Here is what we will see in the UI:\\n\\n![UI error message when Bob tries to upload contract](/assets/images/sudo-contract-ui-error.png)\\n\\nWhen we look at our node terminal to see what \\"went wrong\\" we find:\\n\\n![Terminal error message when Bob tries to upload contract](/assets/images/sudo-contract-terminal-error.png)\\n\\n> \\"Runtime: Sender must be the Sudo key to put_code\\"\\n\\nSo our wrapper module is indeed gating access to the underlying SRML Contract module.\\n\\nNow we will try with Alice:\\n\\n![Success when Alice tries to upload contract](/assets/images/sudo-contract-ui-success.png)\\n\\nA success! Note that the `CodeStored` event which is emitted comes from `srmlContract`, which means ultimately the SRML Contract module is doing all of the work here. Our wrapper module is only doing the minimal it needs to in order to gate access. After this point, Bob or any other user can now create an instance of this contract.\\n\\nWe have successfully extended the SRML Contract module without making any forks or direct changes!\\n\\n## Next Steps\\n\\nWhile this post touches on a number of nuanced details about how we use Substrate to enable this end to end scenario, the big picture idea here should still be quite simple. You have now learned one approach to extending other Substrate runtime modules, and the possibilities with this are endless.\\n\\nHere are some ideas you could try to hack on:\\n\\n- Wrapper for the SRML Contract module which keeps track of all contracts that are uploaded with some additional metadata.\\n- Wrapper for the SRML Balances module which adds a \\"pause\\" functionality to the blockchain, preventing calls to `transfer`.\\n- Wrapper to the SRML Contract module which adds the ability for the Sudo key to \\"update\\" the Wasm code of a contract. (This will hopefully be the topic of a future post).\\n\\nAlso, this implementation of `sudo-contract` is not perfect. If you wanted to improve it, consider adding any of the following:\\n\\n- Adding module storage and some basic functions which allow you to control \\"privileged\\" accounts and remove the dependency on SRML Sudo.\\n- Abstract away direct dependency on the SRML Contract module, and have the module work for wrapping any module which exposes the same dispatchable functions.\\n- Add a second tier of authorization for `create` so that only some users can `put_code`, a larger (but still limited) set of users can `create`, but then all users can `call`.\\n\\nI hope that someone uses the `sudo-contract` module in their production blockchain. If you do end up using it, please let me know!\\n\\nAs always, if you like the content I produce and want to help me keep doing it, take a look at my [donation page](https://shawntabrizi.com/donate/)."},{"id":"/substrate/adding-fees-to-your-substrate-runtime-module/","metadata":{"permalink":"/blog/substrate/adding-fees-to-your-substrate-runtime-module/","source":"@site/blog/2019-05-27-adding-fees-to-your-substrate-runtime-module.md","title":"Adding Fees to Your Substrate Runtime Module","description":"In this post, I will show you how you can easily add a fee for calling a function within your Substrate runtime module.","date":"2019-05-27T00:00:00.000Z","tags":[{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"module","permalink":"/blog/tags/module"},{"inline":true,"label":"rust","permalink":"/blog/tags/rust"},{"inline":true,"label":"fees","permalink":"/blog/tags/fees"},{"inline":true,"label":"balance","permalink":"/blog/tags/balance"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"}],"readingTime":7.99,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Adding Fees to Your Substrate Runtime Module","date":"2019-05-27T00:00:00.000Z","authors":"shawntabrizi","slug":"/substrate/adding-fees-to-your-substrate-runtime-module/","categories":["Substrate"],"tags":["runtime","module","rust","fees","balance","ethereum"]},"unlisted":false,"prevItem":{"title":"Extending Substrate Runtime Modules","permalink":"/blog/substrate/extending-substrate-runtime-modules/"},"nextItem":{"title":"Substrate Smart Contracts Workshop","permalink":"/blog/substrate/substrate-smart-contracts-workshop/"}},"content":"##### In this post, I will show you how you can easily add a fee for calling a function within your Substrate runtime module.\\n\\nWhen using Substrate, you are afforded the flexibility to completely control the fee system within your runtime.\\n\\nBy default, a [`transaction_base_fee`](https://substrate.dev/rustdocs/v1.0/srml_balances/struct.Module.html#method.transaction_base_fee) is added to every transaction you make to your runtime. However, this blanket base fee does NOT take into account anything related to the complexity or storage used as a result of the transaction.\\n\\nSubstrate makes the following recommendation in the `Example` module:\\n\\n> Ensure that calls into each of these [functions] execute in a time, memory and using storage space **proportional to any costs paid for by the caller** or otherwise the difficulty of forcing the call to happen.\\n\\nThus, if your runtime module exposes functions which are heavy in computation or storage needs, you should be sure to add some _additional_ fee on top of the base fee to ensure your blockchain is not attackable.\\n\\n## A Simple Fee\\n\\nThere are a lot of complicated methods you can use for calculating fees for functions. You can take a look at the [Contract module](https://github.com/paritytech/substrate/tree/v1.0/srml/contract) for an example of that.\\n\\nFor this example, I will be showing you the most simple implementation of a fee which will be inline with the rest of your module code.\\n\\n### Withdrawing From Balance\\n\\nThe first tool we will use is the [`withdraw` function](https://substrate.dev/rustdocs/v1.0/srml_support/traits/trait.Currency.html#tymethod.withdraw) provided by the `Currency` trait in the Balances module:\\n\\n```rust\\nfn withdraw(\\n    who: &AccountId,\\n    value: Self::Balance,\\n    reason: WithdrawReason,\\n    liveness: ExistenceRequirement\\n) -> Result<Self::NegativeImbalance, &\'static str>\\n```\\n\\n> Removes some free balance from who account for reason if possible. If liveness is KeepAlive, then no less than ExistentialDeposit must be left remaining.\\n>\\n> This checks any locks, vesting, and liquidity requirements. If the removal is not possible, then it returns Err.\\n\\nWithdraw is designed to be quite flexible. As you can see, it allows you to specify the reason for a withdrawal. In this case, we are taking a _fee_:\\n\\n```rust\\n// use support::traits::WithdrawReason\\nWithdrawReason::Fee\\n```\\n\\nIt can even make sure that removing these funds will not kill an account. For our fee system, this second point will be particularly important since we do not want users to accidentally destroy their account paying for fees!\\n\\nFor this, we simply pass:\\n\\n```rust\\n// use support::traits::ExistenceRequirement\\nExistenceRequirement::KeepAlive\\n```\\n\\nNow we can really safely charge fees to a user upfront and let the logic of the Balances module handle the rest. For ease of reusability, we will create an internal function which can be called within our module to charge a fee to a user:\\n\\n```rust\\nimpl<T: Trait> Module<T> {\\n  fn pay_fee(who: T::AccountId, amount: T::Balance) -> Result {\\n    let _ = <balances::Module<T> as Currency<_>>::withdraw(\\n      &who,\\n      amount,\\n      WithdrawReason::Fee,\\n      ExistenceRequirement::KeepAlive\\n    )?;\\n\\n    Ok(())\\n  }\\n}\\n```\\n\\nThis function will either propagate an error from taking funds from the user, or will complete successfully and return `Ok(())`. We can then handle the error within our runtime.\\n\\n#### Imbalance\\n\\nOne thing we glazed over at this point is the return type of the `withdraw` function:\\n\\n```rust\\n-> Result<Self::NegativeImbalance, &\'static str>\\n```\\n\\nAs you can see, it returns a `NegativeImbalance`, which is probably a type you have never seen before. Without going into _too_ much detail, the [Imbalance system](https://github.com/paritytech/substrate/pull/2048) within the Balances module is a way to ensure that the sum of all funds across all accounts is equal to the `TotalIssuance` managed by the Balances module.\\n\\nSo fortunately, this imbalances system [does all of the hard work for us!](https://stackoverflow.com/questions/56341343/is-handling-the-imbalance-type-mandatory-after-withdraw-or-deposit) All we need to do is ignore this return type and we can be happy that the `TotalIssuance` is updated and this value is actually burned.\\n\\n### Charging a Fee\\n\\nNow that we have created our `pay_fee` function, we need to call it within our runtime module. We will emulate a fixed fee system similar to the low level OPCODEs provided by Ethereum, where each function in our module can define some fixed cost to call the function.\\n\\nThis can be done easily by simply writing a function like so:\\n\\n```rust\\ndecl_module! {\\n  pub struct Module<T: Trait> for enum Call where origin: T::Origin {\\n    pub fn do_something(origin) -> Result\\n    {\\n      let who = ensure_signed(origin)?;\\n      let fee = 1337.into();\\n\\n      Self::pay_fee(who, fee)?;\\n\\n      // Do stuff after fee is paid successfully...\\n\\n      Ok(())\\n    }\\n\\n  }\\n}\\n```\\n\\nThis function checks whether or not `pay_fee` returns successfully, and if not, it propagates the error up and stops execution of the runtime function.\\n\\nIn the situation where a user is unable to `withdraw` funds, we will see the error message:\\n\\n> Runtime: too few free funds in account\\n\\n#### Converting Rust Primitives to Substrate Specific Types\\n\\nYou may notice that `pay_fee` and `withdraw` expect `fee` to be of type `T::Balance`.\\n\\nRemember that Substrate is written to be **very** generic, so in the context of your runtime module, there are minimal assumptions about your blockchain\'s types.\\n\\nFor example, using this generic type system, you would be able to define one Substrate blockchain which uses `u64` for the `Balance` type, another which uses `u128`, and another which uses `u32`. Because we use this generic type system for all the core blockchain types, the same module can be used out of the box across all of these different blockchains!\\n\\nBut this flexibility also means you need to tell the Rust compiler what to do when trying to handle incompatible situations.\\n\\nFor example, what should the module do if we try to put a `u128` value into a `Balance` type which is represented as `u64`? Or if we try to convert that same balance to a `u32`?\\n\\nSubstrate [provides implementations](https://stackoverflow.com/questions/56081117/how-do-you-convert-between-substrate-specific-types-and-rust-primitive-types) of `From`/`TryFrom` and `Into`/`TryInto` to handle such scenarios. The only assumption being made here is that all values are at least a `u32`.\\n\\nThese traits guarantee that the underlying types implement functions which will attempt to convert between types if possible.\\n\\nThus, if we ever need to convert some `u32` value to a `Balance`, we can simply call:\\n\\n```rust\\nlet my_balance: T::Balance = my_u32.into();\\n```\\n\\nIn the situation you need to convert some larger value, you will need to handle the situation where the `Balance` type is not compatible with type to be converted:\\n\\n```rust\\npub fn u64_to_balance(input: u64) -> Option<T::Balance> {\\n    input.try_into().ok()\\n}\\n```\\n\\nNote that this returns an `Option`, thus your subsequent runtime logic needs to decide what to do when the conversion fails and the returned value is `None`.\\n\\nSubstrate also provides a `saturated_into` function which will always succeed, but will coerce your value into the type you want through saturation if necessary:\\n\\n```rust\\npub fn u64_to_balance_saturated(input: u64) -> T::Balance {\\n    input.saturated_into()\\n}\\n```\\n\\nHowever, it is very important that you be conscious when you do such things. From [Gav](https://twitter.com/gavofyork):\\n\\n> `SaturatedConversion` (`saturated_into` and `saturated_from`) should not be used unless you know what you\'re doing, you\'ve thought and considered all options and your use-case implies that saturation is fundamentally correct. The only time I imagine this is the case is deep in runtime arithmetic where you are logically certain it will not overflow, but can\'t provide a proof because it would depend on consistent pre-existing state.\\n\\nRemember, as a runtime developer, Substrate provides you with numerous tools, but it is ultimately up to you to determine how to use them.\\n\\n## A Minimal, Complete, Verifiable Example Module\\n\\nIf you want to try out this simple fee system on your own Substrate chain, you can simply add a module like this to your runtime:\\n\\n```rust\\nuse support::{decl_module, dispatch::Result,\\n  traits::{Currency, ExistenceRequirement, WithdrawReason}};\\nuse system::ensure_signed;\\n\\n// v1.0 branch\\n// use runtime_primitives::traits::As;\\n\\npub trait Trait: balances::Trait {}\\n\\ndecl_module! {\\n  pub struct Module<T: Trait> for enum Call where origin: T::Origin {\\n    pub fn do_something(origin) -> Result\\n    {\\n      let who = ensure_signed(origin)?;\\n\\n      let fee = 1337.into();\\n      // v1.0 branch\\n      // let fee = T::Balance::sa(1337);\\n\\n      Self::pay_fee(who, fee)?;\\n\\n      // Do stuff after fee is paid successfully...\\n\\n      Ok(())\\n    }\\n\\n  }\\n}\\n\\nimpl<T: Trait> Module<T> {\\n  fn pay_fee(who: T::AccountId, amount: T::Balance) -> Result {\\n    let _ = <balances::Module<T> as Currency<_>>::withdraw(\\n      &who,\\n      amount,\\n      WithdrawReason::Fee,\\n      ExistenceRequirement::KeepAlive\\n    )?;\\n\\n    Ok(())\\n  }\\n}\\n```\\n\\nIf we run a local node, we can interact with the module through the [Polkadot UI](https://polkadot.js.org/apps/):\\n\\n![Image of Extrinsic Tab](/assets/images/substrate-fee-extrinsic.png)\\n\\nWe have funded the Bob account with 2000 units, and we are charging a fee of 1337. When we call our function the first time, everything works as expected, and the 1337 unit fee (in addition to the base transaction fee of 1 unit) is removed from the account.\\n\\n![Image of Fee Success](/assets/images/substrate-fee-success.png)\\n\\nHowever, when Bob does not have enough funds to make a second call, they will see a failure message:\\n\\n![Image of Fee Failure](/assets/images/substrate-fee-fail.png)\\n\\n> Note though that the 1 unit base transaction fee was still removed.\\n\\nIf we look at our local node terminal, we can see the reason why this transaction failed:\\n\\n![Image of Fee Error](/assets/images/substrate-fee-error.png)\\n\\n## Next Steps\\n\\nAs mentioned, this is a very minimal and simplistic implementation of a fee system. However, this should give you the tools necessary to build your own advance fee system. Here are some cool ideas:\\n\\n- Create some authorization layer where certain users get lower fees than the average user.\\n- Allow fees to be paid using other tokens that your runtime manages.\\n- Have your fee be calculated based on any input from the user. For example, if you let the user store some `Vec<u8>`, you can charge them some linear cost based on the length of the data.\\n\\nDo you have other ideas? Let me know!\\n\\nAs always, if you enjoy my content, take a quick look at my [donation page](https://shawntabrizi.com/donate/) to help support future work."},{"id":"/substrate/substrate-smart-contracts-workshop/","metadata":{"permalink":"/blog/substrate/substrate-smart-contracts-workshop/","source":"@site/blog/2019-04-28-substrate-smart-contracts-workshop.md","title":"Substrate Smart Contracts Workshop","description":"We recently held our very first Sub0 conference right next to the Parity office in Berlin.","date":"2019-04-28T22:17:06.000Z","tags":[{"inline":true,"label":"smart contract","permalink":"/blog/tags/smart-contract"},{"inline":true,"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":1.435,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Substrate Smart Contracts Workshop","date":"2019-04-28T22:17:06.000Z","authors":"shawntabrizi","slug":"/substrate/substrate-smart-contracts-workshop/","categories":["Substrate"],"tags":["smart contract","rust"],"github":"substrate-contracts-workshop"},"unlisted":false,"prevItem":{"title":"Adding Fees to Your Substrate Runtime Module","permalink":"/blog/substrate/adding-fees-to-your-substrate-runtime-module/"},"nextItem":{"title":"The Sudo Story in Substrate","permalink":"/blog/substrate/the-sudo-story-in-substrate/"}},"content":"We recently held our very first Sub0 conference right next to the Parity office in Berlin.\\n\\nSub0 was an event bringing together users from around the world who share our vision of the future and believe in the technologies we are developing at Parity... and there are quite a few things we are building.\\n\\nOne of the things we showed off for the first time during Sub0 was ink! (previously known as the PDSL), a Rust eDSL for building Wasm smart contracts on Substrate!\\n\\nThis smart contract platform is pretty much the child of two Parity developers: Robin ([Robbepop](https://github.com/Robbepop)) and Sergei ([Pepyakin](https://github.com/pepyakin)). They were so kind as to get me up to speed on what they had built, and it was surprisingly mature and well developed.\\n\\nI spent a few weeks learning about the end to end smart contract system, including the Contract module provided by Substrate and the three different layers of the ink! language.\\n\\nThen, the day before Sub0, I spent [2](https://github.com/shawntabrizi/substrate-contracts-workshop/commit/c6e0223018c17841e342dccc47842a8e18afab55)[4](https://github.com/shawntabrizi/substrate-contracts-workshop/commit/7c26536795e80e0504575dfccf8f8c8871f3eaf4) hours building a brand new workshop based on the same structure as the [Substrate Collectables Workshop](https://github.com/shawntabrizi/substrate-collectables-workshop).\\n\\n[![](/assets/images/img_5cccc806a30b1.png)](https://github.com/shawntabrizi/substrate-contracts-workshop)\\n\\nThrough this tutorial we teach you how to:\\n\\n- Set up the ink! development environment\\n- Deploy and interact with the pre-built `flipper` contract\\n- Write an \\"incrementer\\" contract\\n- Write an ERC20 Token\\n\\nI am also looking to add instructions for building:\\n\\n- An ERC721 Token\\n- A non-token contract, maybe something that more directly interacts with a Substrate module\\n\\nAnyway, I would love for you all to try it out, and hopefully by the time you do, it will have been much more developed!\\n\\nIf you like the work I do, take a quick look at my [donation page](https://shawntabrizi.com/donate/) to help support future endeavors."},{"id":"/substrate/the-sudo-story-in-substrate/","metadata":{"permalink":"/blog/substrate/the-sudo-story-in-substrate/","source":"@site/blog/2019-03-14-the-sudo-story-in-substrate.md","title":"The Sudo Story in Substrate","description":"In this post, I will explain how the Sudo module is used to access permissioned functions in Substrate.","date":"2019-03-14T17:05:44.000Z","tags":[],"readingTime":6.535,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"The Sudo Story in Substrate","date":"2019-03-14T17:05:44.000Z","authors":"shawntabrizi","slug":"/substrate/the-sudo-story-in-substrate/","categories":["Substrate"]},"unlisted":false,"prevItem":{"title":"Substrate Smart Contracts Workshop","permalink":"/blog/substrate/substrate-smart-contracts-workshop/"},"nextItem":{"title":"Substrate at FOSDEM 2019","permalink":"/blog/substrate/substrate-at-fosdem-2019/"}},"content":"##### In this post, I will explain how the Sudo module is used to access permissioned functions in Substrate.\\n\\nIf you have ever run a local Substrate node for testing or development, you have probably interacted with the Sudo module. More specifically, you might have noticed that the \\"Alice\\" account is special, and can do powerful things to your blockchain!\\n\\nIn this blog post, I will show you end to end how the Sudo module works, why \\"Alice\\" is able to use this module, and how it enables access to permissioned functions like the one which enables Substrate runtime upgrades.\\n\\n## What is Sudo?\\n\\n> `sudo` is a program for Unix-like computer operating systems that allows users to run programs with the security privileges of another user, by default the **superuser**. It originally stood for \\"superuser do\\"...\\n>\\n> -- <cite>Wikipedia</cite>\\n\\nIn short, `sudo` is a term used to represent the execution of some highly privileged operation by some highly privileged user. If you are trying to relate this to smart contracts on Ethereum, this is very similar to [the \\"contract owner\\"](https://github.com/OpenZeppelin/openzeppelin-solidity/blob/master/contracts/ownership/Ownable.sol), an account who is allowed to call `onlyOwner` functions.\\n\\n## The Sudo Module\\n\\nThe Substrate runtime module library (SRML) provides a Sudo module which provides this same functionality, but at the runtime level of your blockchain. At the time of writing this post, the Sudo module is [only 60 lines of code](https://github.com/paritytech/substrate/blob/v1.0/srml/sudo/src/lib.rs), so you can easily look through the source code yourself to understand exactly what it does. But I will break it down for you just in case you are unfamiliar with the structure of Runtime modules.\\n\\n### The Sudo Key\\n\\nThe Sudo module has a single storage item: the \\"Sudo key\\".\\n\\n```rust\\ndecl_storage! {\\n\\ttrait Store for Module<T: Trait> as Sudo {\\n\\t\\t/// The `AccountId` of the sudo key.\\n\\t\\tKey get(key) config(): T::AccountId;\\n\\t}\\n}\\n```\\n\\nThis holds the `AccountId` of the person who is the \\"superuser\\" of your blockchain. Notice that it has the `config()` parameter, which means that this value can be set using the \\"genesis configuration\\" of your blockchain. We will talk about that more below.\\n\\n### The Sudo Module Functions\\n\\nThe Sudo module has two dispatchable functions which allow users to interact with the module.\\n\\nThe first function available in the Sudo module is `set_key(origin, new)`, which allows only the Sudo key to change who the Sudo key is. This is not that interesting, so we won\'t go into details.\\n\\nThe second function is `sudo(origin, proposal)`, which allows only the Sudo key to dispatch a privileged call. This authorization check is done in the first two lines of the function:\\n\\n```rust\\nlet sender = ensure_signed(origin)?;\\nensure!(sender == Self::key(), \\"only the current sudo key can sudo\\");\\n```\\n\\nThen the function dispatches actually happens:\\n\\n```rust\\nlet ok = proposal.dispatch(system::RawOrigin::Root.into()).is_ok();\\n```\\n\\nThere are a few different things to note about this innocuous line:\\n\\n- The `proposal` variable is another dispatchable function within your runtime, and is accepted as an input to the `sudo` function.\\n- This `proposal` is called using `system::RawOrigin::Root`, which defines the _new_ origin for the downstream call.\\n\\nAt this point, you might be asking \\"What does this whole `RawOrigin::Root` mean?\\"\\n\\n## Privileged Functions\\n\\nThe Sudo module wouldn\'t do much unless there were also \\"sudo-able\\" functions, and that is precisely what we will talk about next.\\n\\nSubstrate has the concept of \\"Privileged Functions\\" which are functions which specifically require `Root` origin. The `origin` of a call describes where the call has come from, and every dispatchable function should check at the beginning of the function that the call origin matches what is expected. The origin could be `Signed` as it was in the Sudo module, which represents a basic signed transaction, but Substrate also provides a `Root` origin which describes a call that comes from _within_ the runtime itself. There is no way to produce a `Root` origin other than through internal runtime logic, and as such, we can treat functions that require this origin as privileged functions.\\n\\nThis is what a privileged function look like:\\n\\n```rust\\ndecl_module! {\\n    pub struct Module<T: Trait> for enum Call where origin: T::Origin {\\n        pub fn privileged_function(origin) -> Result {\\n            ensure_root(origin)?;\\n            // do something...\\n            Ok(())\\n        }\\n    }\\n}\\n```\\n\\nHowever, macro magic makes this a bit more confusing. Generally, there is a rule for dispatchable functions where the first parameter must always be `origin`. However, when using the `decl_module!` macro, if you omit the `origin` parameter, then it will be added automatically, and `ensure_root(origin)?` will also be added. So an equivalent way to write the `privileged_function` above would be:\\n\\n```rust\\npub fn privileged_function() -> Result {\\n    // do something...\\n    Ok(())\\n}\\n```\\n\\nSo these kinds of functions will only be callable by internal runtime logic like what is implemented in the Sudo module.\\n\\n## A Substrate Runtime Upgrade\\n\\nSo let\'s take a look at a real privileged function which is available within all Substrate runtimes, the runtime upgrade. From the [System module](https://github.com/paritytech/substrate/blob/v1.0/srml/system/src/lib.rs):\\n\\n```rust\\n/// Set the new code.\\npub fn set_code(new: Vec<u8>) {\\n    storage::unhashed::put_raw(well_known_keys::CODE, &new);\\n}\\n```\\n\\nThis single line of logic is enough to power the entire \\"upgrade\\" feature of Substrate forkless runtime upgrades. This privileged function checks that the caller must have `Root` origin, thanks to the `decl_module!` macro, then it puts the Wasm bytecode into a `well_known_key` called `CODE`.\\n\\nThus, when you use something like the Polkadot UI to do a runtime upgrade, it will look like this:\\n\\n![](/assets/images/img_5ccc8f228649c.png)\\n\\nWalking through the UI you will see that:\\n\\n- We are making a transaction using \\"Alice\\", who has been set as the Sudo key.\\n- We are calling the `sudo` function of the Sudo module.\\n- The proposal we are making is the `setCode` function of the Consensus module.\\n- Of which the input to `setCode` is the `wasm` file for our new runtime.\\n\\nWhen this transaction is dispatched, the following logic is executed:\\n\\n1. The `sudo` function checks that \\"Alice\\" is the Sudo key.\\n2. She is, so the rest of the function runs.\\n3. A `Root` origin call is dispatched to the `setCode` function.\\n4. The `setCode` function checks that the origin is `Root`.\\n5. It is, so the rest of the function runs.\\n6. The storage value is updated for the `CODE` well known key.\\n\\nAnd that is the magic of the Sudo module! The last thing we should probably talk about is how \\"Alice\\" became the Sudo module to begin with, and for that we need to look at the genesis configuration of our blockchain.\\n\\n## Initializing the Sudo key\\n\\nHave you wondered why your substrate test network gives \\"Alice\\" a bunch of initial \\"balance units\\" and makes her the Sudo key for your runtime? Well, this is all controlled in your blockchain genesis configuration which is defined in a file called [`chain_spec.rs`](https://github.com/paritytech/substrate/blob/v1.0/node-template/src/chain_spec.rs).\\n\\nWe can see that this code ultimately creates a `GenesisConfig` object with the following initial setting:\\n\\n```rust\\nsudo: Some(SudoConfig {\\n    key: root_key,\\n}),\\n```\\n\\nIf we follow the logic back, this `root_key` is defined as `account_key(\\"Alice\\")` which generates an `AccountId` using the seed string \\"Alice\\".\\n\\n```rust\\nfn account_key(s: &str) -> AccountId {\\n    sr25519::Pair::from_string(&format!(\\"//{}\\", s), None)\\n        .expect(\\"static values are valid; qed\\")\\n        .public()\\n}\\n```\\n\\nThis works great for test networks since \\"Alice\\" can be treated as a well known account that is automatically configured in your UI. However, for a real network, this is probably not what you want to do. Instead, you should pass an `AccountId` directly to this genesis configuration, and keep the seed for this account VERY secret.\\n\\n![](/assets/images/img_5ccc92f09f522.png)\\n\\n## The End\\n\\nThat is the entire sudo story in Substrate. I hope you learned something new and that this shed light to some of the things happening behind the scenes of Substrate runtimes.\\n\\nIf you enjoyed this content and want to support me producing more, feel free to check out my [donation page](https://shawntabrizi.com/donate/)."},{"id":"/substrate/substrate-at-fosdem-2019/","metadata":{"permalink":"/blog/substrate/substrate-at-fosdem-2019/","source":"@site/blog/2019-02-11-substrate-at-fosdem-2019.md","title":"Substrate at FOSDEM 2019","description":"I had the amazing opportunity to speak about Substrate at the Free and Open source Software Developers\' European Meeting (FOSDEM).","date":"2019-02-11T23:01:50.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"parity","permalink":"/blog/tags/parity"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"speaking","permalink":"/blog/tags/speaking"},{"inline":true,"label":"substrate","permalink":"/blog/tags/substrate"},{"inline":true,"label":"video","permalink":"/blog/tags/video"}],"readingTime":0.905,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Substrate at FOSDEM 2019","date":"2019-02-11T23:01:50.000Z","authors":"shawntabrizi","slug":"/substrate/substrate-at-fosdem-2019/","categories":["Substrate"],"tags":["blockchain","parity","programming","speaking","substrate","video"],"github":"substratekitties"},"unlisted":false,"prevItem":{"title":"The Sudo Story in Substrate","permalink":"/blog/substrate/the-sudo-story-in-substrate/"},"nextItem":{"title":"CryptoKitties on Substrate","permalink":"/blog/substrate/cryptokitties-on-substrate/"}},"content":"I had the amazing opportunity to speak about Substrate at the Free and Open source Software Developers\' European Meeting (FOSDEM).\\n\\nIn my talk, I talk about:\\n\\n- The basics of blockchains and blockchain development\\n- Parity\'s history building blockchains\\n- How Substrate came to be as a result of building the Polkadot Network\\n- The basics of Substrate\\n- The value of Rust and Wasm as next generation tools to build Substrate\\n- How these same tools enable forkless runtime upgrades\\n- A live demo of a forkless runtime upgrade\\n- How you can learn to develop on Substrate using the Substrate Kitties Workshop\\n\\nYou can check out the full talk on my YouTube channel:\\n\\n<iframe src=\\"https://www.youtube.com/embed/ELubZ6Rl3iI\\" allow=\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen=\\"\\" width=\\"720px\\" height=\\"480px\\" frameborder=\\"0\\"></iframe>\\n\\nI still need to work on public speaking: being clear, talking slowly, and not getting too nervous. I hope that I can continue to speak about Substrate and not only spread word about this awesome technology we are developing, but also to work on these skills.\\n\\nYou can find my slides on the [FOSDEM site](https://fosdem.org/2019/schedule/event/substrate/)."},{"id":"/substrate/cryptokitties-on-substrate/","metadata":{"permalink":"/blog/substrate/cryptokitties-on-substrate/","source":"@site/blog/2019-01-30-cryptokitties-on-substrate.md","title":"CryptoKitties on Substrate","description":"You saw in my last post that I started to investigate Substrate runtime development by building a simple proof of existence blockchain.","date":"2019-01-30T22:57:52.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"parity","permalink":"/blog/tags/parity"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"substrate","permalink":"/blog/tags/substrate"}],"readingTime":1.165,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"CryptoKitties on Substrate","date":"2019-01-30T22:57:52.000Z","authors":"shawntabrizi","slug":"/substrate/cryptokitties-on-substrate/","categories":["Substrate"],"tags":["blockchain","parity","programming","runtime","substrate"],"github":"substrate-collectables-workshop"},"unlisted":false,"prevItem":{"title":"Substrate at FOSDEM 2019","permalink":"/blog/substrate/substrate-at-fosdem-2019/"},"nextItem":{"title":"Proof of Existence Blockchain with Substrate","permalink":"/blog/substrate/proof-of-existence-blockchain-with-substrate/"}},"content":"You saw in my last post that I started to investigate Substrate runtime development by building a simple [proof of existence](https://shawntabrizi.com/substrate/proof-of-existence-blockchain-with-substrate/) blockchain.\\n\\nI wanted to take it a step further, and develop a more feature rich runtime module to see what the developer experience is like. Naturally, I decided to build CryptoKitties, or at least some subset of the game, on Substrate. If you are unfamiliar, CryptoKitties is one of the most popular Ethereum DApps which is notorious for almost breaking the Ethereum network with crazy high transaction fees.\\n\\nThe CryptoKitties game is something I am actually pretty familiar with thanks to the CryptoZombies tutorial that was created to teach new developers about smart contract development. I even wrote a summary of [what I learned going through the tutorial](https://shawntabrizi.com/ethereum/shawn-notes-cryptozombies-lessons-1-5-in-solidity/) nearly a year ago.\\n\\nThe CryptoZombies tutorial was such an important part of my experience when I was first learning about Solidity, and really these kinds of 0 to 60 tutorials are so important to build a community of developers who can actually build on your platform.\\n\\nLong story short, I created an extensive tutorial for building a CryptoKitties like runtime module on Substrate called the \\"Substrate Collectables Workshop\\".\\n\\n[![](/assets/images/img_5c8ed01e77c2f.png)](https://shawntabrizi.github.io/substrate-collectables-workshop/#/)\\n\\nI took a lot of inspiration from the CryptoZombies tutorial and all the things they did right. Check it out, and tell me what you think!\\n\\nSource is of course open source on [GitHub](https://github.com/shawntabrizi/substrate-collectables-workshop)."},{"id":"/substrate/proof-of-existence-blockchain-with-substrate/","metadata":{"permalink":"/blog/substrate/proof-of-existence-blockchain-with-substrate/","source":"@site/blog/2018-12-31-proof-of-existence-blockchain-with-substrate.md","title":"Proof of Existence Blockchain with Substrate","description":"This post will show you how easy it is to build a proof of existence blockchain with a user interface using Parity\'s Substrate framework.","date":"2019-01-01T03:54:39.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"runtime","permalink":"/blog/tags/runtime"},{"inline":true,"label":"substrate","permalink":"/blog/tags/substrate"}],"readingTime":8.955,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Proof of Existence Blockchain with Substrate","date":"2019-01-01T03:54:39.000Z","authors":"shawntabrizi","slug":"/substrate/proof-of-existence-blockchain-with-substrate/","categories":["Substrate"],"tags":["blockchain","programming","runtime","substrate"],"github":"substrate-proof-of-existence"},"unlisted":false,"prevItem":{"title":"CryptoKitties on Substrate","permalink":"/blog/substrate/cryptokitties-on-substrate/"},"nextItem":{"title":"Verify Ethereum Contracts Using Web3.js and Solc","permalink":"/blog/ethereum/verify-ethereum-contracts-using-web3-js-and-solc/"}},"content":"##### This post will show you how easy it is to build a proof of existence blockchain with a user interface using Parity\'s Substrate framework.\\n\\n2019 will surely be remembered as an exciting year for blockchain development and decentralized applications. Just in time for the new year, Parity\'s Substrate was [officially released in beta](https://www.parity.io/substrate-has-arrived/).\\n\\nIf you are unfamiliar with [Substrate](https://github.com/paritytech/substrate), it is a development framework enabling users to easily build custom blockchains. Whereas blockchain development would normally require you to develop peer-to-peer networking, a database layer, a JSON RPC, consensus algorithms, and more, Substrate provides you with enterprise ready, battle-test software which you can customize for your needs.\\n\\nTo illustrate the simplicity of developing on Substrate, I thought I would try to build one of a simplest practical blockchains that you can develop: [a proof of existence blockchain](https://en.wikipedia.org/wiki/Proof_of_Existence).\\n\\n> Proof of Existence is an online service that verifies the existence of computer files as of a specific time via timestamped transactions in the bitcoin blockchain.\\n\\nThe first proof of existence platform was built as a sort of \\"hack\\" on top of the Bitcoin network. At that time, it would be pretty impractical to build an entire blockchain platform for just this purpose, but with Substrate, not only is it practical, but it is actually easy!\\n\\nYou can find the repository for this ongoing project here:\\n\\n[https://github.com/shawntabrizi/substrate-proof-of-existence](https://github.com/shawntabrizi/substrate-proof-of-existence)\\n\\nSo let\'s jump right in!\\n\\n## Running a Substrate Chain\\n\\nOut of the box, Substrate provides you with all the tools you need to run a blockchain. Along with the underlying infrastructure (networking, database, consensus, etc...), Substrate comes with a [library of runtime modules](https://github.com/paritytech/substrate/tree/master/frame) which give you the features and functionalities of modern blockchains:\\n\\n- Balances (tokens, transfers, etc...)\\n- Democracy (stakeholder voting)\\n- Contract (smart contract platform)\\n- and more!\\n\\nNote that a \'runtime\' is just logic that powers your blockchain. For example, the state transition function in [Bitcoin](https://github.com/ethereum/wiki/wiki/White-Paper#bitcoin-as-a-state-transition-system) or [Ethereum](https://github.com/ethereum/wiki/wiki/White-Paper#ethereum-state-transition-function) could be thought of as the runtimes of those blockchains. These modules, along with custom modules that you develop, can be put together on Substrate to build a custom blockchain.\\n\\nParity provides a bare-bones [substrate node template](https://github.com/shawntabrizi/substrate-package) which integrates a few of these modules into an out-of-the-box working blockchain. This is [all I needed to do](https://substrate.readme.io/v1.0.0/docs/creating-a-custom-substrate-chain#section-prerequisites) to set up a running chain:\\n\\n```bash\\ncurl https://getsubstrate.io -sSf | bash\\nsubstrate-node-new proof-of-existence \\"Shawn Tabrizi\\"\\nsubstrate-ui-new proof-of-existence\\n```\\n\\nYou can then start the blockchain by going into the <code>proof-of-existence</code> folder and running:\\n\\n```bash\\n./target/release/proof-of-existence --dev\\n```\\n\\nAnd finally you can interact with this chain using the <code>proof-of-existence-ui</code> by running:\\n\\n```bash\\nnpm run dev\\n```\\n\\n![An image of the Substrate UI](/assets/images/Screen-Shot-2018-12-31-at-10.36.32-AM.png)\\n\\n## Developing the Proof of Existence Runtime Logic\\n\\nNow that we have things running, we want to add our own runtime logic on top to create a proof of existence blockchain. Fortunately, the logic we need to build is pretty simple and straightforward. Essentially, we just need to maintain some storage on the blockchain which establishes:\\n\\n- An owner\\n- A file hash (or [file digest](https://en.wikipedia.org/wiki/File_verification))\\n- The timestamp of the verification\\n\\nThen, we need to create a function which allows a user to write to this storage and claim a file if it has not already been claimed before. For additional user-friendliness, we can also add a function which allows the specific owner of a file to revoke their claim, so that someone else may claim the file in the future.\\n\\nContinuing to follow [this tutorial](https://substrate.readme.io/v1.0.0/docs/creating-a-custom-substrate-chain#section-step-3-create-a-new-runtime-module) we can learn the basics of building a new runtime module. Here is a link to the entire proof of existence runtime that I wrote: [proof-of-existence/runtime/src/proof_of_existence.rs](https://github.com/shawntabrizi/substrate-proof-of-existence/blob/e4011fca36f19b0d015c82f9badeb5ef644ea165/proof-of-existence/runtime/src/proof_of_existence.rs).\\n\\n### A Closer Look\\n\\nLet\'s take a look at a few of the different parts:\\n\\n```rust\\ndecl_storage! {\\n\\ttrait Store for Module<T: Trait> as POEStorage {\\n\\t\\tProofs get(proofs): map Vec<u8> => (T::AccountId, T::Moment);\\n\\t}\\n}\\n```\\n\\nYou can see our storage structure is quite simple. Just a mapping between a `Vec<u8>` (bytearray) to a tuple of `AccountID` and `Moment` (the substrate equivalent of time).\\n\\n```rust\\nfn create_claim(origin, digest: Vec<u8>) -> Result {\\n    ensure!(!<Proofs<T>>::exists(&digest), \\"This digest has already been claimed\\");\\n\\n    let sender = ensure_signed(origin)?;\\n    let time = <timestamp::Module<T>>::now();\\n\\n    <balances::Module<T>>::decrease_free_balance(&sender, <T::Balance as As<u64>>::sa(POE_FEE))?;\\n    <Proofs<T>>::insert(&digest, (sender.clone(), time.clone()));\\n\\n    Self::deposit_event(RawEvent::ClaimCreated(sender, time, digest));\\n    Ok(())\\n}\\n```\\n\\nWhen the user calls the `create_claim()` function, they provide a signed message including the digest they want to claim. We check that the digest has not already been claimed and that the messaged is signed, where we can also derive the AccountID of the sender.\\n\\nIn my version of the runtime, we charge the user a fee to create a claim. For that operation, we call `decrease_free_balance()` which will also check whether or not the full amount can be deducted from the account. If the account does not have enough funds, it will throw an error, which stops the transaction.\\n\\nNote that only after we have done all these checks do we modify the storage and `insert()` the claim. This is important because if you modify the storage, and the transaction fails at some later point, the storage will remain modified. Substrate warns about this in [their documentation](https://github.com/paritytech/substrate/blob/v1.0/srml/example/src/lib.rs):\\n\\n```rust\\n// Since this is a dispatched function there are two extremely important things to\\n// remember:\\n//\\n// - MUST NOT PANIC: Under no circumstances (save, perhaps, storage getting into an\\n// irreparably damaged state) must this function panic.\\n// - NO SIDE-EFFECTS ON ERROR: This function must either complete totally (and return\\n// `Ok(())` or it must have no side-effects on storage and return `Err(\'Some reason\')`.\\n```\\n\\nThe revoke function is basically the same, but going the other direction:\\n\\n```rust\\nfn revoke_claim(origin, digest: Vec<u8>) -> Result {\\n    ensure!(<Proofs<T>>::exists(&digest), \\"This digest has not been claimed yet\\");\\n\\n    let sender = ensure_signed(origin)?;\\n    let (owner, _time) = Self::proofs(&digest);\\n\\n    ensure!(sender == owner, \\"You must own this claim to revoke it\\");\\n\\n    <Proofs<T>>::remove(&digest);\\n    <balances::Module<T>>::increase_free_balance_creating(&sender, <T::Balance as As<u64>>::sa(POE_FEE));\\n\\n    Self::deposit_event(RawEvent::ClaimRevoked(sender, digest));\\n    Ok(())\\n}\\n```\\n\\nWe importantly check that the owner of the file is the same as the sender requesting to revoke the claim. After we remove the mapping from storage, we then return the funds which were initially spent to create the claim.\\n\\nBoth the `create_claim()` and the `revoke_claim()` function ends with an event being generated, which reports to the outside world when a user makes or revokes a claim. This event can be used to detect a successful transaction, or lack thereof.\\n\\nIn addition to the `proof_of_existence.rs` file we created, we also needed to modify other parts of the `substrate-node-template` to integrate this new module into our blockchain. You can learn more about those details by [completing the tutorial](https://substrate.readme.io/v1.0.0/docs/creating-a-custom-substrate-chain#section-step-4-integrate-our-new-module-into-our-runtime) that we previously referenced.\\n\\n## Creating a User Interface for Our Chain\\n\\nNow that we have completed our custom runtime, we need a way to allow our users to easily interact with this new functionality. For that, we will modify the [substrate-ui](https://github.com/paritytech/substrate-ui/tree/substrate-node-template) that we generated earlier.\\n\\nThe [custom runtime tutorial](https://substrate.readme.io/v1.0.0/docs/creating-a-custom-substrate-chain#section-step-7-updating-our-substrate-ui) already showed you some of the basics of modifying the `substrate-ui`, but for our needs, we will need to write some custom React components.\\n\\nHere is what our segment in the `app.jsx` file will look like:\\n\\n```jsx\\n{% raw %}\\n<Segment style={{ margin: \'1em\' }} padded>\\n\\t<Header as=\'h2\'>\\n\\t\\t<Icon name=\'lock\' />\\n\\t\\t<Header.Content>\\n\\t\\t\\tProof of Existence\\n\\t\\t\\t<Header.Subheader>Claim ownership of your digital files</Header.Subheader>\\n\\t\\t</Header.Content>\\n\\t</Header>\\n\\t<div style={{ paddingBottom: \'1em\' }}>\\n\\t\\t<div style={{ fontSize: \'small\' }}>Owner</div>\\n\\t\\t<SignerBond bond={this.poeAccount} />\\n\\t\\t<If condition={this.poeAccount.ready()} then={<span>\\n\\t\\t\\t<Label>Balance\\n\\t\\t\\t\\t<Label.Detail>\\n\\t\\t\\t\\t\\t<Pretty value={runtime.balances.balance(this.poeAccount)} />\\n\\t\\t\\t\\t</Label.Detail>\\n\\t\\t\\t</Label>\\n\\t\\t</span>} />\\n\\t</div>\\n\\t<div>\\n\\t\\t<FileDigestBond bond={this.poeDigest} content=\'Select File\' />\\n\\t\\t<DigestTag value={runtime.proof_of_existence.proofs(this.poeDigest)} account={this.poeAccount} />\\n\\t</div>\\n\\t<TransactButton\\n\\t\\tcontent=\\"Claim File\\"\\n\\t\\ticon=\'lock\'\\n\\t\\ttx={{\\n\\t\\t\\tsender: this.poeAccount,\\n\\t\\t\\tcall: calls.proof_of_existence.createClaim(this.poeDigest)\\n\\t\\t}}\\n\\t/>\\n</Segment>\\n{% endraw %}\\n```\\n\\n### Custom Components\\n\\nYou can see we added two new components: the `FileDigestBond` and the `DigestTag`.\\n\\nThe `FileDigestBond` is very similar to the `FileUploadBond` which is provided as a part of the `substrate-ui`, but instead of returning the content of the file, it will return the digest by simply hashing that content. We can clone the `FileUploadBond` component, and modify a small section like so:\\n\\n```javascript\\nif (file) {\\n  this.state.name = file.name;\\n  var fileReader = new FileReader();\\n  fileReader.onloadend = (e) => {\\n    let fileContents = new Uint8Array(e.target.result);\\n    let fileDigest = \\"0x\\" + XXH.h64(fileContents.buffer, 0).toString(16);\\n    this.props.bond.trigger(fileDigest);\\n    this.setState({ digest: fileDigest });\\n  };\\n  fileReader.readAsArrayBuffer(file);\\n}\\n```\\n\\nThen to convey the current ownership status of a file, we create a fully custom `DigestTag` component which will show who owns a file (if anyone) and when they claimed it. Here is what that looks like:\\n\\n```javascript\\nimport React from \\"react\\";\\nimport { ReactiveComponent } from \\"oo7-react\\";\\nimport { Label } from \\"semantic-ui-react\\";\\nimport { ss58Encode } from \\"oo7-substrate\\";\\n\\nexport class DigestTag extends ReactiveComponent {\\n  constructor() {\\n    super([\\"value\\", \\"account\\"]);\\n  }\\n\\n  readyRender() {\\n    if (this.state.value) {\\n      let time = this.state.value[1];\\n\\n      // Check if time is 0, which implies not claimed\\n      if (time.number == 0) {\\n        return (\\n          <Label basic color=\\"green\\" pointing=\\"left\\">\\n            <span>Unclaimed!</span>\\n          </Label>\\n        );\\n      } else {\\n        let owner = ss58Encode(this.state.value[0]);\\n\\n        if (ss58Encode(this.state.account) == owner) {\\n          return (\\n            <Label basic color=\\"green\\" pointing=\\"left\\">\\n              <span>\\n                Owner: You!&emsp;|&emsp;When: {time.toLocaleDateString()}\\n              </span>\\n            </Label>\\n          );\\n        } else {\\n          return (\\n            <Label basic color=\\"red\\" pointing=\\"left\\">\\n              <span>\\n                Owner: {owner.substring(0, 8) + \\"\u2026\\"}&emsp;|&emsp;When:{\\" \\"}\\n                {time.toLocaleDateString()}\\n              </span>\\n            </Label>\\n          );\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nIf we put everything together, and run it, we will get something like this:\\n\\n![GIF of using the Proof of Existence UI](/assets/images/poe-example.gif)\\n\\n## Final Thoughts and Next Steps\\n\\nYou can see that with relatively little effort, the Substrate framework allows you to build custom blockchains. Substrate itself provides you with all the infrastructure required to set up a node. The Substrate Runtime Module Library (SRML) gives you a package of pre-built tools and features that you can easily add to your blockchain. The `substrate-node-template` can help you immediately get started with development, and the `substrate-ui` can enable you or other users to quickly interact with your chain.\\n\\nObviously I have picked a very simple project to implement here, but this process has itself been very eye opening to the power of the platform, and has allowed me to become more familiar with these cutting edge tools. But we are not done here. As someone tinkering around, I feel pretty satisfied with my local setup, but this is a platform built to for production ready, scalable blockchains.\\n\\nThe next steps for me will be to investigate:\\n\\n- UI for revoking a claim\\n- Token distribution patterns (maybe a timed faucet?)\\n- Setting up bootnodes on a cloud provider\\n- Creating/distributing binaries for others to run a node\\n- ???\\n\\nAs I tackle these different problems, I will make additional posts to keep you updated with what I learn along the way. You can always follow this project right on my GitHub: [https://github.com/shawntabrizi/substrate-proof-of-existence](https://github.com/shawntabrizi/substrate-proof-of-existence). If you enjoy this content, or are looking forward to reading more about Substrate and blockchain development, feel free to check out my [donations page](https://shawntabrizi.com/donate/). Thanks!"},{"id":"/ethereum/verify-ethereum-contracts-using-web3-js-and-solc/","metadata":{"permalink":"/blog/ethereum/verify-ethereum-contracts-using-web3-js-and-solc/","source":"@site/blog/2018-11-30-verify-ethereum-contracts-using-web3-js-and-solc.md","title":"Verify Ethereum Contracts Using Web3.js and Solc","description":"This tutorial will show you how you can verify the source code of an Ethereum contract using Web3.js and Solc-JS, similar to Etherscan.io.","date":"2018-11-30T18:20:58.000Z","tags":[{"inline":true,"label":"bytecode","permalink":"/blog/tags/bytecode"},{"inline":true,"label":"etherscan","permalink":"/blog/tags/etherscan"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"solidity","permalink":"/blog/tags/solidity"},{"inline":true,"label":"verification","permalink":"/blog/tags/verification"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"}],"readingTime":8.65,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Verify Ethereum Contracts Using Web3.js and Solc","date":"2018-11-30T18:20:58.000Z","authors":"shawntabrizi","slug":"/ethereum/verify-ethereum-contracts-using-web3-js-and-solc/","categories":["Ethereum"],"tags":["bytecode","etherscan","javascript","solidity","verification","web3"],"github":"ethereum-bytecode-verifier-console"},"unlisted":false,"prevItem":{"title":"Proof of Existence Blockchain with Substrate","permalink":"/blog/substrate/proof-of-existence-blockchain-with-substrate/"},"nextItem":{"title":"Combining Rocket with Reqwest to Call an API with Rust","permalink":"/blog/code/combining-rocket-with-reqwest-to-call-an-api-with-rust/"}},"content":"##### This tutorial will show you how you can verify the source code of an Ethereum contract using Web3.js and Solc-JS, similar to Etherscan.io.\\n\\nRecently I have been looking into the process of contract verification for Solidity contracts deployed to the Ethereum network. Many of you might be familiar with this [process on Etherscan.io](https://etherscan.io/verifyContract2) where users can submit the source code to their contracts in order to [get a \\"verified contract\\" status](https://etherscan.io/contractsVerified) on the site.\\n\\nAt first I thought this process would be pretty trivial since the bytecode for any contract is publicly available through the Web3 APIs, and the Solc compiler is pretty straightforward to use. However, if you try and do the naive approach, you will not get results that match! There are some small details about contract verification that are not really well documented today, and I hope to address that in this post, teaching you what I learned looking into these issues myself.\\n\\nBefore I dive in, I want to give a big shout out to the ConsenSys Diligence project [bytecode-verifier](https://github.com/ConsenSys/bytecode-verifier), whose source code ultimately helped me unblock some of the problems I was running into. Unfortunately this project is a bit out of date and won\'t work with the latest Solidity contracts, but we will address those problems here. (As of this writing, they support up to v0.4.21) I have written my own minimal version of the same fundamental program [here](https://github.com/shawntabrizi/ethereum-bytecode-verifier-console) with the updates I mention in this post.\\n\\n## Using the Right Solidity Compiler Version\\n\\nThe first issue I ran into while trying to automate the contract verification process was the problem of using the right Solc version for a given contract. As you may know, the Solidity compiler and even the Solidity programming language is constantly getting updated through [new releases of the software](https://github.com/ethereum/solidity/releases). Depending on which compiler version you use, the resulting bytecode of your contract may change. If you are trying to verify an Ethereum contract, it is important that you have knowledge of which version you need to use to get the correct results. Unfortunately, the common practice for solidity developers is to include a carrot (`^`) in their `pragma solidity` statement (e.g. `pragma solidity ^0.4.16`). This allows the same source code to be compiled with Solidity compilers greater than 0.4.16 without throwing an error. This means to get the right results, you will need to know what compiler version was used to actually generate the bytecode, and not rely completely on the source code.\\n\\nAnother issue is that there is no backwards compatibility in the Solidity compiler. This can be quite annoying if you are trying to automate this process since you will need to have multiple Solc binaries downloaded and managed on your computer. Fortunately Solc-JS can come to the rescue here.\\n\\n### Using a Legacy Version of Solidity with Solc-JS\\n\\nSolc-JS is a set of JavaScript bindings for the Solidity compiler. Built into the library is a version manager [briefly documented here](https://github.com/ethereum/solc-js#using-a-legacy-version):\\n\\n> In order to compile contracts using a specific version of Solidity, the `solc.loadRemoteVersion(version, callback)` method is available. This returns a new solc object that uses a version of the compiler specified.\\n\\nHere is a minimal example of how this function could be used:\\n\\n```javascript\\nconst solc = require(\'solc\');\\n\\nvar solc_version = \\"v0.4.16+commit.d7661dd9\\"\\n\\nsolc.loadRemoteVersion(solc_version, function (err, solc_specific) {\\n    if(!err) {\\n        var output = solc_specific.compile(\\"contract t { function g() {} }\\", 1)\\n    }\\n}\\n```\\n\\nIn the background, the Solc-JS bindings manages loading and using the right Solidity compiler, and simply gives you the binary output you would expect! You can find all the builds of solidity [here](https://ethereum.github.io/solc-bin/bin/list.json) and simply replace the `solc_version` variable with what you need.\\n\\n## Compiling a Contract with Multiple Solidity Files\\n\\nThe next thing we may commonly encounter when trying to do contract verification is handling projects which separate their code into multiple distinct files, and connects them using an `import` statement. Fortunately, the Solc-JS library also handles this. Note that when a contract does an `import` to another solidity file, it is really just concatenating the files, so if this part gives you trouble, or you want to simplify your process a bit, you can just merge all the different solidity files into a single file with multiple contracts. This is how the source code is shown on Etherscan.io.\\n\\nIf you do want to compile Solidity contracts broken up into multiple files, this code snippet should work for you assuming all the files are in the same directory:\\n\\n```javascript\\nvar solc = require(\\"solc\\");\\nvar fs = require(\\"fs\\");\\n\\nvar solc_version = \\"v0.4.16+commit.d7661dd9\\";\\nvar contracts_directory = \\"./contracts\\";\\nvar contract_name = \\"MyContract\\";\\nvar contract_filename = \\"MyContract.sol\\";\\nvar is_optimized = 1;\\n\\nvar input = {};\\n\\nvar files = fs.readdirSync(contracts_directory);\\n\\nfor (file in files) {\\n  let item = files[file];\\n  if (item.slice(-4) == \\".sol\\") {\\n    let file_path = contracts_directory + \\"/\\" + item;\\n    input[item] = fs.readFileSync(file_path, \\"utf8\\");\\n  }\\n}\\n\\nsolc.loadRemoteVersion(solc_version, function (err, solc_specific) {\\n  if (!err) {\\n    var output = JSON.parse(\\n      solc_specific.lowlevel.compileMulti(\\n        JSON.stringify({ sources: input }),\\n        is_optimized\\n      )\\n    );\\n    var bytecode =\\n      output[\\"contracts\\"][contract_filename + \\":\\" + contract_name][\\n        \\"runtimeBytecode\\"\\n      ];\\n    console.log(bytecode);\\n  }\\n});\\n```\\n\\nYou will note that we had to also specify the name of the final contract we want to compile and the name of the file which contains that contract code. There are probably ways to simplify this in your automation, but for the sake of making things general and easy to understand, all of these variables are defined upfront.\\n\\n## Getting the Bytecode of an Existing Ethereum Contract\\n\\nSo now that we know how to generate the bytecode from the smartcontract\'s source, we need to be able to retrieve the existing bytecode which is on the blockchain to compare it to. Web3.js provides a function called [`getCode()`](https://web3js.readthedocs.io/en/1.0/web3-eth.html#getcode) which returns the bytecode of a contract at a specific address. It looks like this:\\n\\n```javascript\\nweb3.eth.getCode(\\"0xd5677cf67b5aa051bb40496e68ad359eb97cfbf8\\")\\n.then(console.log);\\n\\n> \\"0x600160008035811a818181146012578301005b601b6001356025565b8060005260206000f25b600060078202905091905056\\"\\n```\\n\\nDead simple.\\n\\n## The Naive Approach Does Not Work\\n\\nSo what would happen if we use these two new skills and tried to verify a known Ethereum contract? For no particular reason, we will try and verify the source code for the ChainLink Token (LINK).\\n\\nIf you copy the code and compiler settings of the `LinkToken` contract [from Etherscan](https://etherscan.io/address/0x514910771af9ca656af840dff83e8264ecf986ca#code) and use Solc-JS to compile it. You should get the following bytecode:\\n\\n```\\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\\n```\\n\\nHowever if we get the bytecode from Ethereum, it is different!\\n\\n```javascript\\nweb3.eth\\n  .getCode(\\"0x514910771AF9Ca656af840dff83E8264EcF986CA\\")\\n  .then(console.log);\\n```\\n\\n```\\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\\n```\\n\\nIgnoring the `0x` at the front, we can still find that the end of each bytecode is different: `...37bc6e3d3877380029` vs `...27c7374acad8d60029`.\\n\\nSo what\'s going on?\\n\\n## Contract Metadata\\n\\nWhat you are seeing here are some of the artifacts of the [contract metadata](https://solidity.readthedocs.io/en/v0.4.24/metadata.html) which gets generated by the solidity compiler. At the end of the bytecode, the Solidity compiler appends a Swarm hash of the metadata file that gets generated at compile time. This section always starts with with `a165627a7a72305820` which is [translated from `0xa1 0x65 \'b\' \'z\' \'z\' \'r\' \'0\' 0x58 0x20`](https://solidity.readthedocs.io/en/v0.4.24/metadata.html#encoding-of-the-metadata-hash-in-the-bytecode).\\n\\nSo ultimately we don\'t want to directly compare the two resulting bytecodes, but the specific parts of the bytecode which represent the smart contract\'s logic. Fortunately, just like this Swarm hash which is appended at the end of the contract, there is also a common substring in the bytecode which indicates the beginning of the contract: `6060604052`. This bytecode translates to the assembly code which initializes the [\\"free memory pointer\\"](https://solidity.readthedocs.io/en/v0.4.24/assembly.html#conventions-in-solidity):\\n\\n    PUSH1 0x60 PUSH1 0x40 MSTORE\\n\\nSo now, if we only compare the bytecode starting with `6060604052...` and ending with `...a165627a7a72305820` we will actually find that these two bytecodes match!\\n\\n## Handling Changes to Metadata from Different Solidity Versions\\n\\nUnfortunately this is not the end of the story. As the Solidity compiler has changed, so has some of the rules around the contract metadata. For example, before Solidity v0.4.7, there was no metadata! So if we tried to do the naive approach on a very old contract, it would have worked!\\n\\nAlso, after Solidity v0.4.22, the free memory pointer initialization changes from `6060604052...` to `6080604052...`. I am not sure [why this changed](https://ethereum.stackexchange.com/questions/63117/when-did-the-ethereum-free-memory-pointer-change-6060-6080), but it means we will need to add a few conditionals to our general source code verification program.\\n\\n## Generally Working Verification Code\\n\\nSo now that we know how to get the results we want, we can write some code to automate this process. Here is a minimal working example:\\n\\n```javascript\\nvar solc = require(\\"solc\\");\\nvar fs = require(\\"fs\\");\\nvar Web3 = require(\\"web3\\");\\n\\nvar web3 = new Web3(\\n  new Web3.providers.HttpProvider(\\"https://mainnet.infura.io\\")\\n);\\n\\nvar solc_version = \\"v0.4.16+commit.d7661dd9\\";\\nvar contracts_directory = \\"./contracts\\";\\nvar contract_name = \\"LinkToken\\";\\nvar contract_filename = \\"LinkToken.sol\\";\\nvar is_optimized = 1;\\n\\nvar contract_address = \\"0x514910771AF9Ca656af840dff83E8264EcF986CA\\";\\n\\nvar input = {};\\n\\nvar files = fs.readdirSync(contracts_directory);\\n\\nfor (file in files) {\\n  let item = files[file];\\n  if (item.slice(-4) == \\".sol\\") {\\n    let file_path = contracts_directory + \\"/\\" + item;\\n    input[item] = fs.readFileSync(file_path, \\"utf8\\");\\n  }\\n}\\n\\nsolc.loadRemoteVersion(solc_version, async function (err, solc_specific) {\\n  if (!err) {\\n    var output = JSON.parse(\\n      solc_specific.lowlevel.compileMulti(\\n        JSON.stringify({ sources: input }),\\n        is_optimized\\n      )\\n    );\\n    var compiled_bytecode =\\n      \\"0x\\" +\\n      output[\\"contracts\\"][contract_filename + \\":\\" + contract_name][\\n        \\"runtimeBytecode\\"\\n      ];\\n    var blockchain_bytecode = await web3.eth.getCode(contract_address);\\n\\n    processed_compiled_bytecode = processBytecode(compiled_bytecode);\\n    processed_blockchain_bytecode = processBytecode(blockchain_bytecode);\\n\\n    if (processed_blockchain_bytecode == processed_compiled_bytecode) {\\n      console.log(\\"Verified!\\");\\n    } else {\\n      console.log(\\"Not Verified\\");\\n    }\\n  }\\n});\\n\\nfunction processBytecode(bytecode) {\\n  // Semantic versioning\\n  let solc_minor = parseInt(\\n    solc_version\\n      .match(/v\\\\d+?\\\\.\\\\d+?\\\\.\\\\d+?[+-]/gi)[0]\\n      .match(/\\\\.\\\\d+/g)[0]\\n      .slice(1)\\n  );\\n  let solc_patch = parseInt(\\n    solc_version\\n      .match(/v\\\\d+?\\\\.\\\\d+?\\\\.\\\\d+?[+-]/gi)[0]\\n      .match(/\\\\.\\\\d+/g)[1]\\n      .slice(1)\\n  );\\n\\n  if (solc_minor >= 4 && solc_patch >= 22) {\\n    var starting_point = bytecode.lastIndexOf(\\"6080604052\\");\\n    var ending_point = bytecode.search(\\"a165627a7a72305820\\");\\n    return bytecode.slice(starting_point, ending_point);\\n  } else if (solc_minor >= 4 && solc_patch >= 7) {\\n    var starting_point = bytecode.lastIndexOf(\\"6060604052\\");\\n    var ending_point = bytecode.search(\\"a165627a7a72305820\\");\\n    return bytecode.slice(starting_point, ending_point);\\n  } else {\\n    return bytecode;\\n  }\\n}\\n```\\n\\nAnd the result?\\n\\n```\\nC:\\\\Temp\\\\solc\\\\node_project>node index.js\\nVerified!\\n```\\n\\nAs I mentioned before, the main source which unblocked me trying to do this process was the [ConsenSys/bytecode-verifier](https://github.com/ConsenSys/bytecode-verifier) project. However, their code is a bit out of date, and also contains some non-essential parts. I attempted to simply the code in my own version of the blockchain verifier: [shawntabrizi/ethereum-bytecode-verifier-console](https://github.com/shawntabrizi/ethereum-bytecode-verifier-console).\\n\\nThis code has not been battle tested, but I plan on iterating on it and using it for a larger project I am working on. Feel free to grab it and modify it as you need!\\n\\nI hope that this post has taught you something new and possibly helped you answer similar questions that I had! If you enjoyed this content, feel free to check out my [donations page](https://shawntabrizi.com/donate/)."},{"id":"/code/combining-rocket-with-reqwest-to-call-an-api-with-rust/","metadata":{"permalink":"/blog/code/combining-rocket-with-reqwest-to-call-an-api-with-rust/","source":"@site/blog/2018-10-21-combining-rocket-with-reqwest-to-call-an-api-with-rust.md","title":"Combining Rocket with Reqwest to Call an API with Rust","description":"This post will be a short code snippet to show how you can combine the Dynamic Segments example from Rocket and the Calling a Web API example from the Rust Cookbook.","date":"2018-10-22T03:28:05.000Z","tags":[{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"reqwest","permalink":"/blog/tags/reqwest"},{"inline":true,"label":"rocket","permalink":"/blog/tags/rocket"},{"inline":true,"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":2.705,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Combining Rocket with Reqwest to Call an API with Rust","date":"2018-10-22T03:28:05.000Z","authors":"shawntabrizi","slug":"/code/combining-rocket-with-reqwest-to-call-an-api-with-rust/","categories":["Code"],"tags":["programming","reqwest","rocket","rust"]},"unlisted":false,"prevItem":{"title":"Verify Ethereum Contracts Using Web3.js and Solc","permalink":"/blog/ethereum/verify-ethereum-contracts-using-web3-js-and-solc/"},"nextItem":{"title":"Running Parity Substrate on Mac OS X","permalink":"/blog/substrate/running-parity-substrate-on-mac-os-x/"}},"content":"##### This post will be a short code snippet to show how you can combine the [_Dynamic Segments_ example from Rocket](https://rocket.rs/guide/requests/#dynamic-segments) and the [_Calling a Web API_ example from the Rust Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/web/clients/apis.html).\\n\\nI wanted to start playing around with Rust, so I was given a project to build a web application which would interact with downstream apis and present a simple user interface. Unfortunately, it seems that some of these common use cases are not easily documented for new users like me!\\n\\nLet\u2019s fix that \ud83d\ude42\\n\\n## Creating a simple webpage in Rust\\n\\nWe will create a simple webpage which will call a downstream API, and return a result on the page. We will be using Rocket as our web framework and Reqwest as our HTTP client since Rocket only provides an HTTP server.\\n\\nOur example will follow exactly the basic examples from these two sources, but combine them to work together in an easy to understand way.\\n\\nThe Rocket tutorial shows you how you can set up dynamic segments to be able to allow your program to accept a path segment as a variable in your function. In the tutorial we use this to be able to say \u201cHello, NAME\u201d. The Reqwest tutorial shows you how you can check if a certain user exists on GitHub by querying their GitHub Users Endpoint using a HEAD request.\\n\\nSo let\u2019s combine this to create a site where you can navigate to `/check/<username>` and it will tell you if the user exists on GitHub or not!\\n\\nI won\u2019t be diving into the code line by line, since you can read from the tutorials what the different pieces do, but I do want to give you a working code snippet which does just what I said:\\n\\n### Cargo.toml\\n\\n```\\n\u2026\\n[dependencies]\\nrocket = \u201c0.3.17\u201d\\nrocket_codegen = \u201c0.3.17\u201d\\nreqwest = \u201c0.9.3\u201d\\n```\\n\\n### src/main.rs\\n\\n```rust\\n#![feature(plugin)]\\n#![plugin(rocket_codegen)]\\n\\nextern crate reqwest;\\nextern crate rocket;\\n\\nuse rocket::http::RawStr;\\n\\nuse std::time::Duration;\\nuse reqwest::ClientBuilder;\\n\\n\\n#[get(\\"/\\")]\\nfn index() -> &\'static str {\\n    \\"Navigate to http://localhost:8000/check/<type your GitHub name here> to check that everything is working!\\"\\n}\\n\\n#[get(\\"/check/<user>\\")]\\nfn check(user: &RawStr) -> Result<String, Box<std::error::Error>> {\\n    let request_url = format!(\\"https://api.github.com/users/{}\\", user);\\n\\n    let timeout = Duration::new(5, 0);\\n    let client = ClientBuilder::new().timeout(timeout).build()?;\\n    let response = client.head(&request_url).send()?;\\n\\n    if response.status().is_success() {\\n        Ok(format!(\\"{} is a user!\\", user))\\n    } else {\\n        Ok(format!(\\"{} is not a user!\\", user))\\n    }\\n}\\n\\nfn main() {\\n    rocket::ignite().mount(\\"/\\", routes![index, check]).launch();\\n}\\n```\\n\\n## Testing your webpage\\n\\nAfter you have copied these items into your project, you can simply run `cargo run` to spin up your local web server.\\n\\n![](/assets/images/img_5bcd419da1b02.png)\\n\\nThe index page at `http://localhost:8000/` will tell you to navigate to `http://localhost:8000/check/<username>` where you should fill in the GitHub username you want to check. For me, this is: `http://localhost:8000/check/shawntabrizi`, which gives me the following confirmation!\\n\\n![](/assets/images/img_5bcd41b64c8db.png)\\n\\nIf I use a username which doesn\'t exist like `http://localhost:8000/check/shawnfabreezy`, again I get the expected message:\\n\\n![](/assets/images/img_5bcd41cdc9c8e.png)\\n\\n## More to come!\\n\\nI hope that you found this little tutorial helpful. Getting started with Rust can be a little frustrating due to many compile time checks which occur, but sometimes you just need some running code to really get started. If you want to support this blog and other posts like this, feel free to check out [my donations page](/donate/)."},{"id":"/substrate/running-parity-substrate-on-mac-os-x/","metadata":{"permalink":"/blog/substrate/running-parity-substrate-on-mac-os-x/","source":"@site/blog/2018-10-11-running-parity-substrate-on-mac-os-x.md","title":"Running Parity Substrate on Mac OS X","description":"This guide will show you the steps to successfully connect to the Substrate testnet, BBQ Birch using Mac OS X.","date":"2018-10-11T23:06:45.000Z","tags":[{"inline":true,"label":"bbq birch","permalink":"/blog/tags/bbq-birch"},{"inline":true,"label":"osx","permalink":"/blog/tags/osx"},{"inline":true,"label":"parity","permalink":"/blog/tags/parity"},{"inline":true,"label":"polkadot","permalink":"/blog/tags/polkadot"},{"inline":true,"label":"substrate","permalink":"/blog/tags/substrate"}],"readingTime":1.47,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Running Parity Substrate on Mac OS X","date":"2018-10-11T23:06:45.000Z","authors":"shawntabrizi","slug":"/substrate/running-parity-substrate-on-mac-os-x/","categories":["Substrate"],"tags":["bbq birch","osx","parity","polkadot","substrate"]},"unlisted":false,"prevItem":{"title":"Combining Rocket with Reqwest to Call an API with Rust","permalink":"/blog/code/combining-rocket-with-reqwest-to-call-an-api-with-rust/"},"nextItem":{"title":"4 Things I Learned in 4 Years at Microsoft","permalink":"/blog/personal/4-things-i-learned-in-4-years-at-microsoft/"}},"content":"This guide will show you the steps to successfully connect to the Substrate testnet, BBQ Birch using Mac OS X.\\n\\nAll of the instructions on this page are commands which should be copied into your [Terminal application](https://support.apple.com/guide/terminal/welcome/mac).\\n\\n## Prerequisites\\n\\nFirst we need to install [Rust](https://www.rust-lang.org/). Open the terminal app on your Mac, and run the following:\\n\\n```bash\\ncurl https://sh.rustup.rs -sSf | sh\\n```\\n\\nFollow the instructions that appear to complete the installation.\\n\\n> At the end of this installation, you will need to run `source $HOME/.cargo/env` or log out and back into your computer to have your terminal understand `rustup`/`cargo` commands.\\n\\nTo enable [Rust to compile to WebAssembly](https://www.hellorust.com/news/native-wasm-target.html), run the following:\\n\\n```bash\\nrustup update nightly\\nrustup target add wasm32-unknown-unknown --toolchain nightly\\ncargo install --git https://github.com/alexcrichton/wasm-gc\\n```\\n\\nInstall the [Homebrew](https://brew.sh/) package manager for OS X:\\n\\n```bash\\n/usr/bin/ruby -e \\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\\"\\n```\\n\\nFollow the instructions that appear to complete the installation.\\n\\nFinally install some general packages required for running this software:\\n\\n```bash\\nbrew install cmake pkg-config openssl git\\n```\\n\\n> Click each package to learn more about it: [cmake](https://cmake.org/), [pkg-config](https://www.freedesktop.org/wiki/Software/pkg-config/), [openssl](https://www.openssl.org/), [git](https://git-scm.com/)\\n\\n## Installing Substrate\\n\\nNavigate to the folder of your choice, and clone the Substrate repository there:\\n\\n```bash\\ngit clone https://github.com/paritytech/substrate.git\\ncd substrate\\n```\\n\\nThen build the WebAssembly binaries required for Substrate:\\n\\n```bash\\n./scripts/build.sh\\n```\\n\\nFinally build the Rust native code:\\n\\n```bash\\ncargo build\\n```\\n\\n> Optional: If you want to make sure everything got installed correctly, you can run the full suite of Substrate tests:\\n>\\n> ```bash\\n> cargo test --all\\n> ```\\n\\n## Running the Substrate Testnet: BBQ Birch\\n\\nTo start syncing your BBQ Birch node, simply type:\\n\\n```bash\\ncargo run\\n```\\n\\n![Screenshot of BBQ Birch running in Terminal](https://i.imgur.com/jxqqr9Q.png)\\n\\n## What next?\\n\\nMore documentation to come on interacting with the network!"},{"id":"/personal/4-things-i-learned-in-4-years-at-microsoft/","metadata":{"permalink":"/blog/personal/4-things-i-learned-in-4-years-at-microsoft/","source":"@site/blog/2018-09-27-4-things-i-learned-in-4-years-at-microsoft.md","title":"4 Things I Learned in 4 Years at Microsoft","description":"I am in the midst of a turning point in my life. This month I left Microsoft to join Parity Technologies, a start-up company focusing on blockchain infrastructure and the future of the decentralized web.","date":"2018-09-27T17:02:50.000Z","tags":[{"inline":true,"label":"lessons","permalink":"/blog/tags/lessons"},{"inline":true,"label":"life","permalink":"/blog/tags/life"},{"inline":true,"label":"microsoft","permalink":"/blog/tags/microsoft"},{"inline":true,"label":"parity","permalink":"/blog/tags/parity"}],"readingTime":8.81,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"4 Things I Learned in 4 Years at Microsoft","date":"2018-09-27T17:02:50.000Z","authors":"shawntabrizi","slug":"/personal/4-things-i-learned-in-4-years-at-microsoft/","categories":["Personal"],"tags":["lessons","life","microsoft","parity"]},"unlisted":false,"prevItem":{"title":"Running Parity Substrate on Mac OS X","permalink":"/blog/substrate/running-parity-substrate-on-mac-os-x/"},"nextItem":{"title":"Getting Twitter posts for Ethereum using an Oracle","permalink":"/blog/ethereum/getting-twitter-posts-for-ethereum-using-an-oracle/"}},"content":"I am in the midst of a turning point in my life. This month I left Microsoft to join [Parity Technologies](https://www.parity.io/), a start-up company focusing on blockchain infrastructure and the future of the decentralized web.\\n\\n![](/assets/images/img_5bad0b7eeb066.png)\\n\\nIt feels a bit like when I graduated from university, and in a way it is exactly the same! I spent 4 years at Microsoft, met new friend, lived in a new city, and learned new things literally every day. As a recent graduate of \'Microsoft University\', I thought it would be important to reflect on some of the big picture items I learned during my time there. These learnings will be more opinions rather than facts, but nonetheless, things that I will bring with me to this next adventure and continue to develop as I grow in this new role.\\n\\n## Customer Obsession Isn\'t Just a Phrase\\n\\nI joined Microsoft the same year that [Satya Nadella](https://en.wikipedia.org/wiki/Satya_Nadella) became CEO. I distinctly remember his internal speeches when he emphasized the importance of \\"customer obsession\\". In his own words from his book _Hit Refresh_:\\n\\n> First, we need to obsess about our customers. At the core of our business must be the curiosity and desire to meet a customer\u2019s unarticulated and unmet needs...\\n\\nAs a new employee to a large company, I was a little skeptical of this attitude being more talk than action, but I can tell you from my personal experience, customer obsession isn\'t just a phrase. I actually spent a full 2 months at Microsoft learning about the [\\"moneyball\\"](https://en.wikipedia.org/wiki/Moneyball)/[\\"signal\\"](https://en.wikipedia.org/wiki/The_Signal_and_the_Noise) process, and it really has shaped the way that I approach product development and project management. The successful product teams I saw at Microsoft were always able to answer the following questions:\\n\\n- What is the top frustration of your customers?\\n- What are some solutions to those frustrations?\\n- How are you experimenting and measuring those solutions?\\n\\nThe key aspect of these questions is that it starts at the customers who use your products. Many times product owners use their own anecdotal experiences to determine what they should be working on and what their customers are experiencing, but this is a setup for failure. When you are too close to a product, all of your view points become skewed, and you lose sight of what the average customer experiences. I have seen teams spend all their efforts adding new features to a product, when in fact, there were so many problems with sign-up and onboarding that people would likely never even get that far before they gave up! I am not saying that you should mindlessly do everything your customers say, but you do in fact need to understand their perspective and allow that to shape your short and long term plans.\\n\\n## Be Your Number One Customer\\n\\nI mentioned in the previous learning that product owners are often too close to what they are working on to be able to independently understand how their product is actually used. But this is not a bad thing; in fact, I think this is critical in driving a successful product. As a product owner, you need to be your number one customer. You need to be a power user; the person that begs for more features; the person who understands all the bugs and complexities of your software; the person who has built hacks or scripts to overcome limitations. This is a perspective needed to grow your product beyond its current scope and provide vision for what it could become.\\n\\nToo often at Microsoft, I found people who spent their 9-5 working on a product they never used themselves! Whether they wrote code or wrote specs, these people had a task to get done, and usually a small list of check boxes to determine when they were complete. You can imagine that this leads to disorienting products where features are stitched together in a UI rather than building an end to end story for the user. Furthermore, you can\'t expect these individuals to be able to talk on the same frequency as their users, so how can they engage them to learn more?\\n\\nOne of the criticisms I have toward working at Microsoft is that they are very enterprise oriented, and as a result, it is especially hard for individuals to become power users. For example, if you are working on enterprise identity systems, then your customers are people managing thousands of accounts within their organization, with dozens of custom RBAC settings, and even things like on-prem federation. It is not likely that a college hire will be able to easily emulate these behaviors, but in my experience, it is so critically important to try! I think to truly excel at your role, you need to immerse yourself in your product, and this might even require working on extra projects where you take on the \\"customer\\" role. Additionally, this might also mean that not everyone is a perfect fit for every job. To be most successful, you should look for teams where becoming the number one user is natural, rather than forced.\\n\\n## Product, People, and Pay\\n\\nContinuing from the last learning, the product you choose to work on can be an important factor to your success; but that is not the only factor. When choosing a team or job, you need to consider three main factors: product, people, and pay. This is one of those things that each person will need to evaluate for themselves, but I can share some of what I learned about myself in this regard.\\n\\nWhen graduating from college, the only factor that seemed relevant to me was pay. Being broke for 22 years straight meant that my bias toward making a good income and the ability to start saving outweighed most any other factor I could think of. I was and still am a Microsoft fanboy, so it helps that I did not have to look far to find a good fit, but even when I was talking with Microsoft, I don\'t remember thinking for a single moment about what I would actually work on. Just that I would have a stable, well paying job. However, salary at Microsoft is pretty fixed and independent of what you work on. So over time, I started to consider the other two factors.\\n\\nI was fortunate to start my career on a product I found pretty interesting. Identity, authentication, and authorization are technologies that everyone interacts with in the modern day. It is a subset of software security, and if done wrong, can either make customers hate you or lead to compromised accounts. I was generally happy working on the team, and found myself becoming an expert in certain topic areas due to my natural interests in the space. But the grass is always greener on the other side, and I wanted to try my hand on more cutting edge technologies like machine learning. I set out to find another role at Microsoft which would allow me to explore that technology, and I eventually switched to the Azure health and supportability team who was using ML to correlate large outages to specific and shared root causes. Unfortunately, it panned out that I did not get to touch ML while on the team, and that really made it evident how much product really was a factor for my happiness. If I go to work unexcited about what I am learning, or what our mission is, then it will be an uphill battle until the end of the day.\\n\\nBy switching teams, I also learned how much I undervalued the people I worked with. The team I moved to had a much different employee composition, and while I will avoid the specific details I never made close friends with people on this team like I did over in Identity. But that wasn\'t even the main problem. More importantly I learned how much managers and working relationships affected my happiness on a team. It will always happen that you are given a task that you won\'t enjoy, but it needs to come from people that you respect and enjoy working with. When things get hard, you need to know the people around you are there to support you rather than make things worse. These things sound obvious, but it is incredibly hard to evaluate the people on your team until you actually start working with them. This means that if you find people you like, don\'t keep that a secret. Even if your paths diverge, you never know when they will cross again, and you should keep them close. But also, don\'t be afraid to join new teams and try new things. While the goal is to maximize each variable for your needs, you will never quite have a metric to measure and compare until you have had a range of experiences.\\n\\n## You Can\'t Fuck Up\\n\\nThis is probably the lesson which I have the most to learn about, but my current outlook on life is that you can\'t fuck up. Even before I went to university, I knew I wanted to work in tech and in the world of software. However, life would have it that I ended up studying physics and math rather than computer science. At the time, I was so concerned that I would not be able to get a job out of undergrad due to my lack of formal computer science training. I really felt that I had fucked up 4 years of my life.\\n\\nBut I was wrong; everything turned out fine! I literally applied for one job, went end to end through the interview process, and landed a gig at my dream company. Then, when I switched teams at Microsoft, I gave up being a topic expert to explore interesting tech. That didn\'t pan out the way I hoped, and I thought I would be fucked again, but here I am, starting a new role outside of Microsoft having grown more as a result of my experiences. In my time at Microsoft, I have seen people literally \'fail upward\'; and while I am not claiming that is what happened to me, I certainly feel that having had these experiences, even negative, has made me grown as a person.\\n\\nAgain, these are my anecdotal experiences, and I am very willing to be shown wrong as I continue learn. But, I think my current attitudes toward work and life are very much shaped by these lessons. I am very excited for this next chapter, and I can\'t wait to keep learning."},{"id":"/ethereum/getting-twitter-posts-for-ethereum-using-an-oracle/","metadata":{"permalink":"/blog/ethereum/getting-twitter-posts-for-ethereum-using-an-oracle/","source":"@site/blog/2018-08-26-getting-twitter-posts-for-ethereum-using-an-oracle.md","title":"Getting Twitter posts for Ethereum using an Oracle","description":"This post will show you how to use the Oraclize.it blockchain oracle to get and store Twitter posts for Ethereum smart contracts.","date":"2018-08-27T05:16:09.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"oraclize","permalink":"/blog/tags/oraclize"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"smart contract","permalink":"/blog/tags/smart-contract"},{"inline":true,"label":"solidity","permalink":"/blog/tags/solidity"},{"inline":true,"label":"twitter","permalink":"/blog/tags/twitter"}],"readingTime":7.415,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Getting Twitter posts for Ethereum using an Oracle","date":"2018-08-27T05:16:09.000Z","authors":"shawntabrizi","slug":"/ethereum/getting-twitter-posts-for-ethereum-using-an-oracle/","categories":["Ethereum"],"tags":["blockchain","ethereum","oraclize","programming","smart contract","solidity","twitter"],"github":"Ethereum-Twitter-Bounty"},"unlisted":false,"prevItem":{"title":"4 Things I Learned in 4 Years at Microsoft","permalink":"/blog/personal/4-things-i-learned-in-4-years-at-microsoft/"},"nextItem":{"title":"Building a Simple Sign-In Page with MSAL.js for Microsoft Identities","permalink":"/blog/aad/building-a-simple-sign-in-page-with-msal-js-for-microsoft-identities/"}},"content":"##### This post will show you how to use the Oraclize.it blockchain oracle to get and store Twitter posts for Ethereum smart contracts.\\n\\nI recently created a end to end working dApp called [\\"Ethereum Twitter Bounty\\"](https://github.com/shawntabrizi/Ethereum-Twitter-Bounty) as the final project for the [ConsenSys 2018 Developer Program](https://consensys.net/academy/2018developer/). In short, this dApp is a bounty contract which allows people to to pay or get paid to make specific Twitter posts. Imagine a decentralized marketing service where companies or individuals can allow normal users to virally market their product by sharing it with their peers on social networks like Twitter.\\n\\n![](/assets/images/img_5b838a8f8a429.png)\\n\\nThe dApp is broken up into two different contracts: one that fetches and stores Twitter posts on the blockchain and one that manages the bounties and validates the fulfillment conditions.\\n\\nFulfillment in this contract is pretty straight-forward: we just need to check that the contents of one post is equal to the contents of another. But before we can even do that, we need to fetch the Twitter posts from the internet, and make them accessible to our blockchain contracts.\\n\\nThat is what we will be going over in this blog post.\\n\\n## Ethereum Oracles\\n\\nSmart Contracts cannot natively talk to the outside world. Any data which you want a smart contract to access must be available on the blockchain. This is a really common problem that comes up when developing new dApps. Things like the current USD price of Ether, generating random numbers, and finding out the result of a real-world event involves the use of an **Oracle**: an external service which provides data to the blockchain.\\n\\nOracles are actually pretty easy to understand. They simply listen for requests coming from the blockchain for their services, get the data requested, and return it back to the blockchain in the form of a transaction. Anyone could build their own Oracle service and use it for their own personal needs, but that can be pretty complicated or cumbersome for a user who simply wants to create an Ethereum smart contract and not maintain their own cloud service. Additionally, if you are building a decentralized application which uses a custom made oracle, who is to trust that you are not manipulating the results? You start to lose all of the benefits of smart contracts once you delegate results and processes to the outside world.\\n\\nThis is where [Oraclize.it](http://www.oraclize.it/) comes in. They are an open platform for fetching external data, and providing cryptographic proofs of the results. This is particularly important because smart contracts are often in control of a lot of money, and when it comes to triggering contract code, users should be certain of the data flowing into the contract. Oraclize.it also provides all of the libraries and samples required to get started very quickly, which was super convenient for building my final project.\\n\\nHowever, [their documentation](http://docs.oraclize.it/) was a little lacking, specifically when it came to trying out my specific scenario! Here is what they wrote about fetching Twitter posts using their HTML parser:\\n\\n> HTML Parser: helper is useful for HTML scraping. The desired XPATH can be specified as argument of `xpath(..)` as shown in the example:\\n>\\n> `html(https://twitter.com/oraclizeit/status/671316655893561344).xpath(//*[contains(@class, \'tweet-text\')]/text())`\\n\\nYou can actually try this out really quickly on their [Test Query](http://app.oraclize.it/home/test_query) page, and on their particular example, it does work okay... but as soon as you try another Twitter posts, things break down quickly. Here is an example of why this XPATH query is bad:\\n\\n![](/assets/images/img_5b833f04ca84f.png)\\n\\nYou can see that it is capturing much more than just the original twitter post. All of the replies to the main post also contain the `tweet-text` class which means that they get picked up by the Oraclize query. Not only does this cost a TON more gas to save to the blockchain, it adds a ton of data which will make it incredibly complicated to validate when a user makes a copy of the post for our scenario. Additionally, some parts of the post like #hashtags and @mentions do not show up, which are also really important for my scenario. So I had to hunt for a better way to parse these posts.\\n\\nUltimately, we just need to be more specific about where we grab the text from. Here is an upgraded XPATH query which selects only the main tweet text:\\n\\n```\\nhtml(https://twitter.com/<username>/status/<id>).xpath(//div[contains(@class, \'permalink-tweet-container\')]//p[contains(@class, \'tweet-text\')]//text())\\n```\\n\\nThe results are much better, but still a little strange... Take a look at this example:\\n\\n#### Twitter Post\\n\\n![](/assets/images/img_5b834588d4946.png)\\n\\n#### Oraclize Result\\n\\n```json\\n[\\n  \\"This time, the vacuum tunnel will be a bit longer & SpaceX will provide some advance funding for student teams with most promising designs. Bonus award for all race pods that exceed half the speed of sound!\\",\\n  \\"https://\\",\\n  \\"twitter.com/hyperloop/stat\\",\\n  \\"us/1032818998243520512\\",\\n  \\"\\\\u00a0\\",\\n  \\"\\\\u2026\\"\\n]\\n```\\n\\nNote that we only get content from the main post now (yay!), but we still get this strange array format where things like the linked post are broken up into multiple pieces. To demonstrate more of the weird behavior, look at this other example:\\n\\n#### Twitter Post\\n\\n![](/assets/images/img_5b83469aa1592.png)\\n\\n#### Oraclize Result\\n\\n```json\\n[\\n  \\"Amazing news! At \\",\\n  \\"#\\",\\n  \\"HyperloopUPV\\",\\n  \\" we will do our best to be on the top of the \\",\\n  \\"@\\",\\n  \\"SpaceX\\",\\n  \\" \\",\\n  \\"#\\",\\n  \\"Hyperloop\\",\\n  \\" competition this time! \\",\\n  \\"@\\",\\n  \\"boringcompany\\",\\n  \\" \\",\\n  \\"@\\",\\n  \\"hyperloop\\",\\n  \\"\\\\n\\",\\n  \\"#\\",\\n  \\"HyperloopSpain\\",\\n  \\" \\",\\n  \\"@\\",\\n  \\"UPV\\",\\n  \\" \\",\\n  \\"#\\",\\n  \\"SpaceX\\",\\n  \\" \\",\\n  \\"#\\",\\n  \\"ElonMusk\\",\\n  \\" \\",\\n  \\"#\\",\\n  \\"BreakaPod\\"\\n]\\n```\\n\\nIt is evident that things like #hashtags and @mentions, while present now, are broken up into different pieces. It is pretty easy to repair this on the front end by treating it as a JavaScript array, and then joining the parts:\\n\\n```javascript\\nJSON.parse(result).join(\\"\\");\\n\\n/*\\n\\"Amazing news! At #HyperloopUPV we will do our best to be on the top of the @SpaceX #Hyperloop competition this time! @boringcompany @hyperloop\\n#HyperloopSpain @UPV #SpaceX #ElonMusk #BreakaPod\\"\\n*/\\n```\\n\\nThis final result is perfect, and accurately represents what we want. Unfortunately, on the blockchain, the data is still in this array format, and there is no clean, low gas way to change it that I am aware of.\\n\\nFor my purposes, this can cause some validation issues if there are white-space differences between what the user posts and what the fulfiller used to create their bounty. To avoid this, my suggestion is to keep the posts relatively simple. This will also reduce the amount of gas required to store the post on the blockchain.\\n\\nThe final contract code for my \\"Twitter Oracle\\" can be found [here](https://github.com/shawntabrizi/Ethereum-Twitter-Bounty/blob/master/twitter-bounty/contracts/TwitterOracle.sol). Here is a relevant snippet:\\n\\n```\\n/// @notice This function initiates the oraclize process for a Twitter post\\n/// @dev This contract needs ether to be able to call the oracle, which is why this function is also payable\\n/// @param _postId The twitter post to fetch with the oracle. Expecting \\"<user>/status/<id>\\"\\nfunction oraclizeTweet(string _postId)\\npublic\\npayable\\nwhenNotPaused\\n{\\n    // Check if we have enough remaining funds\\n    if (oraclize_getPrice(\\"URL\\") > address(this).balance) {\\n        emit LogInfo(\\"Oraclize query was NOT sent, please add some ETH to cover for the query fee\\");\\n    } else {\\n        emit LogInfo(\\"Oraclize query was sent, standing by for the answer..\\");\\n        // Using XPath to to fetch the right element in the JSON response\\n        string memory query = string(abi.encodePacked(\\"html(https://twitter.com/\\", _postId, \\").xpath(//div[contains(@class, \'permalink-tweet-container\')]//p[contains(@class, \'tweet-text\')]//text())\\"));\\n\\n        bytes32 queryId = oraclize_query(\\"URL\\", query, 6721975);\\n        queryToPost[queryId] = _postId;\\n    }\\n}\\n```\\n\\nIn this code, you can see I have specified a custom gas limit which is extremely high. The default gas limit for the `oraclize_query` is 200,000 gas, which was causing \\"out of gas\\" errors when trying to store the data in the smart contract. I changed the value to be the gas limit for ganache-cli (6,721,975 gas), which is probably not very smart for production, but I did not really invest time into thinking about what a reasonable gas limit would be, and I just wanted to make sure not to run into errors when avoidable.\\n\\nTake note of another key implementation detail. I get a `queryId` as a result of `oraclize_query`. Then I store this value in a mapping, where the value is the Twitter URL (`postId`) that is being oraclized. This is important because the oraclization process is asynchronous, so when I get a result back from Oraclize.it, I need to know which post the text is for. I then check the mapping I created for the result with a matching `queryId`, fetch the `postId`, and then create a mapping using the `postId` as the key, and the resulting post text as the value. A little tricky, but totally works! :)\\n\\nAnyway, I hope you learned something from my little exploration into oraclizing Twitter posts for Ethereum dApp development. Try out my app locally by following the instructions on the [GitHub page](https://github.com/shawntabrizi/Ethereum-Twitter-Bounty). If you know of a better way to solve this problem, let me know! Otherwise, if you enjoyed this content, feel free to take a look at my [donations page](https://shawntabrizi.com/donate/)."},{"id":"/aad/building-a-simple-sign-in-page-with-msal-js-for-microsoft-identities/","metadata":{"permalink":"/blog/aad/building-a-simple-sign-in-page-with-msal-js-for-microsoft-identities/","source":"@site/blog/2018-07-31-building-a-simple-sign-in-page-with-msal-js-for-microsoft-identities.md","title":"Building a Simple Sign-In Page with MSAL.js for Microsoft Identities","description":"In this post I will discuss how I used MSAL.js to build a simple sign-in experience for Microsoft Identities, and some of the things I learned along the way.","date":"2018-08-01T06:25:43.000Z","tags":[{"inline":true,"label":"authentication","permalink":"/blog/tags/authentication"},{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"msal","permalink":"/blog/tags/msal"}],"readingTime":3.91,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Building a Simple Sign-In Page with MSAL.js for Microsoft Identities","date":"2018-08-01T06:25:43.000Z","authors":"shawntabrizi","slug":"/aad/building-a-simple-sign-in-page-with-msal-js-for-microsoft-identities/","categories":["AAD"],"tags":["authentication","azure active directory","javascript","msal"],"github":"Microsoft-Authentication-with-MSAL.js"},"unlisted":false,"prevItem":{"title":"Getting Twitter posts for Ethereum using an Oracle","permalink":"/blog/ethereum/getting-twitter-posts-for-ethereum-using-an-oracle/"},"nextItem":{"title":"Microsoft Identities on the Ethereum Blockchain","permalink":"/blog/aad/microsoft-identities-on-the-ethereum-blockchain/"}},"content":"##### In this post I will discuss how I used MSAL.js to build a simple sign-in experience for Microsoft Identities, and some of the things I learned along the way.\\n\\nI have worked with a lot of different people to onboard to Microsoft\'s Identity system. Whether it be app registration, app development, or even debugging the login experience, people quickly learn that authentication and authorization isn\'t always as straightforward as one might expect.\\n\\nIt is for that reason I think it is really important to build [minimal, complete, and verifiable examples](https://stackoverflow.com/help/mcve) of the authentication process. I previously had built a minimal authentication sample for [ADAL .NET using PowerShell](https://github.com/shawntabrizi/Azure-AD-Authentication-with-PowerShell-and-ADAL), and [this ended up having a number of different uses](https://shawntabrizi.com/aad/azure-ad-authentication-with-powershell-and-adal/).\\n\\nI want to do the same for MSAL.js!\\n\\n## My goals:\\n\\n- Create a login page\\n- Use only basic HTML + JavaScript\\n- Does not require a \'web server\', just simple web hosting\\n- Can obtain an Access Token for a custom resource, with custom scopes\\n- (Stretch Goal) Allow a user to use their own App ID for getting an access token\\n\\n## Starting with a Minimal example\\n\\nFortunately, MSAL.js has a set of really great [minimal examples](https://github.com/AzureAD/microsoft-authentication-library-for-js/tree/dev/lib/msal-core/samples/VanillaJSTestApp) which do not require a back-end web server, unlike its predecessor ADAL.js whose [samples](https://github.com/AzureAD/azure-activedirectory-library-for-js/wiki/Code-samples) ALL require a .NET backend. (WHY???)\\n\\nAlthough there is a `minimal.html` file, I would not start there, since I do not feel that it follows the best practices of using the library. Instead, start with the [`index.html`](https://github.com/AzureAD/microsoft-authentication-library-for-js/blob/dev/lib/msal-core/samples/VanillaJSTestApp/index.html) which is still very raw, but includes an `applicationConfig` object, functionalizes the login flow, and also has a basic UX.\\n\\nOne of the important things I came to realize was the importance of creating a global variable for the `MSAL.UserAgentApplication` object. It seems to do a lot of different things like listen for the callback from the popup window that gets created, and captures the returned ID and access token to be ingested by the `acquireToken` functions. I had mistakenly wrapped this variable into my login function, and the results were... weird.\\n\\nWhen using the popup experience, the redirection happened WITHIN the popup itself rather than the main page where the sign-in experience was initiated.\\n\\n![](/assets/images/img_5b614c4d2d483.png)\\n\\nThis was a really awful experience, which then made me go down the path of switching `loginPopup` to `loginRedirect` so that the user would stay on the same page the whole time. However, this then caused issues where the ID token would just end up in the URL as a fragment (`#`) and the callback function which turned an ID token into an access token would not work anymore!\\n\\nWhen I first was building this page, I actually ignored these problems, since I was working on a Hackathon, and I just needed to keep building. However, I can say that all of this really was caused simply by NOT having the `UserAgentApplication` object in the global context.\\n\\nSO DON\'T DO THAT!\\n\\n## A working sample\\n\\nYou can find the end result of my simple sign-in page here:\\n\\n[https://shawntabrizi.com/Microsoft-Authentication-with-MSAL.js/](https://shawntabrizi.com/Microsoft-Authentication-with-MSAL.js/)\\n\\nSource code is on [GitHub](https://github.com/shawntabrizi/Microsoft-Authentication-with-MSAL.js).\\n\\n![](/assets/images/img_5b614ec708ba1.png)\\n\\nThe page currently will sign you in, and get an access token to the Microsoft Graph with the scope `user.read`. Additionally, it will show you your ID token and access token as both a raw JWT and in its decoded JSON format, which I teach how to do [here](https://shawntabrizi.com/aad/decoding-jwt-tokens/).\\n\\nBesides showing off MSAL.js in a really simple website, I think this app will be useful when trying to use other tools like Postman where you will need to have a valid access token, and generating one may not be so straight forward to the end user.\\n\\n## Next steps\\n\\nWhile this sample in its current form is pretty cool, it does not satisfy all the goals I listed above. That will be the next step for me on this project. I still need to figure out exactly how to call a custom API using a V2 application (which may not even be possible to configure right now until a new UX which a co-worker of mine is working on becomes available). However, allowing a user to input custom scopes should be really easy, and if others configure their own App ID to have my website as a redirect url, then I could see using a custom App ID to be really easy too.\\n\\nI will be making this updates in the same [GitHub](https://github.com/shawntabrizi/Microsoft-Authentication-with-MSAL.js) I listed above, and you should see the changes directly on the [same page as before](https://shawntabrizi.com/Microsoft-Authentication-with-MSAL.js/)! I hope this was useful for some of you, and if there are additional things I could add to make this project work better for you, just let me know through a GitHub issue."},{"id":"/aad/microsoft-identities-on-the-ethereum-blockchain/","metadata":{"permalink":"/blog/aad/microsoft-identities-on-the-ethereum-blockchain/","source":"@site/blog/2018-06-30-microsoft-identities-on-the-ethereum-blockchain.md","title":"Microsoft Identities on the Ethereum Blockchain","description":"My favorite time of the year at Microsoft is the \\"OneWeek\\" Hackathon.","date":"2018-07-01T06:08:30.000Z","tags":[{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"identity","permalink":"/blog/tags/identity"},{"inline":true,"label":"microsoft","permalink":"/blog/tags/microsoft"}],"readingTime":6.545,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Microsoft Identities on the Ethereum Blockchain","date":"2018-07-01T06:08:30.000Z","authors":"shawntabrizi","slug":"/aad/microsoft-identities-on-the-ethereum-blockchain/","categories":["AAD","Ethereum"],"tags":["azure active directory","blockchain","ethereum","identity","microsoft"],"github":"Microsoft-Identities-on-the-Ethereum-Blockchain"},"unlisted":false,"prevItem":{"title":"Building a Simple Sign-In Page with MSAL.js for Microsoft Identities","permalink":"/blog/aad/building-a-simple-sign-in-page-with-msal-js-for-microsoft-identities/"},"nextItem":{"title":"Using Web3.js 1.0 Subscribe and Infura WebSockets to Visualize Ethereum Transactions","permalink":"/blog/ethereum/using-web3-js-1-0-subscribe-and-infura-websockets-to-visualize-ethereum-transactions/"}},"content":"My favorite time of the year at Microsoft is the \\"OneWeek\\" Hackathon.\\n\\n![Hackathon Image](/assets/images/img_5b386bd8dab8a.png)\\n\\nIt\'s a time of the year where you can work with some of the most talented engineers around to solve problems that you all agree are awesome.\\n\\nI have lead a [different](https://shawntabrizi.com/metime/) [project](https://shawntabrizi.com/Skill-Finder/) every year, some of which have been hinted to in my [previous blog posts.](https://shawntabrizi.com/code/scraping-linkedin-topics-skills-data/)\\n\\nThis year will be no different.\\n\\n## Hypothesis: Microsoft\'s identity system has value which can be transferred to the Ethereum blockchain\\n\\nOne of the amazing things about identities on Ethereum is that they are free, anonymous, and simple to create. However this also makes them pretty useless when trying to gate access to decentralized applications. There are solutions to combat this problem if you to build a reputation or token staking system around your application, but this usually involves a new identity to undergo significant onboarding before their identity becomes valuable. What if instead of completely recreating new identities, we could bootstrap them using our existing, modern identities?\\n\\nThere is value in Microsoft Identities. I don\'t mean specifically \\"Microsoft.com accounts\\", I mean the identity system that is built by Microsoft, and is used by [85% of the Fortune 500](https://shawntabrizi.com/aad/does-company-x-have-an-azure-active-directory-tenant/). If a user has an account in the Adobe company tenant, then we can pretty confidently say the following:\\n\\n- That user is an employee of Adobe\\n- That user\'s identity is managed by a Company Administrator at Adobe\\n- That user\'s information first name, last name, phone number, etc. is relatively accurate\\n- and more...\\n\\nThis is actually the very information which powers Microsoft cloud services like Azure, Office 365, and the larger Microsoft Graph ecosystem. It is the same information that millions of apps use to build authentication and single-sign. There is trust because Microsoft has spent the time and energy to build a system with rigorous role based access control (RBAC), and companies have spent their time and energy populating these systems with data which powers their companies.\\n\\nMy hypothesis is that there is value in these Microsoft identities, simply for existing. Not all of them are equally valuable, but for large companies who methodically manage and control that information, the value is actually quite high. The question is then: \\"How can we migrate this value onto Ethereum?\\"\\n\\n## Building an attestation service\\n\\nDecentralized identity systems being developed on Ethereum are attempting to solve a number of critically important problems, however they are not attempting to replace all forms of centralization. There will always be some amount of centralization involved with adding claims or attestations to an identity. For example, if you want to prove that you are a US citizen, you will need to go to your state\'s DMV to get an ID card.\\n\\nIn this case, we are interested in employment, and if you want to prove that your are an employee of a certain company, you will need to prove that you are registered within their internal systems. Imagine a middle tier service that accepts a valid authentication token, and associates that with an Ethereum address that the user provides. Then, if you can then make these proofs easily accessible, you then unlock a number of possibilities for dApps to take advantage of that information. Imagine the following user stories:\\n\\n- An Ethereum address may want to prove who they are (Name, Company, Role, etc\u2026). They could do this by logging into their corporate Azure Active Directory, and store those claims on the blockchain tied to their account.\\n- Imagine Microsoft (or another company) wants to build a \u201cKudos\u201d currency which can be used to thank other Microsoft Employees when they do nice things for one another. However, it should only be used between Microsoft employees. Using this identity claim, a smart contract which runs this cryptocurrency could verify that only Microsoft employees are the senders and recipients of the currency.\\n- Imagine Microsoft (or another company) wants to build a decentralized market similar to SellBuy (a Microsoft internal alias for selling items) using the Ethereum blockchain. Similar to the other stories, this decentralized market should only allow people from Microsoft, or other trusted companies to join and list their products on the marketplace. So, the dApp uses these Microsoft Identity claims to gate access to the marketplace.\\n- Imagine a decentralized application like [Blind](https://www.teamblind.com/), where users from different companies can anonymously chat within their own personal and shared forums. On Ethereum, an app like this can be completely censorship free and immutable, a problem that is present with the current Blind app. To ensure that these anonymous users have the right company identifier, we would use the claims about their accounts stored on the blockchain.\\n\\n## The plan\\n\\n![High level architecture](/assets/images/img_5b527aea81ca1.png)\\n\\nThe plan for the hackathon is actually relatively simple, mostly because we will be executing a proof of concept versus a battle tested, production ready design. Let\'s dive deeper into the pieces of the puzzle:\\n\\n### Web Front End\\n\\nThe sign up process:\\n\\n- The user goes to our web front end, and iniates a Microsoft identity login flow\\n- This results in an app + user access token for our middle tier resource, signed by Microsoft\'s identity provider\\n- The user then signs this access token with their Ethereum private key\\n- The web front end then sends a payload to the middle tier service with the access token and the signature for that payload\\n\\n### Middle Tier Service\\n\\nRegistering the user:\\n\\n- The middle tier service will recieve a \\"doubly signed\\" message, where the entire JWT is signed by the Ethereum address, and the contents of the JWT are signed by Microsoft\\n- It will verify that the token\'s resource is for the middle tier\\n- It will verify that the token was signed by Microsoft\\n- It will verify that the payload has been signed by the Ethereum account\\n- It will parse out specific claims about the user that it wants to store on the blockchain (MVP is Tenant ID)\\n- It will generate a transaction to our back end smart contract to store this data on Ethereum\\n\\n### Back End Identity Contract\\n\\nStoring the identity information:\\n\\n- The back end identity contract will be owned by the middle tier service\\n- It will define an extensible user object\\n- It will have a mapping between Ethereum accounts and that user object\\n- It will expose a getter function for other contracts to access that data\\n\\n### CorpCoin Contract\\n\\nTesting our identity system:\\n\\n- The CorpCoin contract will be a basic ERC20 token\\n- It will have a requirement that users have to be from a certain company/tenant to interact with the coin\\n- It will interact with our back end identity contract to verify the user\'s company/tenant\\n- It will issue coins to users who have not been issued coins in the past to bootstrap the \'economy\'\\n\\n## More to come!\\n\\nThat is all I am ready to share for now, but note that this is just a very high level overview. There are so many intricacies and details to think about when solving this problem. Here are just a few:\\n\\n- How can we automatically remove claims of a user once their off-chain identity has been deleted or removed from a particular group/claim?\\n- How can we store identity claims on the blockchain such that it does not expose sensitive information about the user to people who they do not want to share it with?\\n- What default claims should we be storing, and how can we allow users to extend those claims for their own needs?\\n- How could we decentralize the middle tier service which verifies the user\u2019s Microsoft Identity, to increase trust that the claims stored on the blockchain are real?\\n\\n## Watch this video to learn more!\\n\\n<iframe src=\\"https://www.youtube.com/embed/Wg6kg2mLA3k\\" width=\\"720px\\" height=\\"480px\\"></iframe>"},{"id":"/ethereum/using-web3-js-1-0-subscribe-and-infura-websockets-to-visualize-ethereum-transactions/","metadata":{"permalink":"/blog/ethereum/using-web3-js-1-0-subscribe-and-infura-websockets-to-visualize-ethereum-transactions/","source":"@site/blog/2018-05-24-using-web3-js-1-0-subscribe-and-infura-websockets-to-visualize-ethereum-transactions.md","title":"Using Web3.js 1.0 Subscribe and Infura WebSockets to Visualize Ethereum Transactions","description":"In this tutorial, you will learn how to subscribe to an Ethereum WebSocket using Web3.js to dynamically pull pending transactions on the blockchain.","date":"2018-05-25T06:42:19.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"graph","permalink":"/blog/tags/graph"},{"inline":true,"label":"html","permalink":"/blog/tags/html"},{"inline":true,"label":"infura","permalink":"/blog/tags/infura"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"},{"inline":true,"label":"websockets","permalink":"/blog/tags/websockets"}],"readingTime":4.065,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Using Web3.js 1.0 Subscribe and Infura WebSockets to Visualize Ethereum Transactions","date":"2018-05-25T06:42:19.000Z","authors":"shawntabrizi","slug":"/ethereum/using-web3-js-1-0-subscribe-and-infura-websockets-to-visualize-ethereum-transactions/","categories":["Ethereum"],"tags":["blockchain","ethereum","graph","html","infura","javascript","web3","websockets"],"github":"ETH-Transaction-Graph"},"unlisted":false,"prevItem":{"title":"Microsoft Identities on the Ethereum Blockchain","permalink":"/blog/aad/microsoft-identities-on-the-ethereum-blockchain/"},"nextItem":{"title":"Shawn Notes: CryptoZombies Lessons 1 - 5 in Solidity","permalink":"/blog/ethereum/shawn-notes-cryptozombies-lessons-1-5-in-solidity/"}},"content":"##### In this tutorial, you will learn how to subscribe to an Ethereum WebSocket using Web3.js to dynamically pull pending transactions on the blockchain.\\n\\nRecently, I have been pretty lurking on the [Ethereum StackExchange](https://ethereum.stackexchange.com/), and I have been seeing a pretty common question repeat itself.\\n\\n> How can I detect when a specific transaction occurs on Ethereum?\\n\\nIt might be that the user wants to see every time their account gets a payment. Or maybe they want track when another contract sends Ether.\\n\\nBy trying to answer this question I learned in [Web3.js 1.0](https://web3js.readthedocs.io/en/1.0/index.html) is introducing support for WebSockets, which will allow client applications to have an open and interactive session with the Ethereum blockchain. With WebSockets, you can automatically receive event-driven responses from Ethereum without having to repeatedly poll the blockchain. (_As of this writing, Web3.js 1.0 is still in beta._)\\n\\nAs of right now, you can [subscribe to 4 kinds of events](https://web3js.readthedocs.io/en/1.0/web3-eth-subscribe.html): `pendingTransactions`, `newBlockHeaders`, `syncing`, and `logs`.\\n\\nLogs are particularly interesting because you can add additional `options` like an Ethereum address to only get back the logs for that account, which will allow you to really easily monitor when different events happen, such as a payment.\\n\\n```javascript\\nvar subscription = web3.eth\\n  .subscribe(\\n    \\"logs\\",\\n    {\\n      address: \\"0x123456..\\",\\n      topics: [\\"0x12345...\\"],\\n    },\\n    function (error, result) {\\n      if (!error) console.log(result);\\n    }\\n  )\\n  .on(\\"data\\", function (log) {\\n    console.log(log);\\n  })\\n  .on(\\"changed\\", function (log) {});\\n\\n// unsubscribes the subscription\\nsubscription.unsubscribe(function (error, success) {\\n  if (success) console.log(\\"Successfully unsubscribed!\\");\\n});\\n```\\n\\n## ETH Transaction Graph\\n\\nI wanted try out this feature, so I decided I would build a little visual which would show transactions on the Ethereum blockchain. I thought it would be cool to let transactions come in, and see if we can start to find addresses which have a ton of activity. A natural visual for transactions is to [draw a graph](https://en.wikipedia.org/wiki/Graph_drawing), and if we do things correctly, we should see these central nodes appear; where tons of different addresses are interacting with a single Ethereum contract or other type of account.\\n\\nYou can check out the web app I built here: [https://shawntabrizi.com/ETH-Transaction-Graph/](https://shawntabrizi.com/ETH-Transaction-Graph/).\\nLetting it run for a little bit will generate a neat [force-directed graph drawing](https://en.wikipedia.org/wiki/Force-directed_graph_drawing) like the one below:\\n\\n![Graph Drawing Result](/assets/images/img_5b07a77621420.png)\\n\\nYou can find the source code to this app on [my GitHub](https://github.com/shawntabrizi/ETH-Transaction-Graph).\\n\\n### How does it work?\\n\\nBefore you can do anything with Web3.js, you will need to gain access to an Ethereum WebSocket. This technology is still pretty new, so there aren\'t many options. But you can always rely on [Infura](https://infura.io/) to be ahead of the game. Although as of this writing, [WebSockets on Infura are not quite production ready](https://github.com/INFURA/infura/issues/97), they should be in the near future. Connecting to the WebSocket is easy:\\n\\n```javascript\\nvar web3 = new Web3(\\"wss://mainnet.infura.io/_ws\\");\\n```\\n\\n_Note that it is important you do not include the normal check for [MetaMask](https://metamask.io/), since the browser plugin does not yet support WebSockets._\\n\\nNow that you are connected to your WebSocket, you just need to set up a subscription using Web3.js:\\n\\n```javascript\\nsubscription = web3.eth\\n  .subscribe(\\"pendingTransactions\\", function (error, result) {})\\n  .on(\\"data\\", function (transactionHash) {\\n    web3.eth.getTransaction(transactionHash).then(function (transaction) {\\n      createNode(transaction.from, transaction.to);\\n    });\\n  });\\n```\\n\\nUsing the code above, I will get a stream of transactions hashes coming to my app. Whenever I get a transaction hash, I use `web3.eth.getTransaction` to then get data about the transaction. Finally, from that data, I get the `to` address and the `from` address. These go into my `createNode` function which adds the transaction to the graph drawing. I won\'t be going into much details about [D3.js](https://d3js.org/), which generates the graph, because it is black magic to me. (Lots of copy and pasting code to get this sample to work...)\\n\\nIf you are setting up a subscription, don\'t forget to also set up a way to unsubscribe! This is particularly important for my little sample because the browser starts to get REALLY slow once you have a few thousand transactions drawn.\\n\\n```javascript\\nsubscription.unsubscribe(function (error, success) {\\n  if (success) console.log(\\"Successfully unsubscribed!\\");\\n});\\n```\\n\\nThat\'s really all you need to know to set up a sample similar to this! I wanted to keep this example as dead simple as possible, but you could imagine a million ways to modify and improve the design here. For example, imagine having each node grow and shrink as Ether is sent to and from the Ethereum Address! If you want to fork my code, I would love to see what kinds of projects you can think of using my sample as a baseline.\\n\\nAs always, I am excited to share with you fun, simple projects in the Ethereum development space, and I would love to hear any and all feedback you have. Know a cool project that you think someone should build and write about? Let me know!\\n\\nIf this post helped you, and you want to say \\"Thanks!\\", take a look at my [donations page](https://shawntabrizi.com/donate/)."},{"id":"/ethereum/shawn-notes-cryptozombies-lessons-1-5-in-solidity/","metadata":{"permalink":"/blog/ethereum/shawn-notes-cryptozombies-lessons-1-5-in-solidity/","source":"@site/blog/2018-04-23-shawn-notes-cryptozombies-lessons-1-5-in-solidity.md","title":"Shawn Notes: CryptoZombies Lessons 1 - 5 in Solidity","description":"In this post I will be summarizing some of the key takeaways from lessons 1 - 5 of the popular CryptoZombies tutorial. This should cover all of the Solidity aspects of building a CryptoKitties clone.","date":"2018-04-23T09:02:26.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"cryptozombies","permalink":"/blog/tags/cryptozombies"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"shawn notes","permalink":"/blog/tags/shawn-notes"},{"inline":true,"label":"solidity","permalink":"/blog/tags/solidity"}],"readingTime":9.705,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Shawn Notes: CryptoZombies Lessons 1 - 5 in Solidity","date":"2018-04-23T09:02:26.000Z","authors":"shawntabrizi","slug":"/ethereum/shawn-notes-cryptozombies-lessons-1-5-in-solidity/","categories":["Ethereum"],"tags":["blockchain","cryptozombies","ethereum","programming","shawn notes","solidity"]},"unlisted":false,"prevItem":{"title":"Using Web3.js 1.0 Subscribe and Infura WebSockets to Visualize Ethereum Transactions","permalink":"/blog/ethereum/using-web3-js-1-0-subscribe-and-infura-websockets-to-visualize-ethereum-transactions/"},"nextItem":{"title":"Graphing ETH Balance History of an Ethereum Address using Parallel Asynchronous Requests in Web3.js","permalink":"/blog/ethereum/graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js/"}},"content":"##### In this post I will be summarizing some of the key takeaways from lessons 1 - 5 of the popular [CryptoZombies tutorial](https://cryptozombies.io). This should cover all of the Solidity aspects of building a CryptoKitties clone.\\n\\nThis post should not act as a replacement for taking the course, which I strongly recommend you do. Instead this will be a good way to keep some of the new concepts and best practices fresh in your mind if you are jumping into a new Solidity project. This post will be written assuming that the reader is relatively versed in other programming languages so things like the modular function, if/else statements, and for loops wont be included.\\n\\nReally I want to use this post as a way for me to look back and refresh my memory the next time I jump on a Solidity project.\\n\\nSo let\'s get started!\\n\\n![](/assets/images/img_5add32b676792.png)\\n\\n## Lesson 1 Summary\\n\\n- Use `pragma solidity ^` to define the version of Solidity you want to use\\n- Create a contract using `contract contractName {}`\\n- `uint` is an alias for `uint256`. You can also individually specify `uint8`, `uint16`, etc...\\n- Exponential operator in Solidity is `**`, i.e. `uint x = 5 **2; // equal to 5^2 = 25`\\n- Structs are available in Solidity and can be defined like:\\n\\n  ```solidity\\n  struct Person {\\n  \\tuint age;\\n  \\tstring name;\\n  }\\n  ```\\n\\n- Fixed (`uint[2] fixedArray`) and dynamic arrays (`uint[] dynamicArray`) are both available in Solidity.\\n- You can also create an array of structs.\\n- If you declare an array as `public`, Solidity will automatically create a getter method for it.\\n- You can define a function within a contract like so:\\n\\n  ```solidity\\n  function eatHamburgers(string _name, uint _amount) {...}\\n  ```\\n\\n- It is convention to start function parameter variable names with an underscore (`_`), differentiating them from global variables.\\n- Functions are `public` by default, which means anyone can call them. Usually you want to mark your functions `private` by default, and only make functions public if you want to expose them to others. We also start private functions with an underscore (`_`) by convention.\\n- You need to declare the type of value a function will return if it returns a value like so:\\n\\n  ```solidity\\n  function sayHello() public returns (string) {\\n  \\treturn greeting;\\n  }\\n  ```\\n\\n- If a function does not change any values or write anything, we can additionally label the function as a `view` function.\\n- If the function doesn\'t even read from the state of the app, and only uses the function parameters we can label it `pure`.\\n- Both `view` and `pure` functions can help users save gas when using your contract. More about this in lesson 3.\\n- Typecasting is available in Solidity by wrapping a variable with the type, i.e. `uint8(var)`.\\n- You can use events to communicate that something has happened on the blockchain. This is particularly useful when creating a front-end for your Solidity application.\\n\\n  ```solidity\\n  // declare the event\\n  event IntegersAdded(uint x, uint y, uint result);\\n\\n  function add(uint _x, uint _y) public {\\n  \\tuint result = _x + _y;\\n  \\t// fire an event to let the app know the function was called:\\n  \\tIntegersAdded(_x, _y, result);\\n  \\treturn result;\\n  }\\n  ```\\n\\n## Lesson 2 Summary\\n\\n- In Solidity, there is a variable type `address`, that is a unique id to represent an Ethereum account.\\n- Solidity has the concept of a `mapping`, which is like a hash table, [but a little different](https://solidity.readthedocs.io/en/develop/types.html#mappings).\\n\\n  ```solidity\\n  // For a financial app, storing a uint that holds the user\'s account balance:\\n  mapping (address => uint) public accountBalance;\\n  ```\\n\\n- You can use `msg.sender` to refer to the address of the person or smart contract that called a particular function.\\n- We can make functions throw an error if a condition is not true using `require();`.\\n- You can use the inheritance phrase `is` to split up your contracts into a more object-oriented design.\\n\\n  ```solidity\\n  contract Doge {\\n  \\tfunction catchphrase() public returns (string) {\\n  \\treturn \\"So Wow CryptoDoge\\";\\n  \\t}\\n  }\\n\\n  contract BabyDoge is Doge {\\n  \\tfunction anotherCatchphrase() public returns (string) {\\n  \\treturn \\"Such Moon BabyDoge\\";\\n  \\t}\\n  }\\n  ```\\n\\n- You can also split up your code into multiple files using the `import` keyword, referencing the other Solidity files.\\n- There are two places you can store variables: `storage` and `memory`. `storage` being permanently written information in the blockchain, and `memory` being temporary data which disappears at the end of a function call.\\n- In addition to `public` and `private`, functions can also be `internal` or `external`. `internal` is the same as `private` except that it is also accessible to contracts which inherit from the one that contains this function. `external` is similar to `public` except that they can only be called outside the contract, not by functions inside the contract.\\n- You can define an `interface` to allow one contract to talk to another that we do not own.\\n\\n  ```solidity\\n  interface NumberInterface {\\n  \\tfunction getNum(address _myAddress) public view returns (uint);\\n  }\\n\\n  contract MyContract {\\n  \\taddress NumberInterfaceAddress = 0xab38...\\n  \\t// ^ The address of the FavoriteNumber contract on Ethereum\\n  \\tNumberInterface numberContract = NumberInterface(NumberInterfaceAddress);\\n  \\t// Now `numberContract` is pointing to the other contract\\n\\n  \\tfunction someFunction() public {\\n  \\t\\t// Now we can call `getNum` from that contract:\\n  \\t\\tuint num = numberContract.getNum(msg.sender);\\n  \\t\\t// ...and do something with `num` here\\n  \\t}\\n  }\\n  ```\\n\\n- You can have functions return multiple variables and do multiple assignments using this format:\\n\\n  ```solidity\\n  function multipleReturns() internal returns(uint a, uint b, uint c) {\\n  \\treturn (1, 2, 3);\\n  }\\n\\n  function processMultipleReturns() external {\\n  \\tuint a;\\n  \\tuint b;\\n  \\tuint c;\\n  \\t// This is how you do multiple assignment:\\n  \\t(a, b, c) = multipleReturns();\\n  }\\n\\n  // Or if we only cared about one of the values:\\n  function getLastReturnValue() external {\\n  \\tuint c;\\n  \\t// We can just leave the other fields blank:\\n  \\t(,,c) = multipleReturns();\\n  }\\n  ```\\n\\n## Lesson 3 Summary\\n\\n- Ethereum contracts are immutable, so it may make sense to break apart your project into multiple contracts, where individual parts could get updated later by referencing a new contract with updated code.\\n- It is common practice to add ownership to contracts. One such library that adds this functionality out of the box is [OpenZeppelin\'s Ownable.sol](https://github.com/OpenZeppelin/zeppelin-solidity/blob/master/contracts/ownership/Ownable.sol).\\n- Constructors are a special kind of function that has the same name as the contract and will only execute one time: when the contract is first created.\\n- A `modifier` is logic that can be applied on top of a function, usually to check requirements before the function actually executes.\\n- Function `modifier`s can also accept arguments.\\n\\n  ```solidity\\n  // A mapping to store a user\'s age:\\n  mapping (uint => uint) public age;\\n\\n  // Modifier that requires this user to be older than a certain age:\\n  modifier olderThan(uint _age, uint _userId) {\\n  \\trequire(age[_userId] >= _age);\\n  \\t_;\\n  }\\n\\n  // Must be older than 16 to drive a car (in the US, at least).\\n  // We can call the `olderThan` modifier with arguments like so:\\n  function driveCar(uint _userId) public olderThan(16, _userId) {\\n  \\t// Some function logic\\n  }\\n  ```\\n\\n- Transactions on Ethereum are not free, and running functions or storing values on the blockchain require `gas`.\\n- Normally, Solidity will reserve 256 bits of storage no matter what variable sub-type you use, so you will not save on gas.\\n- The exception to this rule is in a `struct`, where Solidity will package multiple smaller variables together.\\n- Solidity uses unix time, and has built into the language time units like `seconds`, `minutes`, `hours`, `days`, `weeks`, `years`.\\n- You can also call `now`, which will return the current unix time for that block.\\n- Earlier we mentioned that we can label functions with `view` if they are read only functions. Solidity also makes `view` functions free to call, therefore, wherever possible, make sure to make your functions `external view`. The same applies to `pure` functions.\\n- When you write to `storage`, it takes a ton of gas. The tutorial states:\\n\\n  > In order to keep costs down, you want to avoid writing data to storage except when absolutely necessary. Sometimes this involves seemingly inefficient programming logic \u2014 like rebuilding an array in `memory` every time a function is called instead of simply saving that array in a variable for quick lookups.\\n\\n- With the current version of Solidity, `memory` arrays must be fixed length.\\n- CryptoZombies then talks about the correct way to implement mapping zombies to owners, it is a little long so I won\'t summarize it here.\\n\\n## Lesson 4 Summary\\n\\n- You can mark a function as `payable` which will allow the function to accept Ether payments. Otherwise, the function will reject all transactions containing Ether.\\n- The amount of ether sent to the contract can be viewed using `msg.value`.\\n\\n  ```solidity\\n  contract OnlineStore {\\n  \\tfunction buySomething() external payable {\\n  \\t\\t// Check to make sure 0.001 ether was sent to the function call:\\n  \\t\\trequire(msg.value == 0.001 ether);\\n  \\t\\t// If so, some logic to transfer the digital item to the caller of the function:\\n  \\t\\ttransferThing(msg.sender);\\n  \\t}\\n  }\\n  ```\\n\\n- You can use `this.balance` to get the amount of ether available in the contract.\\n- You can use the `transfer` function to send funds to any Ethereum address.\\n\\n  ```solidity\\n  contract GetPaid is Ownable {\\n  \\tfunction withdraw() external onlyOwner {\\n  \\t\\towner.transfer(this.balance);\\n  \\t}\\n  }\\n  ```\\n\\n- You can use `keccak256` for random number generation, but it is exploitable, so only use it in non-critical scenarios.\\n- The rest of this lesson is about implementing specific game functionality in CryptoZombies and refactoring old code, which I wont go into here.\\n\\n## Lesson 5 Summary\\n\\n- [ERC20](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-20.md) tokens are great for representing alternative currencies within Ethereum.\\n- [ERC721](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-721.md) tokens are good for representing collectables and other non-fungible assets.\\n- Solidity contracts can inherit from multiple contracts like so:\\n\\n  ```solidity\\n  contract ZombieOwnership is ZombieAttack, ERC721 {...}\\n  ```\\n\\n- You need to define the functions which make the token ERC721 compatible, and the logic may change depending on the specific implementation and functionality of your token. I will not go into these details.\\n- You cannot have function modifiers with the same name.\\n- You should use another [OpenZeppelin library called SafeMath](https://github.com/OpenZeppelin/zeppelin-solidity/blob/master/contracts/math/SafeMath.sol) to prevent overflows and underflows which may lead to security vulnerabilities.\\n- SafeMath uses the `assert` statement to make sure that the result of math functions are as we would suspect.\\n\\n  ```solidity\\n  function add(uint256 a, uint256 b) internal pure returns (uint256) {\\n  \\tuint256 c = a + b;\\n  \\tassert(c >= a);\\n  \\treturn c;\\n  }\\n  ```\\n\\n- `assert` is similar to `require` in that it will throw an error if false, however `assert` will not refund unused gas when a function fails.\\n- You will need to define the SafeMath library for each different type of `uint` you use in your contract. As is, SafeMath will only work on `uint256`.\\n- You can add comments to Solidity code using `//` or multi-line comments with `/* ... */`.\\n- The standard way to format your comments among the Solidity community is the [Ethereum Natural Specification Format](https://github.com/ethereum/wiki/wiki/Ethereum-Natural-Specification-Format).\\n\\n# That\'s all folks!\\n\\nAgain, I strongly recommend that you follow the [CryptoZombies tutorial](https://cryptozombies.io). It is funny, comprehensive, and very informative. I talked about a lot of the Solidity specific knowledge that CryptoZombies teaches, but I leave out the most critical learning aspect: Designing and implementing a game using Solidity. This you can only learn by completing the full tutorial.\\n\\n- Did this post help you?\\n- Did I miss a critical piece of knowledge?\\n- Did I make a mistake?\\n\\nLet me know, and as always, if you like what I do, you can send me a friendly thanks in the [form of a donation](https://shawntabrizi.com/donate/) :)"},{"id":"/ethereum/graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js/","metadata":{"permalink":"/blog/ethereum/graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js/","source":"@site/blog/2018-03-11-graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js.md","title":"Graphing ETH Balance History of an Ethereum Address using Parallel Asynchronous Requests in Web3.js","description":"This tutorial will show you how you can query the ETH balance of an Ethereum address across multiple Ethereum blocks, and visualize the results as a graph.","date":"2018-03-12T06:31:35.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"}],"readingTime":5.92,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Graphing ETH Balance History of an Ethereum Address using Parallel Asynchronous Requests in Web3.js","date":"2018-03-12T06:31:35.000Z","authors":"shawntabrizi","slug":"/ethereum/graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js/","categories":["Ethereum"],"tags":["ethereum","javascript","web3"],"github":"ETH-Balance-Graph"},"unlisted":false,"prevItem":{"title":"Shawn Notes: CryptoZombies Lessons 1 - 5 in Solidity","permalink":"/blog/ethereum/shawn-notes-cryptozombies-lessons-1-5-in-solidity/"},"nextItem":{"title":"Set up Azure Service Health Alerts programmatically using PowerShell","permalink":"/blog/code/set-azure-service-health-alerts-programmatically-using-powershell/"}},"content":"##### This tutorial will show you how you can query the ETH balance of an Ethereum address across multiple Ethereum blocks, and visualize the results as a graph.\\n\\nContinuing my series of simple to consume [Web3.js tutorials](https://shawntabrizi.com/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/), I want to now look into how we can start to visualize data from the Ethereum blockchain. In the past month or so, a bunch of ponzi scheme Ethereum contracts have been popping. The first notable one was POWHcoin which had the following brilliant whitepaper:\\n\\n![](/assets/images/img_5acdcc67280aa.png)\\n\\nThis made me wonder what the actual graph of this Ethereum smart contract looked like...\\n\\nUnfortunately, this smart contract eventually got hacked, so we won\'t quite be able to see the peaks and valleys like the picture above shows, but we will be able to see the contract get completely liquidated, which may be interesting itself.\\n\\nLet\'s build it!\\n\\n## Getting the ether balance at a certain block\\n\\nThe main thing we need to do here is get the ETH balance of an address at a specific block. This is actually quite easy by extending the call we make in my first tutorial:\\n\\n[**web3.eth.getBalance**](https://github.com/ethereum/wiki/wiki/JavaScript-API#web3ethgetbalance)\\n\\n```\\nweb3.eth.getBalance(addressHexString [, defaultBlock] [, callback])\\n```\\n\\nSo if we simply pass a block number into this function, we should be able to get the balance at that point in time.\\n\\n## Getting the first transaction for an Ethereum address\\n\\nBefore we create a loop getting the various balances, we need to know where to start looking. We could start at block 0, but we will get a ton of empty data before the address was ever used. Unfortunately, Web3.js does not have a good way to query the first transaction of an Ethereum address, so we will use Etherscan\'s API to get that information. Note that if you want to do this entirely with Web3, you can, you will just need to create a smart searching algorithm, and look at a ton of blocks... probably not very efficient.\\n\\n[**Etherescan Developer APIs:** Accounts](https://etherscan.io/apis#accounts)\\n\\n```\\nhttps://api.etherscan.io/api?module=account&action=txlist&address=0xA7CA36F7273D4d38fc2aEC5A454C497F86728a7A&startblock=0&page=1&offset=1\\n```\\n\\nThis query, which includes the POWHcoin address, will return the first transaction of that address, which is the contract creation. You can verify it is a contract creation call because the `to` value is empty, and the `contractAddress` is populated and matches the address we are observing.\\n\\nNow we know to start looking at block 499258.\\n\\n## Get the current block number\\n\\nReally the last piece of information we need is the current block number, so we know where to end our loop. This information is also available via Web3.js with the following call:\\n\\n[**web3.eth.blockNumber**](https://github.com/ethereum/wiki/wiki/JavaScript-API#web3ethblocknumber)\\n\\n```\\nweb3.eth.getBlockNumber(callback(error, result){ ... })\\n```\\n\\n## So let\'s make a simple loop:\\n\\nHere is a simple implementation of what we want to accomplish:\\n\\n```javascript\\n// Check for MetaMask, otherwise use an HTTP Provider\\nwindow.addEventListener(\\"load\\", function () {\\n  if (typeof web3 !== \\"undefined\\") {\\n    console.log(\\"Web3 Detected! \\" + web3.currentProvider.constructor.name);\\n    window.web3 = new Web3(web3.currentProvider);\\n  } else {\\n    console.log(\\"No Web3 Detected... using HTTP Provider\\");\\n    window.web3 = new Web3(\\n      new Web3.providers.HttpProvider(\\"https://mainnet.infura.io/<APIKEY>\\")\\n    );\\n  }\\n});\\n\\n// Wrapper for Web3 callback\\nconst promisify = (inner) =>\\n  new Promise((resolve, reject) =>\\n    inner((err, res) => {\\n      if (err) {\\n        reject(err);\\n      } else {\\n        resolve(res);\\n      }\\n    })\\n  );\\n\\n// Get the first transaction block for an address\\nasync function getFirstBlock(address) {\\n  let response = await fetch(\\n    \\"https://api.etherscan.io/api?module=account&action=txlist&address=\\" +\\n      address +\\n      \\"&startblock=0&page=1&offset=10&sort=asc\\"\\n  );\\n  let data = await response.json();\\n\\n  return data.result[0].blockNumber;\\n}\\n\\n// Given an address and a range of blocks, query the Ethereum blockchain for the ETH balance across the range\\nasync function getBalanceInRange(address, startBlock, endBlock) {\\n  // Number of points to fetch between block range\\n  var pointCount = 50;\\n\\n  // Calculate the step size given the range of blocks and the number of points we want\\n  var step = Math.floor((endBlock - startBlock) / pointCount);\\n  // Make sure step is at least 1\\n  if (step < 1) {\\n    step = 1;\\n  }\\n\\n  // Store the final result here\\n  var balances = [];\\n\\n  // Loop over the blocks, using the step value\\n  for (let i = startBlock; i < endBlock; i = i + step) {\\n    // Get the ETH value at that block\\n    let wei = await promisify((cb) => web3.eth.getBalance(address, i, cb));\\n    let ether = parseFloat(web3.fromWei(wei, \\"ether\\"));\\n    // Add result to final output\\n    balances.push({\\n      block: i,\\n      balance: ether,\\n    });\\n  }\\n\\n  return balances;\\n}\\n\\n// Main function\\nasync function graphBalance() {\\n  // Ethereum Address we want to look at\\n  var address = \\"0xA7CA36F7273D4d38fc2aEC5A454C497F86728a7A\\";\\n\\n  // Find the intial range, from first block to current block\\n  var startBlock = parseInt(await getFirstBlock(address));\\n  var endBlock = parseInt(await promisify((cb) => web3.eth.getBlockNumber(cb)));\\n\\n  var balances = await getBalanceInRange(address, startBlock, endBlock);\\n  console.log(balances);\\n}\\n\\ngraphBalance();\\n```\\n\\nAnd the output we get is:\\n\\n```\\n[{\\"block\\":4990258,\\"balance\\":0},{\\"block\\":4995223,\\"balance\\":690.3946401936848},{\\"block\\":5000188,\\"balance\\":812.2356284509416},{\\"block\\":5005153,\\"balance\\":1033.1702951440611},{\\"block\\":5010118,\\"balance\\":74.00248959231331},{\\"block\\":5015083,\\"balance\\":0.23714259262709267},{\\"block\\":5020048,\\"balance\\":0.000334978488623982},{\\"block\\":5025013,\\"balance\\":0.005036602641690202},{\\"block\\":5029978,\\"balance\\":0.005036602641690202},{\\"block\\":5034943,\\"balance\\":0.0526236993093083},{\\"block\\":5039908,\\"balance\\":0.04323124310148216},{\\"block\\":5044873,\\"balance\\":0.06245124310148216},{\\"block\\":5049838,\\"balance\\":0.06283124310148216},{\\"block\\":5054803,\\"balance\\":0.004397267827962452},{\\"block\\":5059768,\\"balance\\":0.004397267827962452},{\\"block\\":5064733,\\"balance\\":0.004397267827962452},{\\"block\\":5069698,\\"balance\\":0.03639726782796245},{\\"block\\":5074663,\\"balance\\":0.03639726782796245},{\\"block\\":5079628,\\"balance\\":0.03919726782796245},{\\"block\\":5084593,\\"balance\\":0.03919726782796245},{\\"block\\":5089558,\\"balance\\":0.04199726782796245},{\\"block\\":5094523,\\"balance\\":0.04205726782796245},{\\"block\\":5099488,\\"balance\\":0.04205726782796245},{\\"block\\":5104453,\\"balance\\":0.5869572678279624},{\\"block\\":5109418,\\"balance\\":0.000534060075850865},{\\"block\\":5114383,\\"balance\\":0.000534060075850865},{\\"block\\":5119348,\\"balance\\":0.000534060075850865},{\\"block\\":5124313,\\"balance\\":0.000534060075850865},{\\"block\\":5129278,\\"balance\\":0.000534060075850865},{\\"block\\":5134243,\\"balance\\":0.000534060075850865},{\\"block\\":5139208,\\"balance\\":0.02642294883092996},{\\"block\\":5144173,\\"balance\\":0.026442948830929958},{\\"block\\":5149138,\\"balance\\":0.026442948830929958},{\\"block\\":5154103,\\"balance\\":0.026442948830929958},{\\"block\\":5159068,\\"balance\\":0.03494294883092996},{\\"block\\":5164033,\\"balance\\":0.03993222727211795},{\\"block\\":5168998,\\"balance\\":0.04793222727211795},{\\"block\\":5173963,\\"balance\\":0.04793222727211795},{\\"block\\":5178928,\\"balance\\":0.04793222727211795},{\\"block\\":5183893,\\"balance\\":0.04793222727211795},{\\"block\\":5188858,\\"balance\\":0.04076568624207472},{\\"block\\":5193823,\\"balance\\":0.04076568624207472},{\\"block\\":5198788,\\"balance\\":0.04076568624207472},{\\"block\\":5203753,\\"balance\\":0.04076568624207472},{\\"block\\":5208718,\\"balance\\":0.04076568624207472},{\\"block\\":5213683,\\"balance\\":0.04076568624207472},{\\"block\\":5218648,\\"balance\\":0.04076568624207472},{\\"block\\":5223613,\\"balance\\":0.04076568624207472},{\\"block\\":5228578,\\"balance\\":0.04076568624207472},{\\"block\\":5233543,\\"balance\\":0.04076568624207472},{\\"block\\":5238508,\\"balance\\":0.04076568624207472}]\\n```\\n\\nLooks good so far! But if you run the code yourself, you will feel how dreadfully slow it is... That is because we are essentially doing a serial query, waiting around 100ms for every data-point:\\n\\n![](/assets/images/img_5aa5b062b26a2.png)\\n\\nBut we can dramatically improve this by making the Web3.js requests asynchronous and in parallel!\\n\\n## Web3.js Asynchronous and Parallel Calls\\n\\nWe will need to update our `getBalanceInRange` function. Instead of `await`ing the balance data within each loop, we should simply queue up all of the requests, and then have them be processed at once with `Promise.all`.\\n\\nSo our new loop would look something like this:\\n\\n```javascript\\n// Given an address and a range of blocks, query the Ethereum blockchain for the ETH balance across the range\\nasync function getBalanceInRange(address, startBlock, endBlock) {\\n  // Number of points to fetch between block range\\n  var pointCount = 50;\\n\\n  // Calculate the step size given the range of blocks and the number of points we want\\n  var step = Math.floor((endBlock - startBlock) / pointCount);\\n  // Make sure step is at least 1\\n  if (step < 1) {\\n    step = 1;\\n  }\\n\\n  // Queue the promises here\\n  var promises = [];\\n\\n  // Loop over the blocks, using the step value\\n  for (let i = startBlock; i < endBlock; i = i + step) {\\n    // Create a promise to query the ETH balance for that block\\n    var promise = promisify((cb) => web3.eth.getBalance(address, i, cb));\\n    // Queue the promise and include data about the block for output\\n    promises.push(\\n      promise.then((wei) => ({\\n        block: i,\\n        balance: parseFloat(web3.fromWei(wei, \\"ether\\")),\\n      }))\\n    );\\n  }\\n  // Resolve all promises in parellel\\n  var balances = await Promise.all(promises);\\n\\n  return balances;\\n}\\n```\\n\\nRunning this is MUCH faster, and the parallel nature of the calls can be seen in the network traces:\\n\\n![](/assets/images/img_5aa5b09b0bb6f.png)\\n\\n## Final Result\\n\\nFrom there, you just need to pipe the data into your favorite JavaScript graph library, and you will get a view like this:\\n\\n![](/assets/images/img_5aa5ad6c84572.png)\\n\\nHere you can see the quick rise of the ponzi scheme, the sell offs of \'weak hands\', and finally the contract getting hacked and liquidating all the funds.\\n\\nI will not go into detail about making this graph in this tutorial, but you can see my final app here:\\n\\n[https://shawntabrizi.com/ethgraph/?address=0xA7CA36F7273D4d38fc2aEC5A454C497F86728a7A&start=4990258&end=5010862](https://shawntabrizi.com/ethgraph/?address=0xA7CA36F7273D4d38fc2aEC5A454C497F86728a7A&start=4990258&end=5010862)\\n\\nYou can find the source code here:\\n\\n[https://github.com/shawntabrizi/ETH-Balance-Graph](https://github.com/shawntabrizi/ETH-Balance-Graph)\\n\\nIf you have any suggestions, or find any bugs please let me know! If you found this short tutorial helpful, feel free to send a donation using the information [here](https://shawntabrizi.com/donate/)."},{"id":"/code/set-azure-service-health-alerts-programmatically-using-powershell/","metadata":{"permalink":"/blog/code/set-azure-service-health-alerts-programmatically-using-powershell/","source":"@site/blog/2018-01-10-set-azure-service-health-alerts-programmatically-using-powershell.md","title":"Set up Azure Service Health Alerts programmatically using PowerShell","description":"The Azure Service Health team has been working hard to make it easy for you to set up custom service health alerts for your Azure resources. While we primarily focus on user experiences in the portal, we also know that there are many power users who are interested in doing these same actions in a programmatic way.","date":"2018-01-11T07:03:59.000Z","tags":[{"inline":true,"label":"arm","permalink":"/blog/tags/arm"},{"inline":true,"label":"azure resource manager","permalink":"/blog/tags/azure-resource-manager"},{"inline":true,"label":"azure service health","permalink":"/blog/tags/azure-service-health"},{"inline":true,"label":"health alert","permalink":"/blog/tags/health-alert"},{"inline":true,"label":"powershell","permalink":"/blog/tags/powershell"}],"readingTime":5.055,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Set up Azure Service Health Alerts programmatically using PowerShell","date":"2018-01-11T07:03:59.000Z","authors":"shawntabrizi","slug":"/code/set-azure-service-health-alerts-programmatically-using-powershell/","categories":["Code"],"tags":["arm","azure resource manager","azure service health","health alert","powershell"]},"unlisted":false,"prevItem":{"title":"Graphing ETH Balance History of an Ethereum Address using Parallel Asynchronous Requests in Web3.js","permalink":"/blog/ethereum/graphing-eth-balance-history-of-an-ethereum-address-using-parallel-asynchronous-requests-in-web3-js/"},"nextItem":{"title":"Programmatically fetch multiple APIs in parallel using async and await in JavaScript","permalink":"/blog/code/programmatically-fetch-multiple-apis-parallel-using-async-await-javascript/"}},"content":"The Azure Service Health team has been working hard to make it [easy for you to set up custom service health alerts](https://azure.microsoft.com/en-us/blog/get-notified-when-azure-service-incidents-impact-your-resources/) for your Azure resources. While we primarily focus on user experiences in the portal, we also know that there are many power users who are interested in doing these same actions in a programmatic way.\\n\\nThis post will walk you through the steps required to programmatically create a service health alert using [Azure Resource Manager templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-authoring-templates) and [Azure PowerShell](https://docs.microsoft.com/en-us/powershell/azure/overview).\\n\\nAs described [here](https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-create-activity-log-alerts-with-resource-manager-template), you can create any kind of activity log alert using this method ([Administrative, Autoscale, Recommendation, etc...](https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-overview-activity-logs#categories-in-the-activity-log)), however for the purposes of clarity, we will specifically focus on service health alerts.\\n\\n## Getting Started\\n\\nBefore you can follow this tutorial, you must have Azure PowerShell installed on your system. This will give you access to the `AzureRM` module, which is needed to interact with your Azure subscription.\\n\\nYou can follow the instructions [here to install Azure PowerShell](https://docs.microsoft.com/en-us/powershell/azure/install-azurerm-ps).\\n\\n## Creating an ARM Template for Service Health Alerts\\n\\nA service health alert is represented by JSON which is stored in your Azure subscription, and follows rules defined by Azure\'s internal system. This JSON is also referred to as the Azure Resource Manager template. You can find an example of a general Activity Log Alert template [here](https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-create-activity-log-alerts-with-resource-manager-template).\\n\\nFor service health alerts, we should use a template like this:\\n\\n```javascript\\n{\\n  \\"$schema\\": \\"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\\",\\n  \\"contentVersion\\": \\"1.0.0.0\\",\\n  \\"parameters\\": {\\n    \\"activityLogAlertName\\": {\\n      \\"type\\": \\"string\\",\\n      \\"metadata\\": {\\n        \\"description\\": \\"Unique name (within the Resource Group) for the Activity log alert.\\"\\n      }\\n    },\\n    \\"activityLogAlertEnabled\\": {\\n      \\"type\\": \\"bool\\",\\n      \\"defaultValue\\": true,\\n      \\"metadata\\": {\\n        \\"description\\": \\"Indicates whether or not the alert is enabled.\\"\\n      }\\n    },\\n    \\"actionGroupResourceId\\": {\\n      \\"type\\": \\"string\\",\\n      \\"metadata\\": {\\n        \\"description\\": \\"Resource Id for the Action group.\\"\\n      }\\n    }\\n  },\\n  \\"resources\\": [\\n    {\\n      \\"type\\": \\"Microsoft.Insights/activityLogAlerts\\",\\n      \\"apiVersion\\": \\"2017-04-01\\",\\n      \\"name\\": \\"[parameters(\'activityLogAlertName\')]\\",\\n      \\"location\\": \\"Global\\",\\n      \\"kind\\": null,\\n      \\"tags\\": {},\\n      \\"properties\\": {\\n        \\"enabled\\": \\"[parameters(\'activityLogAlertEnabled\')]\\",\\n        \\"description\\": \\"\\",\\n        \\"scopes\\": [\\n            \\"[subscription().id]\\"\\n        ],\\n        \\"condition\\": {\\n          \\"allOf\\": [\\n            {\\n              \\"field\\": \\"category\\",\\n              \\"equals\\": \\"ServiceHealth\\",\\n              \\"containsAny\\" : null\\n            }\\n          ]\\n        },\\n        \\"actions\\": {\\n          \\"actionGroups\\":\\n          [\\n            {\\n              \\"actionGroupId\\": \\"[parameters(\'actionGroupResourceId\')]\\",\\n              \\"webhookProperties\\": {}\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ]\\n}\\n```\\n\\nYou should save this file as `servicehealthalert.json`.\\n\\n#### A quick note on generating custom ARM templates\\n\\nI would assume that most of you reading this post do not want or need to start from scratch. Instead, you know what you want to achieve using the Azure portal UX, and you simply want to automate that process. In that case, there is an easy way for you to generate a custom ARM template for your needs:\\n\\n1.  Go to the [Health Alerts](https://portal.azure.com/#blade/Microsoft_Azure_Health/AzureHealthBrowseBlade/healthalerts) section in Azure Service Health\\n2.  Create a [new service health alert](https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-activity-log-alerts-on-service-notifications?toc=%2fazure%2fservice-health%2ftoc.json)\\n3.  Take note of the resource group you save the alert in\\n4.  Then go to the [Azure Resource Explorer](https://resources.azure.com/)\\n5.  Navigate to: subscriptions > (subscription) > resourceGroups > (resourceGroup) > providers > microsoft.insights > activityLogAlerts > (alertName)\\n6.  You will find a JSON representation of your alert with all your custom conditions\\n7.  Copy those extra conditions into the template above\\n\\n## Examples of Custom Conditions\\n\\nThe template provided above is a very broad service health alert that will be triggered whenever any activity log with the category `ServiceHealth` is created. However, using this same JSON template, you can further specify what kinds of events you want to be alerted for:\\n\\n- Only be notified for certain Azure services\\n- Only be notified for certain regions\\n- Only be notified for certain health alert types\\n\\nNote that there are over 122 Azure services, across more than 28 regions, so your JSON can start to get a little messy if you try to hand type it all out. I recommend you follow the instructions above on generating custom ARM templates.\\n\\nHere is an example of an ARM template that takes advantage of all 3 of these conditions:\\n\\n```javascript\\n{\\n   \\"$schema\\":\\"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\\",\\n   \\"contentVersion\\":\\"1.0.0.0\\",\\n   \\"parameters\\":{\\n      \\"activityLogAlertName\\":{\\n         \\"type\\":\\"string\\",\\n         \\"metadata\\":{\\n            \\"description\\":\\"Unique name (within the Resource Group) for the Activity log alert.\\"\\n         }\\n      },\\n      \\"activityLogAlertEnabled\\":{\\n         \\"type\\":\\"bool\\",\\n         \\"defaultValue\\":true,\\n         \\"metadata\\":{\\n            \\"description\\":\\"Indicates whether or not the alert is enabled.\\"\\n         }\\n      },\\n      \\"actionGroupResourceId\\":{\\n         \\"type\\":\\"string\\",\\n         \\"metadata\\":{\\n            \\"description\\":\\"Resource Id for the Action group.\\"\\n         }\\n      }\\n   },\\n   \\"resources\\":[\\n      {\\n         \\"type\\":\\"Microsoft.Insights/activityLogAlerts\\",\\n         \\"apiVersion\\":\\"2017-04-01\\",\\n         \\"name\\":\\"[parameters(\'activityLogAlertName\')]\\",\\n         \\"location\\":\\"Global\\",\\n         \\"kind\\":null,\\n         \\"tags\\":{\\n\\n         },\\n         \\"properties\\":{\\n            \\"enabled\\":\\"[parameters(\'activityLogAlertEnabled\')]\\",\\n            \\"description\\":\\"\\",\\n            \\"scopes\\":[\\n               \\"[subscription().id]\\"\\n            ],\\n            \\"condition\\":{\\n               \\"allOf\\":[\\n                  {\\n                     \\"field\\":\\"category\\",\\n                     \\"equals\\":\\"ServiceHealth\\",\\n                     \\"containsAny\\":null\\n                  },\\n                  {\\n                     \\"field\\":\\"properties.incidentType\\",\\n                     \\"equals\\":\\"Informational\\",\\n                     \\"containsAny\\":null\\n                  },\\n                  {\\n                     \\"field\\":\\"properties.incidentType\\",\\n                     \\"equals\\":\\"ActionRequired\\",\\n                     \\"containsAny\\":null\\n                  },\\n                  {\\n                     \\"field\\":\\"properties.impactedServices[?(@.ServiceName == \'Advisor\' || @.ServiceName == \'Alerts & Metrics\' || @.ServiceName == \'App Service\')].ImpactedRegions[*].RegionName\\",\\n                     \\"equals\\":null,\\n                     \\"containsAny\\":[\\n                        \\"Australia East\\",\\n                        \\"Brazil South\\",\\n                        \\"Canada East\\",\\n                        \\"Central US\\"\\n                     ]\\n                  }\\n               ]\\n            },\\n            \\"actions\\":{\\n               \\"actionGroups\\":[\\n                  {\\n                     \\"actionGroupId\\":\\"[parameters(\'actionGroupResourceId\')]\\",\\n                     \\"webhookProperties\\":{\\n\\n                     }\\n                  }\\n               ]\\n            }\\n         }\\n      }\\n   ]\\n}\\n```\\n\\nThe ARM template is actually pretty powerful in terms of the logic that it can resolve. You are not limited to only these 3 options when customizing the alert. You can pretty much add logic to any attribute which exists in the activity log. However, if you customize the JSON template too much, you might break the UX in the Azure portal. This is not a big deal, but can cause problems loading the alert in the portal, and if you do ever update the alert in the portal, you will likely lose all of your custom logic.\\n\\n## Creating a new alert using PowerShell\\n\\nNow that you have your ARM template created, creating a new activity log alert in your subscription is relatively easy. Using the `AzureRM` module, run the following:\\n\\n```powershell\\nLogin-AzureRmAccount\\n\\nSelect-AzureRmSubscription -SubscriptionName <subscription_name>\\n\\nNew-AzureRmResourceGroupDeployment -Name ExampleDeployment -ResourceGroupName <resource_group> -TemplateFile <path:\\\\to\\\\servicehealthalert.json>\\n```\\n\\nYou should then be prompted to type in the Alert Name and the Action Group Resource ID:\\n\\n```powershell\\nSupply values for the following parameters:\\n(Type !? for Help.)\\nactivityLogAlertName: <Alert Name>\\nactionGroupResourceId: /subscriptions/<subscriptionId>/resourceGroups/<resouceGroup>/providers/microsoft.insights/actionGroups/<actionGroup>\\n```\\n\\nIf there are no errors, you should get the following confirmation in PowerShell:\\n\\n```powershell\\nDeploymentName          : ExampleDeployment\\nResourceGroupName       : <resourceGroup>\\nProvisioningState       : Succeeded\\nTimestamp               : 11/8/2017 2:32:00 AM\\nMode                    : Incremental\\nTemplateLink            :\\nParameters              :\\n                          Name                     Type       Value\\n                          ===============          =========  ==========\\n                          activityLogAlertName     String     <Alert Name>\\n                          activityLogAlertEnabled  Bool       True\\n                          actionGroupResourceId    String     /subscriptions/<subscriptionId>/resourceGroups/<resouceGroup>/providers/microsoft.insights/actionGroups/<actionGroup>\\n\\nOutputs                 :\\nDeploymentDebugLogLevel :\\n```\\n\\nAnd that is it! You can now stay fully informed when Azure service issues affect you."},{"id":"/code/programmatically-fetch-multiple-apis-parallel-using-async-await-javascript/","metadata":{"permalink":"/blog/code/programmatically-fetch-multiple-apis-parallel-using-async-await-javascript/","source":"@site/blog/2017-12-09-programmatically-fetch-multiple-apis-parallel-using-async-await-javascript.md","title":"Programmatically fetch multiple APIs in parallel using async and await in JavaScript","description":"When I was building ethfolio, I had to figure out how to retrieve the token information for multiple Ethereum addresses. To get this information, you have to query an API per address that you want to retrieve.","date":"2017-12-09T11:13:44.000Z","tags":[{"inline":true,"label":"async","permalink":"/blog/tags/async"},{"inline":true,"label":"fetch","permalink":"/blog/tags/fetch"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"}],"readingTime":1.215,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Programmatically fetch multiple APIs in parallel using async and await in JavaScript","date":"2017-12-09T11:13:44.000Z","authors":"shawntabrizi","slug":"/code/programmatically-fetch-multiple-apis-parallel-using-async-await-javascript/","categories":["Code"],"tags":["async","fetch","javascript"]},"unlisted":false,"prevItem":{"title":"Set up Azure Service Health Alerts programmatically using PowerShell","permalink":"/blog/code/set-azure-service-health-alerts-programmatically-using-powershell/"},"nextItem":{"title":"ethfolio: A client side app to show your Ethereum token distribution","permalink":"/blog/ethereum/ethfolio-client-side-app-show-ethereum-token-distribution/"}},"content":"When I was building [ethfolio](https://shawntabrizi.com/ethfolio/), I had to figure out how to retrieve the token information for multiple Ethereum addresses. To get this information, you have to query an API per address that you want to retrieve.\\n\\nIdeally, all of these calls would happen Asynchronously and in parallel, to give the best and fastest experience to the user. However, how exactly to do this is not so obvious (to me at least).\\n\\nLet\'s assume that you have an array of URLs that you want to fetch at the same time. You can use an asynchronous function like this to easily retrieve all the data in the best way possible:\\n\\n```javascript\\nasync function getAllUrls(urls) {\\n  try {\\n    var data = await Promise.all(\\n      urls.map((url) => fetch(url).then((response) => response.json()))\\n    );\\n\\n    return data;\\n  } catch (error) {\\n    console.log(error);\\n\\n    throw error;\\n  }\\n}\\n```\\n\\nThis function will return an array of \'responses\' which you can then use for the rest of your program. Calling the function is simple too!\\n\\n```javascript\\nvar responses = await getAllUrls(urls);\\n```\\n\\nUltimately the trick here is that a `Promise` gets executed as soon as it gets created. The `map` function will create all the `fetch` promises and they will immediately start to execute. Then, we wait for all the promises to complete using the `Promise.all` function.\\n\\nI tried other ways to construct this kind of function, but none were quite as simple or effective as this. I hope you find this helpful :)"},{"id":"/ethereum/ethfolio-client-side-app-show-ethereum-token-distribution/","metadata":{"permalink":"/blog/ethereum/ethfolio-client-side-app-show-ethereum-token-distribution/","source":"@site/blog/2017-12-04-ethfolio-client-side-app-show-ethereum-token-distribution.md","title":"ethfolio: A client side app to show your Ethereum token distribution","description":"A common question that I have for others investing in cryptocurrencies is: \\"What coins are you invested in?\\"","date":"2017-12-04T15:54:33.000Z","tags":[{"inline":true,"label":"cryptocurrency","permalink":"/blog/tags/cryptocurrency"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"}],"readingTime":3.465,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"ethfolio: A client side app to show your Ethereum token distribution","date":"2017-12-04T15:54:33.000Z","authors":"shawntabrizi","slug":"/ethereum/ethfolio-client-side-app-show-ethereum-token-distribution/","categories":["Ethereum"],"tags":["cryptocurrency","ethereum","javascript"],"github":"ethfolio"},"unlisted":false,"prevItem":{"title":"Programmatically fetch multiple APIs in parallel using async and await in JavaScript","permalink":"/blog/code/programmatically-fetch-multiple-apis-parallel-using-async-await-javascript/"},"nextItem":{"title":"Making Web3.js work asynchronously with JavaScript Promises and await","permalink":"/blog/ethereum/making-web3-js-work-asynchronously-javascript-promises-await/"}},"content":"A common question that I have for others investing in cryptocurrencies is: _\\"What coins are you invested in?\\"_\\n\\nIt is surprisingly hard to distinguish fear, uncertainty, doubt (FUD), fear of missing out (FOMO), and solid informed advice apart from one another. But as the saying goes:\\n\\n> Put your money where your mouth is.\\n\\nThis is the goal of ethfolio:\\n\\n- Allow users to easily import the coins they currently are holding\\n- Allow users to share this token distribution\\n- And do so safely without needing to expose their Ethereum addresses or total value of their holdings\\n\\nThis actually seemed very straightforward, so I decided to tackle the project. Rather than go through all the work to actually query the blockchain as I have been learning to do in my other posts, I decided to just use an API which consolidates all this information.\\n\\n[Ethplorer.io](https://ethplorer.io/) was perfect for this. They have a `/getAddressInfo/` [api endpoint](https://github.com/EverexIO/Ethplorer/wiki/Ethplorer-API?from=etop) which provides details about the tokens at an Ethereum address, and even details about those tokens like the price in USD.\\n\\nThis was perfect! So I begin to envision the user story:\\n\\n1.  User goes to ethfolio, types in one or more Ethereum addresses\\n2.  User presses a \\"submit\\" button\\n3.  In the background we query the Ethplorer API to get all the tokens across all the addresses, and their current price in USD\\n4.  We then add up total amount of each token the user has across their wallets\\n5.  Then, we use the price information, to calculate the relative USD price of each token balance\\n6.  Then we calculate the total USD price across all balances, to find the percentage a particular token represents in the overall portfolio\\n7.  Display the results\\n\\nFollowing this process, the user can then share details about their token distribution without ever needing to share details about their specific addresses (which is important for anonymity) and without needing to share details about the total balance they hold (which could be sensitive to the user.)\\n\\nWith these steps in mind, I spent a day and created this pretty quickly:\\n\\n[https://shawntabrizi.com/ethfolio/](https://shawntabrizi.com/ethfolio/)\\n\\n![](/assets/images/img_5a24f8f17ea4a.png)\\n\\nYou may notice that there are some options to \\"Store total USD value\\" and \\"Store all addresses\\". Lets talk about what I tried to do with \\"Save & Share\\".\\n\\nI wanted this to be a completely client side application. This means I don\'t have to do any server stuff, the user can better trust the application (because they can see all the code), and calling the API would come from each user, so I wouldn\'t get throttled if the site takes off. My question then was: \\"How can I create a database for a client side application?\\"\\n\\nThis means that writing and reading from the database needs to be in such a way that it won\'t compromise the data being stored. My first thought was to try and use Google Forms/Sheets. Google Forms is a method for publicly entering data, which can get stored into a Google Sheet which can be publicly viewed. Using Google\'s authorization layer, I could make it so that users could not edit the sheet, only add data to it, and read from it.\\n\\nI got the part of the app to write to Google working. It doesn\'t look pretty because the app gives a [CORS error](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS), but Google seems to accept the data anyway.\\n\\n**Ignore the error**\\n\\n![](/assets/images/img_5a24fdb9c8555.png)\\n\\n**Cause data gets saved anyway...**\\n\\n![](/assets/images/img_5a24fd98c904a.png)\\n\\nHowever, I was not able to ever retrieve this data! Even though this sheet is public, it seems like to call any of Google\'s APIs you must be authenticated, at least with an API key.\\n\\nI created an API key, but trying to use it I get this error:\\n\\n![](/assets/images/img_5a24fec22ed25.png)\\n\\n```\\n\\"message\\": \\"API Key not found. Please pass a valid API key.\\"\\n```\\n\\nEvery API key is associated with a \\"Google App Project\\", and I needed to specifically enable the Google Sheets API for the project where I was creating the API key.\\n\\n![](/assets/images/img_5a2505a15b129.png)\\n\\nNow that this call works, I just need to build an interface to read from the sheet, and re-display the data! That will be coming up next!"},{"id":"/ethereum/making-web3-js-work-asynchronously-javascript-promises-await/","metadata":{"permalink":"/blog/ethereum/making-web3-js-work-asynchronously-javascript-promises-await/","source":"@site/blog/2017-11-24-making-web3-js-work-asynchronously-javascript-promises-await.md","title":"Making Web3.js work asynchronously with JavaScript Promises and await","description":"One of the things I learned when writing my \\"Hello World\\" tutorial for Ethereum and Web3.js was the importance of having your functions which call the blockchain run asynchronously. Without this, we would be unable to support users who use MetaMask as their Ethereum provider, and probably even more important, we may bring bad user experiences by locking up the browser during long HTTP requests. From the MetaMask developer FAQ:","date":"2017-11-24T08:47:40.000Z","tags":[{"inline":true,"label":"async","permalink":"/blog/tags/async"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"promises","permalink":"/blog/tags/promises"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"}],"readingTime":3.07,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Making Web3.js work asynchronously with JavaScript Promises and await","date":"2017-11-24T08:47:40.000Z","authors":"shawntabrizi","slug":"/ethereum/making-web3-js-work-asynchronously-javascript-promises-await/","categories":["Ethereum"],"tags":["async","javascript","promises","web3"]},"unlisted":false,"prevItem":{"title":"ethfolio: A client side app to show your Ethereum token distribution","permalink":"/blog/ethereum/ethfolio-client-side-app-show-ethereum-token-distribution/"},"nextItem":{"title":"Ethereum Token Contract ABI in Web3.js for ERC-20 and Human Standard Tokens","permalink":"/blog/ethereum/ethereum-token-contract-abi-web3-erc-20-human-standard-tokens/"}},"content":"One of the things I learned when writing my [\\"Hello World\\" tutorial for Ethereum and Web3.js](https://shawntabrizi.com/ethereum/correcting-ethereum-web3-js-hello-world/) was the importance of having your functions which call the blockchain run asynchronously. Without this, we would be unable to support users who use MetaMask as their Ethereum provider, and probably even more important, we may bring bad user experiences by locking up the browser during long HTTP requests. From the MetaMask developer FAQ:\\n\\n> Using synchronous calls is both a technical limitation and a user experience issue. They block the user\'s interface. So using them is a bad practice, anyway. Think of this API restriction as a gift to your users.\\n\\nSetting up a Web3 function to work asynchronously was pretty easy to figure out for a single call; but what about making multiple calls through Web3, that all need to be asynchronous, but also have dependencies on one another?\\n\\nAn example would be calculating the ERC-20 token balance of an Ethereum address. To do this, you need to know both the balance of tokens at the address, but also the decimals value for that token to convert to the right units. JavaScript Promises are the natural solution here. They allow you to track the status of an asynchronous function, and perform actions after your multiple dependencies all resolve.\\n\\n## Turning Web3.js functions into JavaScript Promises\\n\\nIn my [\\"Hello World\\" tutorial](https://shawntabrizi.com/ethereum/correcting-ethereum-web3-js-hello-world/), I show you can make an asynchronous requests by adding an error first callback to the Web3.js functions:\\n\\n```javascript\\nweb3.eth.getBalance(address, function (error, result) {\\n  if (!error) {\\n    console.log(result);\\n  } else {\\n    console.error(error);\\n  }\\n});\\n```\\n\\nAs I mentioned, if we depend on multiple calls from the Ethereum blockchain to create a result, using [JavaScript Promises](https://developers.google.com/web/fundamentals/primers/promises) is a good solution. They allow you to react to a success or a failure from an asynchronous function. Creating a promise from the error first callback function is pretty straightforward:\\n\\n```javascript\\nfunction getBalance (address) {\\n  return new Promise (function (resolve, reject) {\\n    web3.eth.getBalance(address, function (error, result) {\\n      if (error) {\\n        reject(error);\\n      } else {\\n        resolve(result);\\n    }\\n  })\\n}\\n```\\n\\nBut we can actually make this process even simpler for multiple Web3 functions by creating a wrapper which both makes the function asynchronous, and turn it into a promise; basically automating what we would repeat above for each different Web3 function we call.\\n\\nHere is the wrapper from [0xcaff posted in StackExchange](https://ethereum.stackexchange.com/a/24238/19577):\\n\\n```javascript\\nconst promisify = (inner) =>\\n  new Promise((resolve, reject) =>\\n    inner((err, res) => {\\n      if (err) {\\n        reject(err);\\n      } else {\\n        resolve(res);\\n      }\\n    })\\n  );\\n```\\n\\nNow that we have a Promise, we can take advantage of the [async/await pattern](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/async_function) which simplifies not only the look, but also the behavior of Promises.\\n\\nPutting this all together, let\'s show how simple this makes getting the token balance for an ETH account. We convert our original \\"Hello World\\" `getBalance` into an asynchronous function, like so:\\n\\n```javascript\\nasync function getBalance() {\\n  var address, wei, balance;\\n  address = document.getElementById(\\"address\\").value;\\n  wei = promisify((cb) => web3.eth.getBalance(address, cb));\\n  try {\\n    balance = web3.fromWei(await wei, \\"ether\\");\\n    document.getElementById(\\"output\\").innerHTML = balance + \\" ETH\\";\\n  } catch (error) {\\n    document.getElementById(\\"output\\").innerHTML = error;\\n  }\\n}\\n```\\n\\nNot much shorter for a single function, but will certainly make things better the more separate functions we call. My next post will show the results of these smaller educational posts, and how we can put it together to create the project I have been hinting above: [Getting the ERC-20 balance of an Ethereum Address](https://github.com/shawntabrizi/ERC20-Token-Balance).\\n\\nI hope this teaches you something! Again, this may be trivial to many, but was not so straightforward when I first started to tackling these problems. If it did help you, feel free to support me with a [small donation.](https://shawntabrizi.com/donate/)"},{"id":"/ethereum/ethereum-token-contract-abi-web3-erc-20-human-standard-tokens/","metadata":{"permalink":"/blog/ethereum/ethereum-token-contract-abi-web3-erc-20-human-standard-tokens/","source":"@site/blog/2017-11-08-ethereum-token-contract-abi-web3-erc-20-human-standard-tokens.md","title":"Ethereum Token Contract ABI in Web3.js for ERC-20 and Human Standard Tokens","description":"This post will introduce you to Token Contract ABIs in Ethereum, and show you how you can use a the Human Standard Token ABI to access the token balances of ERC-20 compatible tokens.","date":"2017-11-08T16:13:19.000Z","tags":[{"inline":true,"label":"application binary interface","permalink":"/blog/tags/application-binary-interface"},{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"erc20","permalink":"/blog/tags/erc-20"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"human standard token","permalink":"/blog/tags/human-standard-token"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"token","permalink":"/blog/tags/token"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"}],"readingTime":6.06,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Ethereum Token Contract ABI in Web3.js for ERC-20 and Human Standard Tokens","date":"2017-11-08T16:13:19.000Z","authors":"shawntabrizi","slug":"/ethereum/ethereum-token-contract-abi-web3-erc-20-human-standard-tokens/","categories":["Ethereum"],"tags":["application binary interface","blockchain","erc20","ethereum","human standard token","javascript","token","web3"],"github":"ERC-20-Token-Balance"},"unlisted":false,"prevItem":{"title":"Making Web3.js work asynchronously with JavaScript Promises and await","permalink":"/blog/ethereum/making-web3-js-work-asynchronously-javascript-promises-await/"},"nextItem":{"title":"Correcting the Ethereum and Web3.js \\"Hello World\\"","permalink":"/blog/ethereum/correcting-ethereum-web3-js-hello-world/"}},"content":"##### This post will introduce you to Token Contract ABIs in Ethereum, and show you how you can use a the [Human Standard Token ABI](https://github.com/shawntabrizi/ERC-20-Token-Balance/blob/master/human_standard_token_abi.js) to access the token balances of ERC-20 compatible tokens.\\n\\nLet me start by saying that this post may be trivial to some, but was very confusing to me, a brand new user to the Ethereum and Web3.js development world. After writing my [\\"Hello World\\" project](https://shawntabrizi.com/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/) which gets the ETH balance for an Ethereum Address in Web3, I began to think about the next small step I can take and teach to others. Naturally, I thought it would make sense to do a similar project, but instead, get the Token Balance for ERC-20 tokens at an Ethereum Address.\\n\\nWith Web3.js, you can easily find the template for this functionality:\\n\\n```javascript\\nvar tokenContract = eth.contract(tokenABI).at(tokenAddress);\\nvar tokenBalance = tokenContract.balanceOf(ethereumAddress);\\n```\\n\\nSeems easy enough to get the [Token Address](https://etherscan.io/tokens), but what is this `tokenABI` value that we need to use?\\n\\n## A simplified explanation of a token contract ABI\\n\\nA little bit of searching will give you documents that teach you about the Application Binary Interface (ABI) [like this](https://solidity.readthedocs.io/en/develop/abi-spec.html), but no real layman\'s terms explanation. Here is my attempt at one:\\n\\n> In Ethereum, the Application Binary Interface (ABI) is a predefined template of the functions a contract exposes.\\n\\nYou know when you import a new library into an IDE, you automatically get all that nice autocomplete and Intellisense? You type the library name, add \\"`.`\\" and a list of functions appears in front of you:\\n\\n![](/assets/images/img_5a02bedd6be8c.png)\\n\\nImagine if instead, you needed to know ahead of time the functions the library exposes, and then define them for the IDE so that the autocomplete would work... that is pretty much what is happening here.\\n\\nAn Ethereum Contract ABI will define the different functions that a contract exposes, and each function definition will contain things like the function type, name, inputs, outputs, [and more](https://solidity.readthedocs.io/en/develop/abi-spec.html#json). It even contains information like whether the contract accepts payments in ether or not. Here is the JSON ABI of the `balanceOf()` function we ultimately want to use:\\n\\n```json\\n{\\n  \\"constant\\": true,\\n  \\"inputs\\": [\\n    {\\n      \\"name\\": \\"_owner\\",\\n      \\"type\\": \\"address\\"\\n    }\\n  ],\\n  \\"name\\": \\"balanceOf\\",\\n  \\"outputs\\": [\\n    {\\n      \\"name\\": \\"balance\\",\\n      \\"type\\": \\"uint256\\"\\n    }\\n  ],\\n  \\"payable\\": false,\\n  \\"type\\": \\"function\\"\\n}\\n```\\n\\nI think this is easy enough to read: It is a `function` which accepts an `address` as an input, and outputs a `balance` as an unsigned int. **But is this enough to start talking to an Ethereum contract? What about all the other functions that contract might have? How will I find the full contract ABI for each contract I want to talk to?**\\n\\nHere are the things I learned when trying to answer these questions:\\n\\n- You do NOT need the full ABI to interact with a Token Contract. You only need to define the functions which you want to use.\\n- You cannot programmatically generate the ABI for a given contract using data from the Ethereum blockchain. In order to generate the full contract ABI from scratch, you will need the full contract source code, before it is compiled. Note that only the compiled code exists at a contract address.\\n- Some contracts have functions which are intentionally \'hidden\' from the public, and they do not intend the public to use.\\n\\nTherefore, there really is no way to dynamically call any contract address. If only there were some standard set of functions shared across all contracts...\\n\\n## The ERC-20 and Human Standard Token\\n\\nFrom [the specification](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-20-token-standard.md):\\n\\n> ## Simple Summary\\n>\\n> A standard interface for tokens.\\n\\nThe value of ERC-20 tokens is that they all have a standard set of functions which allow you to interact with each of them in the exact same way. This is why there is so much hype around new Ethereum application which use ERC-20 tokens: there is nearly zero effort to add these tokens to existing platforms like Cryptocurrency Exchanges which means smoother adoption, easier to sell/track, and of course more hype ($$$).\\n\\nWhat does it take to be ERC-20 compliant?\\n\\nNot much really. You just need to expose the non-optional methods and events described [here](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-20-token-standard.md). Beyond the core ERC-20 standard, there are also standard optional parameters which are intended for humans. See [here](https://github.com/ConsenSys/Tokens):\\n\\n> In other words. This is intended for deployment in something like a Token Factory or Mist wallet, and then used by humans. Imagine coins, currencies, shares, voting weight, etc. Machine-based, rapid creation of many tokens would not necessarily need these extra features or will be minted in other manners.\\n>\\n> 1. Initial Finite Supply (upon creation one specifies how much is minted). 2) In the absence of a token registry: Optional Decimal, Symbol & Name. 3) Optional approveAndCall() functionality to notify a contract if an approval() has occurred.\\n\\nThis is the \\"Human Standard Token\\", which of course is a super-set of the ERC-20 standard. Additionally, many tokens have a `version()` function which is also available in the ABI provided below.\\n\\n## So let\'s get it working!\\n\\nNow that you have sufficient background to understand what is going on, lets actually go and make some calls to ERC-20 token contracts.\\n\\nTo start, you can find the Human Standard Token ABI [here on GitHub](https://github.com/shawntabrizi/ERC-20-Token-Balance/blob/master/human_standard_token_abi.js). The JS file simply puts the ABI JSON object into a varaible called `human_standard_token_abi` which allows you to really easily use it in a project.\\n\\nThe most basic project that can take advantage of these things would look something like this:\\n\\n```html\\n<html>\\n  <head>\\n    <meta charset=\\"UTF-8\\" />\\n    <script type=\\"text/javascript\\" src=\\"./web3.min.js\\"><\/script>\\n    <script type=\\"text/javascript\\" src=\\"./human_standard_token_abi.js\\"><\/script>\\n    <script>\\n      var web3 = new Web3(\\n        new Web3.providers.HttpProvider(\\"https://mainnet.infura.io/<APIKEY>\\")\\n      );\\n\\n      address = \\"0x0e2e75240c69495d2b9e768b548db381de2142b9\\"; //From Etherscan\\n      contractAddress = \\"0xd26114cd6EE289AccF82350c8d8487fedB8A0C07\\"; //OMG\\n      contractABI = human_standard_token_abi;\\n\\n      tokenContract = web3.eth.contract(contractABI).at(contractAddress);\\n\\n      console.log(tokenContract.balanceOf(address).toNumber());\\n    <\/script>\\n  </head>\\n  <body>\\n    <p>Check the console (F12)</p>\\n  </body>\\n</html>\\n```\\n\\nAnd the output:\\n\\n![](/assets/images/img_5a02b864c00dc.png)\\n\\nTake a look at an implementation of this sample [here](https://shawntabrizi.com/ERC20-Token-Balance/). You can find the source code for this on [GitHub](https://github.com/shawntabrizi/ERC20-Token-Balance).\\n\\n## What if...\\n\\nLet\'s try a few fringe scenarios with this contract ABI. We will be testing this against [OmiseGo](https://etherscan.io/token/OmiseGo?a=0x0e2e75240c69495d2b9e768b548db381de2142b9#readContract).\\n\\n- What if we call a contract with a function we defined, but does not exist on the contract?\\n\\nOMG does not have a `version()` function, but we define it by default in our `human_standard_token_abi`. So let\'s call it:\\n\\n```javascript\\nconsole.log(tokenContract.version());\\n```\\n\\n> \\"Uncaught Error: new BigNumber() not a base 16 number\\"\\n\\n![](/assets/images/img_5a02ba10afdf7.png)\\n\\n- What if we call a contract with a function that exists, but we did not define in the ABI?\\n\\nOMG has the function `mintingFinished()` which is not part of the Human Standard Token ABI. If we call it, we get the following error:\\n\\n> \\"Uncaught TypeError: tokenContract.mintingFinished is not a function\\"\\n\\n![](/assets/images/img_5a02bad1835e2.png)\\n\\n## What I hope you learned:\\n\\n- The requirements to call an ERC-20 compliant token contract using Web3.js\\n- What a Token Contract ABI is on the Ethereum Blockchain\\n- Why the ERC-20 standard is pretty great for develeopers\\n- How to easily add the `human_standard_token_abi` to your JavaScript web application\\n\\nKeep an eye out for [my next post](https://shawntabrizi.com/ethereum/making-web3-js-work-asynchronously-javascript-promises-await/), which will detail how to set up Web3.js for JavaScript promises, so we can execute multiple Web3 functions asynchronously.\\n\\nIf you liked this post, and want to support me, feel free to [send me a donation.](https://shawntabrizi.com/donate/)"},{"id":"/ethereum/correcting-ethereum-web3-js-hello-world/","metadata":{"permalink":"/blog/ethereum/correcting-ethereum-web3-js-hello-world/","source":"@site/blog/2017-11-05-correcting-ethereum-web3-js-hello-world.md","title":"Correcting the Ethereum and Web3.js \\"Hello World\\"","description":"Just 2 days ago I blogged about a quick project which I considered a \\"Hello World\\" application for Ethereum and Web3.js. However, I quickly learned that even in my short 31 lines of code, I made numerous mistakes which do not follow the best practices for developing Web3.js applications.","date":"2017-11-05T15:57:51.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"html","permalink":"/blog/tags/html"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"}],"readingTime":3.625,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Correcting the Ethereum and Web3.js \\"Hello World\\"","date":"2017-11-05T15:57:51.000Z","authors":"shawntabrizi","slug":"/ethereum/correcting-ethereum-web3-js-hello-world/","categories":["Ethereum"],"tags":["blockchain","ethereum","html","javascript","web3"]},"unlisted":false,"prevItem":{"title":"Ethereum Token Contract ABI in Web3.js for ERC-20 and Human Standard Tokens","permalink":"/blog/ethereum/ethereum-token-contract-abi-web3-erc-20-human-standard-tokens/"},"nextItem":{"title":"Ethereum and Web3.js \\"Hello World\\": Get the ETH Balance of an Ethereum Address","permalink":"/blog/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/"}},"content":"Just 2 days ago I [blogged about a quick project](https://shawntabrizi.com/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/) which I considered a \\"Hello World\\" application for Ethereum and Web3.js. However, I quickly learned that even in my short 31 lines of code, I made numerous mistakes which do not follow the best practices for developing Web3.js applications.\\n\\nThe main part of the sample was the Web3.js stuff, which could be broken into two logical sections:\\n\\n1.  Establishing a Web3 Provider\\n2.  Getting the ETH balance of an Ethereum Address\\n\\nBoth of these sections had mistakes in my original code, and this post will show you how to fix them! I will be updating the main blog post to include these changes as well, but I wanted to document the subtleties of the changes, and what I have learned since then. BTW, all of these mistakes could be avoided if you read the [MetaMask developer documentation](https://github.com/MetaMask/faq/blob/master/DEVELOPERS.md#partly_sunny-web3---ethereum-browser-environment-check).\\n\\n## Ethereum Browser Environment Check\\n\\nIn my original sample, I simply depend on the Web3 HTTP Provider to access the Ethereum network. However, using [MetaMask](https://metamask.io/) or the [Mist Browser](https://github.com/ethereum/mist), users will already have direct access to the Ethereum network through those providers, and do not need to use the HTTP Provider. As said in the [Web3 JavaScript app API Documentation](https://github.com/ethereum/wiki/wiki/JavaScript-API#adding-web3):\\n\\n> ...you need to create a web3 instance, setting a provider. To make sure you don\'t overwrite the already set provider when in mist, check first if the web3 is available...\\n\\nTo fix this, we mostly follow the [code sample](https://github.com/MetaMask/faq/blob/master/DEVELOPERS.md#partly_sunny-web3---ethereum-browser-environment-check) provided by MetaMask:\\n\\n```javascript\\nwindow.addEventListener(\\"load\\", function () {\\n  if (typeof web3 !== \\"undefined\\") {\\n    console.log(\\"Web3 Detected! \\" + web3.currentProvider.constructor.name);\\n    window.web3 = new Web3(web3.currentProvider);\\n  } else {\\n    console.log(\\"No Web3 Detected... using HTTP Provider\\");\\n    window.web3 = new Web3(\\n      new Web3.providers.HttpProvider(\\"https://mainnet.infura.io/noapikey\\")\\n    );\\n  }\\n});\\n```\\n\\nAs they mention on the MetaMask developer documentation:\\n\\n> Note that the environmental web3 check is wrapped in a `window.addEventListener(\'load\', ...)` handler. This approach avoids race conditions with web3 injection timing.\\n\\nWith our new code, as soon as the page loads, we detect if the browser being used already has a Web3 provider set up, and if it does we use it! Otherwise, we will use the HTTP Provider from [Infura.io](https://infura.io/). For most users, I would assume they do not have MetaMask, and thus this change is not very important; but it is certainly best practice, and I am happy to oblige.\\n\\nChrome with MetaMask:\\n\\n![](/assets/images/img_59feb77ae6a85.png)\\n\\nFirefox without Web3 Provider:\\n\\n![](/assets/images/img_59feb7629ffba.png)\\n\\n## Asynchronous calls to the Ethereum network\\n\\nIf you have been following along word for word, you might have copied the changes mentioned above, loaded it in your MetaMask enabled browser ([from your web server](https://github.com/MetaMask/faq/blob/master/DEVELOPERS.md#globe_with_meridians-https---web-server-required)), and tried to get your ETH balance... Here is what you will see:\\n\\n![](/assets/images/img_59feb8e353a07.png)\\n\\nIf we continue to read the MetaMask developer documentation, we would see the following:\\n\\n> The user does not have the full blockchain on their machine, so data lookups can be a little slow. For this reason, we are unable to support most synchronous methods.\\n\\nThis means we need to turn our call to get the ETH balance, which is currently a synchronous HTTP request, into an asynchronous request. We can do this by adding an error first callback as the last parameter of the function:\\n\\n```javascript\\nfunction getBalance() {\\n  var address, wei, balance;\\n  address = document.getElementById(\\"address\\").value;\\n  try {\\n    web3.eth.getBalance(address, function (error, wei) {\\n      if (!error) {\\n        var balance = web3.fromWei(wei, \\"ether\\");\\n        document.getElementById(\\"output\\").innerHTML = balance + \\" ETH\\";\\n      }\\n    });\\n  } catch (err) {\\n    document.getElementById(\\"output\\").innerHTML = err;\\n  }\\n}\\n```\\n\\nIf we try to run this code now with MetaMask as our provider, everything works again!\\n\\n![](/assets/images/img_59febfad543a1.png)\\n\\n## The first, but certainly not last mistake...\\n\\nPhew! We fixed our Hello World application! Take a look at the [overall changes on GitHub](https://github.com/shawntabrizi/ETH-Balance/commit/daa8ac6c380c6f870807023e295d51a03a21edef). I think this goes to show how difficult it can be to learn things on your own, and some of the best practices that can be overlooked so easily. I hope that I am able to go through these issues so that you don\'t have to. If you find any other issues with this or future samples I create, please let me know!\\n\\nSpecial shout out to [Reddit user JonnyLatte](https://www.reddit.com/r/ethdev/comments/7acshg/in_the_spirit_of_devcon3_build_your_first_web3js/dp9xdff/?utm_content=permalink&utm_medium=user&utm_source=reddit&utm_name=frontpage) for telling me the errors in my ways, and getting me to read more of the documentation around Web3!\\n\\nAs always, if you found this content helpful, feel free to check out my [donation page](https://shawntabrizi.com/donate/)."},{"id":"/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/","metadata":{"permalink":"/blog/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/","source":"@site/blog/2017-11-02-ethereum-web3-js-hello-world-get-eth-balance-ethereum-address.md","title":"Ethereum and Web3.js \\"Hello World\\": Get the ETH Balance of an Ethereum Address","description":"Using just 41 lines of HTML + JS, we create a Web3.JS application which can get the ETH Balance of an Ethereum Address [Final Result] [GitHub]","date":"2017-11-02T18:19:09.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"html","permalink":"/blog/tags/html"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"web3","permalink":"/blog/tags/web-3"}],"readingTime":7.675,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Ethereum and Web3.js \\"Hello World\\": Get the ETH Balance of an Ethereum Address","date":"2017-11-02T18:19:09.000Z","authors":"shawntabrizi","slug":"/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/","categories":["Ethereum"],"tags":["blockchain","ethereum","html","javascript","web3"],"github":"ETH-Balance"},"unlisted":false,"prevItem":{"title":"Correcting the Ethereum and Web3.js \\"Hello World\\"","permalink":"/blog/ethereum/correcting-ethereum-web3-js-hello-world/"},"nextItem":{"title":"Getting started with Ethereum Wallet","permalink":"/blog/ethereum/getting-started-ethereum-wallet/"}},"content":"##### Using just 41 lines of HTML + JS, we create a Web3.JS application which can get the ETH Balance of an Ethereum Address [[Final Result]](https://shawntabrizi.com/ethbalance/) [[GitHub]](https://github.com/shawntabrizi/ETH-Balance)\\n\\nFor me, the hardest part of learning new technical skills is overcoming the hurdle of simply getting started. The Ethereum development space is booming, and the ability to make relatively simple web applications that interact with the Ethereum blockchain is at a premium. Today, development on World Wide Web requires you to compete with a huge number of fully developed, feature rich applications, where it is very unlikely that you are actually contributing value. However, the same is absolutely not true for Ethereum and blockchain as a whole. There are so many utilities and tools that can bring value to this ecosystem, all with relatively low feature requirements.\\n\\n**So let\'s overcome the first barrier by building a \\"Hello World\\" application.**\\n\\nFrom my perspective, the perfect project for something like this would be a bare-bones single-page application which fetches the ETH balance of an Ethereum address. This is about as simple as it gets to allow a user to interact with the blockchain, and thanks to Web3.js, it is also really simple to implement!\\n\\n## Prerequisites\\n\\nTo gain access to the Ethereum network, you will need to gain access to a Web3 Provider. As I will talk about more below, this comes natively with certain Ethereum focused browsers, but for the average user you will need to provide them with their own gateway to the blockchain. Basically, you need someone to provide your app the data that is actually on the blockchain, and Web3.js has the ability to interact directly with an HTTP Provider to bring you this data with minimal effort.\\n\\nI used [Infura.io](https://infura.io/) as my Ethereum provider (for no other reason than they showed up first when searching), and after spending less than a minute registering with them for free, I was given my unique address to their Main Ethereum Network where my API Key was appended at the end.\\n\\n![](/assets/images/img_59fad787df3db.png)\\n\\nSave this URL, as you will be using it very shortly.\\n\\n```\\nhttps://mainnet.infura.io/<apikey>\\n```\\n\\nThe only other thing you need to get started is your own copy of Web3.js which can be [found on GitHub](https://github.com/ethereum/web3.js). Just download and unpack the ZIP file.\\n\\nCreate a new folder where you want your project to live, and create an `index.html` file. Then, from the Web3.js download, copy `web3.min.js` to that folder.\\n\\n```\\nethbalance/     (folder)\\n\u251c\u2500\u2500 index.html\\n\u2514\u2500\u2500 web3.min.js\\n```\\n\\nFinally, make sure you initialize your `index.html` skeleton:\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <meta charset=\\"UTF-8\\" />\\n  </head>\\n\\n  <body></body>\\n</html>\\n```\\n\\n## Let\'s get started!\\n\\nTo make this as simple as possible, we are going to create a single HTML file which will contain all the code necessary to complete this project. It will be broken into 2 parts:\\n\\n1.  HTML to render a bare-bones web page\\n2.  JavaScript to initialize a Web3 object, and interact with our HTTP Provider\\n\\n### HTML Body\\n\\nOur HTML body needs a text field for the user to input an Ethereum address, a button to trigger the JavaScript, and an output area to display the result. I am going to assume that if you are reading this post, you have enough familiarity with HTML that I can just breeze over this, and give you the code:\\n\\n```html\\n<body>\\n  <h1>ETH Balance Fetcher</h1>\\n  <p>Enter your Ethereum Address:</p>\\n  <input type=\\"text\\" size=\\"50\\" id=\\"address\\" />\\n  <button type=\\"button\\" onClick=\\"getBalance();\\">Get Balance</button>\\n  <br />\\n  <br />\\n  <div id=\\"output\\"></div>\\n</body>\\n```\\n\\nThe only thing of note here is that when you click the button we created, it triggers a JavaScript function `getBalance()`, and that is what we are going to write next!\\n\\n### HTML Head: JavaScript and Web3\\n\\nNow it is time to prepare the JavaScript required to make this all work. We are going to need to get the Ethereum address inputted by the user, initiate our connection to the Ethereum Provider, and then query the blockchain for the ETH balance at that address. Oh, of course we will also send back the result and update the HTML with the value. Here is our HTML head template:\\n\\n```html\\n<head>\\n  \x3c!-- Check if Web3 already defined --\x3e\\n  \x3c!-- If not, connect to HTTP Provider --\x3e\\n  \x3c!-- getBalance() function --\x3e\\n</head>\\n```\\n\\nFirst we will load and set up our Web3 provider. If you are using an Ethereum compatible browser like [Brave](https://brave.com/), Chrome + [MetaMask](https://metamask.io/), or [Mist](https://github.com/ethereum/mist/releases), you will already have your own Web3 provider established natively. You can access that connection with this:\\n\\n```html\\n<head>\\n  <meta charset=\\"UTF-8\\" />\\n  <script type=\\"text/javascript\\" src=\\"./web3.min.js\\"><\/script>\\n  <script type=\\"text/javascript\\">\\n    window.addEventListener(\\"load\\", function () {\\n      if (typeof web3 !== \\"undefined\\") {\\n        console.log(\\"Web3 Detected! \\" + web3.currentProvider.constructor.name);\\n        window.web3 = new Web3(web3.currentProvider);\\n      }\\n    });\\n\\n    \x3c!-- If not, connect to HTTP Provider --\x3e\\n    \x3c!-- getBalance() function --\x3e\\n  <\/script>\\n</head>\\n```\\n\\nMore likely, the user does not have one of these browsers, so we need to establish our own connection to the Ethereum network. We can do this with the URL that you saved earlier from Infura.io, and establishing an HTTP Provider:\\n\\n```html\\n<head>\\n  <meta charset=\\"UTF-8\\" />\\n  <script type=\\"text/javascript\\" src=\\"./web3.min.js\\"><\/script>\\n  <script type=\\"text/javascript\\">\\n    window.addEventListener(\\"load\\", function () {\\n      if (typeof web3 !== \\"undefined\\") {\\n        console.log(\\"Web3 Detected! \\" + web3.currentProvider.constructor.name);\\n        window.web3 = new Web3(web3.currentProvider);\\n      } else {\\n        console.log(\\"No Web3 Detected... using HTTP Provider\\");\\n        window.web3 = new Web3(\\n          new Web3.providers.HttpProvider(\\"https://mainnet.infura.io/<APIKEY>\\")\\n        );\\n      }\\n    });\\n\\n    \x3c!-- getBalance() function --\x3e\\n  <\/script>\\n</head>\\n```\\n\\nAt this point, we can do just about anything that Web3.js offers, but for our purposes we only need to query the blockchain for the address, and return the ETH balance. So let\'s set up our `getBalance()` function:\\n\\n```html\\n<head>\\n  <meta charset=\\"UTF-8\\" />\\n  <script type=\\"text/javascript\\" src=\\"./web3.min.js\\"><\/script>\\n  <script type=\\"text/javascript\\">\\n    window.addEventListener(\\"load\\", function () {\\n      if (typeof web3 !== \\"undefined\\") {\\n        console.log(\\"Web3 Detected! \\" + web3.currentProvider.constructor.name);\\n        window.web3 = new Web3(web3.currentProvider);\\n      } else {\\n        console.log(\\"No Web3 Detected... using HTTP Provider\\");\\n        window.web3 = new Web3(\\n          new Web3.providers.HttpProvider(\\"https://mainnet.infura.io/<APIKEY>\\")\\n        );\\n      }\\n    });\\n    function getBalance() {\\n      var address, wei, balance;\\n      address = document.getElementById(\\"address\\").value;\\n      try {\\n        web3.eth.getBalance(address, function (error, wei) {\\n          if (!error) {\\n            var balance = web3.fromWei(wei, \\"ether\\");\\n            document.getElementById(\\"output\\").innerHTML = balance + \\" ETH\\";\\n          }\\n        });\\n      } catch (err) {\\n        document.getElementById(\\"output\\").innerHTML = err;\\n      }\\n    }\\n  <\/script>\\n</head>\\n```\\n\\nWalking through the function:\\n\\nFirst we store the value of the text field from our HTML page into the `address` variable. Then, we will `try` to use the `web3` object we initialized earlier to call the function `web3.eth.getBalance()` which accepts an Ethereum Address as an input. Note that we need to make this call asynchronously as the user does not have the full blockchain loaded on their machine, so some calls may run slow. Rather than lock the user\'s interface, we let the the call happen in the background, and when it is complete, we trigger an update to the page. This is [required to support MetaMask](https://github.com/MetaMask/faq/blob/master/DEVELOPERS.md#dizzy-all-async---think-of-metamask-as-a-light-client), but benefits all Web3 applications. If you want to learn more about how to make these requests asynchronous, take a look at the [\\"Using callbacks\\" section](https://github.com/ethereum/wiki/wiki/JavaScript-API#using-callbacks) in the Web3 documentation.\\n\\nOnce the asynchronous request is complete, we will get back a Wei balance as a result. But we want the Ether value, so we do one last step to convert the value: `web3.fromWei(wei, \'ether\')`. If all of this is successful, we update the `output` div with our result, otherwise if it fails at any point we `catch` the error, and output that message instead.\\n\\nHere is the final `index.html` file which you should be able to use as soon as you paste in your `apikey` from Infura.io. You can also download this project directly from my [GitHub](https://github.com/shawntabrizi/ETH-Balance).\\n\\n```html\\n<!doctype html>\\n<html>\\n  <head>\\n    <meta charset=\\"UTF-8\\" />\\n    <script type=\\"text/javascript\\" src=\\"./web3.min.js\\"><\/script>\\n    <script type=\\"text/javascript\\">\\n      window.addEventListener(\\"load\\", function () {\\n        if (typeof web3 !== \\"undefined\\") {\\n          console.log(\\n            \\"Web3 Detected! \\" + web3.currentProvider.constructor.name\\n          );\\n          window.web3 = new Web3(web3.currentProvider);\\n        } else {\\n          console.log(\\"No Web3 Detected... using HTTP Provider\\");\\n          window.web3 = new Web3(\\n            new Web3.providers.HttpProvider(\\n              \\"https://mainnet.infura.io/<APIKEY>\\"\\n            )\\n          );\\n        }\\n      });\\n      function getBalance() {\\n        var address, wei, balance;\\n        address = document.getElementById(\\"address\\").value;\\n        try {\\n          web3.eth.getBalance(address, function (error, wei) {\\n            if (!error) {\\n              var balance = web3.fromWei(wei, \\"ether\\");\\n              document.getElementById(\\"output\\").innerHTML = balance + \\" ETH\\";\\n            }\\n          });\\n        } catch (err) {\\n          document.getElementById(\\"output\\").innerHTML = err;\\n        }\\n      }\\n    <\/script>\\n  </head>\\n  <body>\\n    <h1>ETH Balance Fetcher</h1>\\n    <p>Enter your Ethereum Address:</p>\\n    <input type=\\"text\\" size=\\"50\\" id=\\"address\\" />\\n    <button type=\\"button\\" onClick=\\"getBalance();\\">Get Balance</button>\\n    <br />\\n    <br />\\n    <div id=\\"output\\"></div>\\n  </body>\\n</html>\\n```\\n\\nYou can try this out right now using the version I hosted here: [https://shawntabrizi.com/ethbalance/](https://shawntabrizi.com/ethbalance/)\\n\\n![](/assets/images/img_59faeb0dd20de.png)\\n\\nOne thing you should note is that this is a client-side JavaScript application. There are a million reasons why making client-side apps is so much easier for development, but one big downside is that your API key will be exposed to anyone who simply inspects the HTML. Please do not abuse my API key, and please do not ship a production application using a method like this unless you are prepared to get your API key terminated.\\n\\nNote that I have made updates to this blog post, correcting some issues regarding best practices with Web3\\\\. You can learn more about the errors I made along the way [here](https://shawntabrizi.com/ethereum/correcting-ethereum-web3-js-hello-world/).\\n\\nI hope this helps you get started with developing for Ethereum using Web3.js! If you liked this content, and want to support me, feel free to [send a donation](https://shawntabrizi.com/donate/)."},{"id":"/ethereum/getting-started-ethereum-wallet/","metadata":{"permalink":"/blog/ethereum/getting-started-ethereum-wallet/","source":"@site/blog/2017-10-16-getting-started-ethereum-wallet.md","title":"Getting started with Ethereum Wallet","description":"Well, Cryptocurrencies have consumed my attention for the past 5 months. I first learned about Ethereum back in May when I was attending a Bachelor party. Since then, I have been a relatively hands off investor, but it is time to actually start contributing and learning about the development platform.","date":"2017-10-16T13:14:22.000Z","tags":[{"inline":true,"label":"blockchain","permalink":"/blog/tags/blockchain"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"}],"readingTime":2.09,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Getting started with Ethereum Wallet","date":"2017-10-16T13:14:22.000Z","authors":"shawntabrizi","slug":"/ethereum/getting-started-ethereum-wallet/","categories":["Ethereum"],"tags":["blockchain","ethereum"]},"unlisted":false,"prevItem":{"title":"Ethereum and Web3.js \\"Hello World\\": Get the ETH Balance of an Ethereum Address","permalink":"/blog/ethereum/ethereum-web3-js-hello-world-get-eth-balance-ethereum-address/"},"nextItem":{"title":"Adding AAD Service Principal to the Company Administrator Role using the AAD PowerShell Module","permalink":"/blog/aad/adding-aad-service-principal-company-administrator-role-using-aad-powershell-module/"}},"content":"Well, Cryptocurrencies have consumed my attention for the past 5 months. I first learned about Ethereum back in May when I was attending a Bachelor party. Since then, I have been a relatively hands off investor, but it is time to actually start contributing and learning about the development platform.\\n\\nTo start, we need to install the Ethereum Wallet, which also downloads the entire Ethereum chain, which as of this post is over 4.3 million blocks long. The unfortunate part of this process is that it takes a lot of disk space, and if you are like me, you have a SSD for your OS and installed programs, and an HDD for storage. 4.3 million blocks is supposed to be around 70+ GB of files, which I certainly do not want on my SSD.\\n\\nEthereum Wallet does not currently let you choose where to store these files, so by default they go to:\\n\\n```\\nC:\\\\Users\\\\<username>\\\\AppData\\\\Roaming\\\\Ethereum\\n```\\n\\nSo how can we move these files to our HDD?\\n\\nUsing [Symbolic Links](https://en.wikipedia.org/wiki/Symbolic_link) (Symlinks)!\\n\\n# Let\'s do it!\\n\\nI want to store all these files on my HDD at this folder path:\\n\\n```\\nD:\\\\Cryptocoins\\\\Ethereum\\n```\\n\\nBefore I install [Ethereum Wallet](https://github.com/ethereum/mist/releases), I should set up the symlink as so:\\n\\n> Run CMD as an Administrator\\n\\n```bash\\nmklink /j \\"C:\\\\Users\\\\<username>\\\\AppData\\\\Roaming\\\\Ethereum\\" \\"D:\\\\Cryptocoins\\\\Ethereum\\"\\n```\\n\\nIf successful, you will get the following message:\\n\\n```\\nJunction created for C:\\\\Users\\\\<username>\\\\AppData\\\\Roaming\\\\Ethereum <<===>> D:\\\\Cryptocoins\\\\Ethereum\\n```\\n\\nThis will create a [Directory Junction](https://en.wikipedia.org/wiki/NTFS_junction_point) which basically makes the first folder path point to the second folder path. Then when I install and run Ethereum Wallet, it will start to download all of the blocks into this alternate directory!\\n\\n![](/assets/images/img_59e43dd34c878.png)\\n\\nAnd in my HDD:\\n\\n![](/assets/images/img_59e43defb599e.png)\\n\\n# What if I migrate an existing installation?\\n\\nLet\'s say you already installed and downloaded the Ethereum Chain but you want to do this. Well that is also pretty easy. First copy all the contents from\\n\\n```\\nC:\\\\Users\\\\<username>\\\\AppData\\\\Roaming\\\\Ethereum\\n```\\n\\nto your new location. It should be pretty large, so it may take a while. Be extra careful to make a backup of your `keystore` folder. Once the files have been copied over, you must delete the entire original Ethereum directory, including the `Ethereum` folder. Then you can run the same `mklink` in CMD as an Administrator.\\n\\n```bash\\nmklink /j \\"C:\\\\Users\\\\<username>\\\\AppData\\\\Roaming\\\\Ethereum\\" \\"D:\\\\Cryptocoins\\\\Ethereum\\"\\n```\\n\\nIf you forgot to delete the `Ethereum` folder, you will get this error:\\n\\n```\\nCannot create a file when that file already exists.\\n```\\n\\nI hope this helps someone! It certainly helped me as I was getting started."},{"id":"/aad/adding-aad-service-principal-company-administrator-role-using-aad-powershell-module/","metadata":{"permalink":"/blog/aad/adding-aad-service-principal-company-administrator-role-using-aad-powershell-module/","source":"@site/blog/2017-10-16-adding-aad-service-principal-company-administrator-role-using-aad-powershell-module.md","title":"Adding AAD Service Principal to the Company Administrator Role using the AAD PowerShell Module","description":"When creating a new Azure Active Directory application, developers may run into a a problem when calling the AAD Graph API where they lack the correct permissions to call the APIs they want when calling in the App Only Flow (Client Credentials Flow).","date":"2017-10-16T09:20:39.000Z","tags":[{"inline":true,"label":"aad powershell module","permalink":"/blog/tags/aad-powershell-module"},{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"graph api","permalink":"/blog/tags/graph-api"},{"inline":true,"label":"powershell","permalink":"/blog/tags/powershell"},{"inline":true,"label":"tenant administrator","permalink":"/blog/tags/tenant-administrator"}],"readingTime":2.31,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Adding AAD Service Principal to the Company Administrator Role using the AAD PowerShell Module","date":"2017-10-16T09:20:39.000Z","authors":"shawntabrizi","slug":"/aad/adding-aad-service-principal-company-administrator-role-using-aad-powershell-module/","categories":["AAD"],"tags":["aad powershell module","azure active directory","graph api","powershell","tenant administrator"]},"unlisted":false,"prevItem":{"title":"Getting started with Ethereum Wallet","permalink":"/blog/ethereum/getting-started-ethereum-wallet/"},"nextItem":{"title":"Common Microsoft Resources in Azure Active Directory","permalink":"/blog/aad/common-microsoft-resources-azure-active-directory/"}},"content":"When creating a new Azure Active Directory application, developers may run into a a problem when calling the AAD Graph API where they lack the correct permissions to call the APIs they want when calling in the App Only Flow (Client Credentials Flow).\\n\\nIs this message familiar to you?\\n\\n```json\\n\\"odata.error\\":{\\n  \\"code\\":\\"Authorization_RequestDenied\\",\\n  \\"message\\":{\\n      \\"lang\\":\\"en\\",\\"value\\":\\"Insufficient privileges to complete the operation.\\"\\n  }\\n}\\n```\\n\\nThe correct thing to do would be to try and investigate the permissions you have granted to your application, but there are some APIs which are not even supported through the permissions publicly exposed by the AAD Graph API. Maybe you just want to overcome this error for the time being and continue testing your end to end experience.\\n\\nUsing the AAD PowerShell Module, you can:\\n\\n- Give your application full access to the Graph API in the context of my tenant.\\n\\n**or**\\n\\n- Grant your application permissions to my tenant which are not currently supported with the permissions exposed by the AAD Graph API.\\n\\n\\"How?\\" you might ask.\\n\\nWell, you can elevate the level of access an Application has in your tenant by adding the service principal of that application to the `Company Administrator` Directory Role. This will give the Application the same level of permissions as the Company Administrator, who can do anything. You can follow these same instructions for any type of Directory Role depending on the level of access you want to give to this application.\\n\\n> **Note:** This will only affect the access your app has in your tenant.\\n>\\n> Also you must already be a Company Administrator of the tenant to follow these instructions.\\n\\nIn order to make the change, you will need to install the [Azure Active Directory PowerShell Module](https://docs.microsoft.com/en-us/powershell/msonline/v1/azureactivedirectory).\\n\\nOnce you have the module installed, authenticate to your tenant with your Administrator Account:\\n\\n```powershell\\nConnect-MSOLService\\n```\\n\\nThen we need to get the Object ID of both the Service Principal we want to elevate, and the Company Administrator Role for your tenant.\\n\\nSearch for Service Principal by App ID GUID:\\n\\n```powershell\\n$sp = Get-MsolServicePrincipal -AppPrincipalId <App ID GUID>\\n```\\n\\nSearch for Directory Role by Name\\n\\n```powershell\\n$role = Get-MsolRole -RoleName \\"Company Administrator\\"\\n```\\n\\nNow we can use the `Add-MsolRoleMember` command to add this role to the service principal.\\n\\n```powershell\\nAdd-MsolRoleMember -RoleObjectId $role.ObjectId -RoleMemberType ServicePrincipal -RoleMemberObjectId $sp.ObjectId\\n```\\n\\nTo check everything is working, lets get back all the members of the Company Administrator role:\\n\\n```powershell\\nGet-MsolRoleMember -RoleObjectId $role.ObjectId\\n```\\n\\nYou should see your application in that list, where `RoleMemberType` is `ServicePrincipal` and `DisplayName` is the name of your application.\\n\\nNow your application should be able to perform any Graph API calls that the Company Administrator could do, all without a user signed-in, using the Client Credential Flow.\\n\\nLet me know if this helps!"},{"id":"/aad/common-microsoft-resources-azure-active-directory/","metadata":{"permalink":"/blog/aad/common-microsoft-resources-azure-active-directory/","source":"@site/blog/2017-09-20-common-microsoft-resources-azure-active-directory.md","title":"Common Microsoft Resources in Azure Active Directory","description":"I have seen a lot of StackOverflow posts trying to debug pretty basic errors when getting an access token to Microsoft Resources. Sometimes the issue is as simple as a typo in the \\"resource\\" value in the token request. When helping these users, I struggle to find public documentation which shows plainly the correct resource values for these different APIs!","date":"2017-09-20T09:23:24.000Z","tags":[{"inline":true,"label":"access token","permalink":"/blog/tags/access-token"},{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"first party applications","permalink":"/blog/tags/first-party-applications"},{"inline":true,"label":"resource","permalink":"/blog/tags/resource"}],"readingTime":1.155,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Common Microsoft Resources in Azure Active Directory","date":"2017-09-20T09:23:24.000Z","authors":"shawntabrizi","slug":"/aad/common-microsoft-resources-azure-active-directory/","categories":["AAD"],"tags":["access token","azure active directory","first party applications","resource"]},"unlisted":false,"prevItem":{"title":"Adding AAD Service Principal to the Company Administrator Role using the AAD PowerShell Module","permalink":"/blog/aad/adding-aad-service-principal-company-administrator-role-using-aad-powershell-module/"},"nextItem":{"title":"Refresh Tokens for Azure AD V2 Applications in Flask","permalink":"/blog/aad/refresh-tokens-azure-ad-v2-applications-flask/"}},"content":"I have seen a lot of StackOverflow posts trying to debug pretty basic errors when getting an access token to Microsoft Resources. Sometimes the issue is as simple as a typo in the \\"resource\\" value in the token request. When helping these users, I struggle to find public documentation which shows plainly the correct resource values for these different APIs!\\n\\nThat is going to change starting now. Here will be a list of the most popular Microsoft APIs exposed on Azure Active Directory, along with the basic information you may need to get an access token to those resources for PROD. (If you want the details for other Environments, let me know!)\\n\\n> Note: The Resource URI must match exactly what is written below, including any trailing \'/\' or lack thereof.\\n\\n<table>\\n<tbody>\\n<tr>\\n<th>Resource Name</th>\\n<th>Resource URI</th>\\n<th>Application ID</th>\\n</tr>\\n<tr>\\n<td>AAD Graph API</td>\\n<td>https://graph.windows.net/</td>\\n<td>00000002-0000-0000-c000-000000000000</td>\\n</tr>\\n<tr>\\n<td>Office 365 Exchange Online</td>\\n<td>https://outlook-sdf.office.com/</td>\\n<td>00000002-0000-0ff1-ce00-000000000000</td>\\n</tr>\\n<tr>\\n<td>Microsoft Graph</td>\\n<td>https://graph.microsoft.com</td>\\n<td>00000003-0000-0000-c000-000000000000</td>\\n</tr>\\n<tr>\\n<td>Skype for Business Online</td>\\n<td>https://api.skypeforbusiness.com/</td>\\n<td>00000004-0000-0ff1-ce00-000000000000</td>\\n</tr>\\n<tr>\\n<td>Office 365 Yammer</td>\\n<td>https://api.yammer.com/</td>\\n<td>00000005-0000-0ff1-ce00-000000000000</td>\\n</tr>\\n<tr>\\n<td>OneNote</td>\\n<td>https://onenote.com/</td>\\n<td>2d4d3d8e-2be3-4bef-9f87-7875a61c29de</td>\\n</tr>\\n<tr>\\n<td>Windows Azure Service Management API</td>\\n<td>https://management.core.windows.net/</td>\\n<td>797f4846-ba00-4fd7-ba43-dac1f8f63013</td>\\n</tr>\\n<tr>\\n<td>Office 365 Management APIs</td>\\n<td>https://manage.office.com</td>\\n<td>c5393580-f805-4401-95e8-94b7a6ef2fc2</td>\\n</tr>\\n<tr>\\n<td>Microsoft Teams Services</td>\\n<td>https://api.spaces.skype.com/</td>\\n<td>cc15fd57-2c6c-4117-a88c-83b1d56b4bbe</td>\\n</tr>\\n<tr>\\n<td>Azure Key Vault</td>\\n<td>https://vault.azure.net</td>\\n<td>cfa8b339-82a2-471a-a3c9-0fc0be7a4093</td>\\n</tr>\\n</tbody>\\n</table>\\n<br/>\\n\\nWho knows if this will actually end up helping anyone, but I hope it will!"},{"id":"/aad/refresh-tokens-azure-ad-v2-applications-flask/","metadata":{"permalink":"/blog/aad/refresh-tokens-azure-ad-v2-applications-flask/","source":"@site/blog/2017-08-16-refresh-tokens-azure-ad-v2-applications-flask.md","title":"Refresh Tokens for Azure AD V2 Applications in Flask","description":"I have been working on a few projects recently that used Flask, a Python web framework, and Azure Active Directory to do things related to the Microsoft Graph. Using flask_oauthlib and the Azure AD V2 endpoint, it has been really easy to set up basic authentication for my web apps.","date":"2017-08-17T07:45:38.000Z","tags":[{"inline":true,"label":"access token","permalink":"/blog/tags/access-token"},{"inline":true,"label":"authentication","permalink":"/blog/tags/authentication"},{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"flask","permalink":"/blog/tags/flask"},{"inline":true,"label":"microsoft graph","permalink":"/blog/tags/microsoft-graph"},{"inline":true,"label":"python","permalink":"/blog/tags/python"},{"inline":true,"label":"refresh token","permalink":"/blog/tags/refresh-token"}],"readingTime":3.095,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Refresh Tokens for Azure AD V2 Applications in Flask","date":"2017-08-17T07:45:38.000Z","authors":"shawntabrizi","slug":"/aad/refresh-tokens-azure-ad-v2-applications-flask/","categories":["AAD"],"tags":["access token","authentication","azure active directory","flask","microsoft graph","python","refresh token"]},"unlisted":false,"prevItem":{"title":"Common Microsoft Resources in Azure Active Directory","permalink":"/blog/aad/common-microsoft-resources-azure-active-directory/"},"nextItem":{"title":"Revoking Consent for Azure Active Directory Applications","permalink":"/blog/aad/revoking-consent-azure-active-directory-applications/"}},"content":"I have been working on a few projects recently that used Flask, a Python web framework, and Azure Active Directory to do things related to the Microsoft Graph. Using [flask_oauthlib](https://flask-oauthlib.readthedocs.io/en/latest/) and the [Azure AD V2 endpoint](https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-appmodel-v2-overview), it has been really easy to set up basic authentication for my web apps.\\n\\nHowever, we quickly ran into basic authentication headaches like token expiry. It seems pretty obvious to the end user that as long as they haven\'t logged out since the last time they visited the site, they should stay automatically logged in. However, if you set up a naive implementation of authentication, you will find that the access token you store for the user is only valid for a limited time; [by default just 1 hour](https://docs.microsoft.com/en-us/azure/active-directory/active-directory-configurable-token-lifetimes).\\n\\n**So do we make our user sign in every hour?**\\n\\nHell no. We need to use refresh tokens which can be exchanged for NEW access tokens, all without the user being asked to sign in again.\\n\\n**How do we get a refresh token?**\\n\\nIn order to get a refresh token from the Azure AD V2 endpoint, you need to make sure your application requests a specific scope: offline_access. As stated [here](https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-v2-scopes#openid-connect-scopes):\\n\\n> When a user approves the `offline_access` scope, your app can receive refresh tokens from the v2.0 token endpoint. Refresh tokens are long-lived. Your app can get new access tokens as older ones expire.\\n>\\n> If your app does not request the `offline_access` scope, it won\'t receive refresh tokens.\\n\\n**So how do we do it?**\\n\\nUnfortunately flask_oauthlib does not directly support refresh tokens, but it does support [remote methods](https://flask-oauthlib.readthedocs.io/en/latest/client.html?highlight=post#invoking-remote-methods), so we should be able to simply make a POST request for a new access token! Here is the surprisingly simple code you need:\\n\\n```python\\n    data = {}\\n    data[\'grant_type\'] = \'refresh_token\'\\n    data[\'refresh_token\'] = session[\'refresh_token\']\\n    data[\'client_id\'] = microsoft.consumer_key\\n    data[\'client_secret\'] = microsoft.consumer_secret\\n\\n    response = (microsoft.post(microsoft.access_token_url, data=data)).data\\n```\\n\\nThat\'s it! What you are not seeing here is the original code used to get an access token, but I am just doing the normal flask_oauthlib stuff (which you can find in [this sample](https://github.com/Azure-Samples/active-directory-python-flask-graphapi-web-v2)), and storing the results of the token response in the user\'s session (Access Token, Expires In, and Refresh Token).\\n\\nNow, in order to invoke this code at the right time, I need to create a [view decorator](http://flask.pocoo.org/docs/0.12/patterns/viewdecorators/), and Flask has a sample for almost exactly what we want to do here: a login required decorator.\\n\\nBasically, we add this decorator to any view where we expect the user to be signed in. If the user has no token, we will redirect them to the login page. If they have an expired token and a refresh token, we will use the refresh token to get a new access token. Otherwise, if the token is present and valid, we simply let the view load.\\n\\nCheck it out in full action here:\\n\\n```python\\ndef login_required(f):\\n    @wraps(f)\\n    def wrap(*args, **kwargs):\\n        if \'microsoft_token\' in session:\\n            if session[\'expires_at\'] > datetime.now():\\n                return f(*args, **kwargs)\\n            elif \'refresh_token\' in session:\\n                # Try to get a Refresh Token\\n                data = {}\\n                data[\'grant_type\'] = \'refresh_token\'\\n                data[\'refresh_token\'] = session[\'refresh_token\']\\n                data[\'client_id\'] = microsoft.consumer_key\\n                data[\'client_secret\'] = microsoft.consumer_secret\\n\\n                response = (microsoft.post(microsoft.access_token_url, data=data)).data\\n\\n                if response is None:\\n                    session.clear()\\n                    print(\\"Access Denied: Reason=%s\\\\nError=%s\\" % (response.get(\'error\'),request.get(\'error_description\')))\\n                    return redirect(url_for(\'index\'))\\n                else:\\n                    session[\'microsoft_token\'] = (response[\'access_token\'], \'\')\\n                    session[\'expires_at\'] = datetime.now() + timedelta(seconds=int(response[\'expires_in\']))\\n                    session[\'refresh_token\'] = response[\'refresh_token\']\\n                    return f(*args, **kwargs)\\n        else:\\n            return redirect(url_for(\'login\'))\\n    return wrap\\n```\\n\\nSo now when we want to make a page require login we simply set up our view function like so:\\n\\n```python\\n@app.route(\'/home\')\\n@login_required\\ndef home():\\n    #view code goes here\\n```\\n\\nAdding this kind of code to your Azure AD Flask application can really be the difference between a good and bad user experience. Let me know if this ends up helping you out!"},{"id":"/aad/revoking-consent-azure-active-directory-applications/","metadata":{"permalink":"/blog/aad/revoking-consent-azure-active-directory-applications/","source":"@site/blog/2017-08-11-revoking-consent-azure-active-directory-applications.md","title":"Revoking Consent for Azure Active Directory Applications","description":"Today I was presenting one of my hackathon projects which I worked on this year to the Identity team at Microsoft. In order for my project to work, I needed to get consent to read the mail of the signed-in user. Depending on who you talk to, a permission like this could be easy as pie to consent to or something that they would never accept. Some people fall in the middle where they are happy to consent as long as they can choose to revoke that consent after they are done playing with the app.","date":"2017-08-11T15:17:38.000Z","tags":[{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"azure portal","permalink":"/blog/tags/azure-portal"},{"inline":true,"label":"consent","permalink":"/blog/tags/consent"},{"inline":true,"label":"my apps portal","permalink":"/blog/tags/my-apps-portal"},{"inline":true,"label":"tenant administrator","permalink":"/blog/tags/tenant-administrator"}],"readingTime":2.37,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Revoking Consent for Azure Active Directory Applications","date":"2017-08-11T15:17:38.000Z","authors":"shawntabrizi","slug":"/aad/revoking-consent-azure-active-directory-applications/","categories":["AAD"],"tags":["azure active directory","azure portal","consent","my apps portal","tenant administrator"]},"unlisted":false,"prevItem":{"title":"Refresh Tokens for Azure AD V2 Applications in Flask","permalink":"/blog/aad/refresh-tokens-azure-ad-v2-applications-flask/"},"nextItem":{"title":"Scraping LinkedIn Topics and Skills Data","permalink":"/blog/code/scraping-linkedin-topics-skills-data/"}},"content":"Today I was presenting one of my hackathon projects which I worked on this year to the Identity team at Microsoft. In order for my project to work, I needed to get consent to read the mail of the signed-in user. Depending on who you talk to, a permission like this could be easy as pie to consent to or something that they would never accept. Some people fall in the middle where they are happy to consent as long as they can choose to revoke that consent after they are done playing with the app.\\n\\nThat is why I am writing this. How easy it is to forget that it is NOT very obvious what you need to do to revoke consent for an Azure Active Directory Application. Even people on the Identity team don\'t always know! So let\'s talk about how you can do it :)\\n\\n## Using the My Apps Portal for Individual User Consent\\n\\nYou can revoke individual user consent through the [My Apps Portal](https://myapps.microsoft.com/). Here you should see a view of all applications that you or even your administrator (on your behalf) has consented to:\\n\\n![](/assets/images/img_598d51215d1c7.png)\\n\\nWith applications your admin has consented to, all you can do is open the app, however for apps where you individually consented as a user, you can click \\"Remove\\" which will revoke consent for the application.\\n\\n![](/assets/images/img_598d517f175d3.png)\\n\\n## Using the Azure Portal to Remove Tenant Wide Consent\\n\\nIf you are a tenant administrator, and you want to revoke consent for an application across your entire tenant, you can go to the [Azure Portal](https://portal.azure.com/). Whether it be for a bunch of users who individually consented or for an admin who consented on behalf of all the users, by simply deleting the application\'s service principal, you will remove all [OAuth 2 Permission Grants](https://msdn.microsoft.com/en-us/library/azure/ad/graph/api/entity-and-complex-type-reference#oauth2permissiongrant-entity) (the object used to store consent) linked to that application. Think about removing the service principal like uninstalling the application from your tenant.\\n\\nYou could delete the service principal a bunch of different ways like through [Azure Active Directory PowerShell](https://docs.microsoft.com/en-us/powershell/module/azuread/remove-azureadserviceprincipal) or through the [Microsoft Graph API](https://developer.microsoft.com/en-us/graph/docs/api-reference/beta/api/serviceprincipal_delete), but the easiest way for the average administrator is right through the Azure Portal.\\n\\nNavigate to the Enterprise Applications blade in the Azure portal:\\n\\n![](/assets/images/img_598d58dbe2787.png)\\n\\nThen click \\"All Applications\\" and search for the application you want to revoke consent for:\\n\\n![](/assets/images/img_598d594ddf163.png)\\n\\nWhen you click the application, you will be brought to an \\"Overview\\" section, where a tempting button called \\"Delete\\" will be at the top. Before you click this button, you might want to take a peak at the \\"Permissions\\" section to see the types of consent that was granted to this application:\\n\\n![](/assets/images/img_598d59b5e2851.png)\\n\\nOnce you feel confident that you want to delete this application, go back to \\"Overview\\" and click \\"Delete\\"!\\n\\n![](/assets/images/img_598d5ae51090c.png)\\n\\nViola! The app and all consent associated with that app is now gone."},{"id":"/code/scraping-linkedin-topics-skills-data/","metadata":{"permalink":"/blog/code/scraping-linkedin-topics-skills-data/","source":"@site/blog/2017-07-23-scraping-linkedin-topics-skills-data.md","title":"Scraping LinkedIn Topics and Skills Data","description":"Have you ever asked:","date":"2017-07-24T06:58:16.000Z","tags":[{"inline":true,"label":"crawling","permalink":"/blog/tags/crawling"},{"inline":true,"label":"hack","permalink":"/blog/tags/hack"},{"inline":true,"label":"linkedin","permalink":"/blog/tags/linkedin"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"scraper","permalink":"/blog/tags/scraper"},{"inline":true,"label":"skills","permalink":"/blog/tags/skills"},{"inline":true,"label":"topics","permalink":"/blog/tags/topics"}],"readingTime":7.17,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Scraping LinkedIn Topics and Skills Data","date":"2017-07-24T06:58:16.000Z","authors":"shawntabrizi","slug":"/code/scraping-linkedin-topics-skills-data/","categories":["Code"],"tags":["crawling","hack","linkedin","programming","scraper","skills","topics"],"github":"LinkedIn-Topic-Skill-Analysis"},"unlisted":false,"prevItem":{"title":"Revoking Consent for Azure Active Directory Applications","permalink":"/blog/aad/revoking-consent-azure-active-directory-applications/"},"nextItem":{"title":"Clients and Tokens and Claims! Oh My!","permalink":"/blog/aad/clients-tokens-claims-oh-my/"}},"content":"Have you ever asked:\\n\\n- What are the most popular skills among LinkedIn users?\\n- What are the most popular skills among Microsoft employees?\\n- Other top tech companies? (Google, Amazon, Facebook, etc...)\\n- What are the most interconnected skills?\\n\\nThese are questions that LinkedIn does not provide a direct answer to. However, through their \\"[Topics Directory](https://www.linkedin.com/directory/topics/)\\", we should be able to come to these conclusions ourselves!\\n\\nThe Topics Directory seems to be an index over all the different skills that people have put on their profile, alphabetically ordered by skill name. Some pages, like [Azure](https://www.linkedin.com/topic/azure), have very specific metadata about the skill, while others like [Azure Active Directory](https://www.linkedin.com/topic/azure-active-directory), show up in the directory, but do not have this additional metadata.\\n\\n![](/assets/images/img_59746ae38bf91.png)\\n\\nIf we look at the additional metadata, you can see that it calls out a number of very interesting data points. It tells you:\\n\\n1.  How many people have this skill\\n2.  The top 10 companies that have employees who register this skill\\n3.  (My guess) The top skills that people have who also have this skill\\n4.  (My guess) The top related skills\\n\\nNow clearly, there is some poor Web Design, in that there are two different sections, both with the same title \\"Top Skills\\", but contain different data. We will have to do our own interpretation of what this data exactly means, but nonetheless, the data is all useful.\\n\\nSo how do we start scouring this data to answer the questions I proposed at the start of this post? Well, by scraping it of course, and storing it into our own database. Now, this is [not an original idea](https://tech.bragboy.com/2016/11/crawl-all-linkedin-skills.html), but certainly I have not seen anyone collect the level of data which I am interested in. I want to have a copy of all the data points above for each topic, all in a single list!\\n\\n## So let\'s do it!\\n\\nOf course we will be using [Python](https://www.python.org/) + [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) + [Requests](https://docs.python-requests.org/en/master/). You can find the latest version of my LinkedIn scraper on [my GitHub](https://github.com/shawntabrizi/LinkedIn-Topic-Skill-Analysis). Here, I will only be looking at the main function, which describes the logic of my code, not the specific functions which actually does the scraping. You can find that on [my GitHub](https://github.com/shawntabrizi/LinkedIn-Topic-Skill-Analysis).\\n\\n```python\\nfrom bs4 import BeautifulSoup\\nimport requests\\nimport string\\nimport re\\nimport json\\n\\n# ...\\n# sub-functions removed, check GitHub for full source\\n# ...\\n\\ndef main():\\n    letters = list(string.ascii_lowercase)\\n    letters.append(\'more\')\\n    base_url = \\"https://www.linkedin.com/directory/topics-\\"\\n    for letter in letters:\\n        letter_url = base_url + letter + \\"/\\"\\n        content = get_content(letter_url)\\n        for con in content:\\n            if letter == \'y\' or letter == \'z\':\\n                sub_content = content\\n            else:\\n                letter_page_url = con.find(\\"a\\")\\n                print(letter_page_url)\\n                if letter_page_url.has_attr(\'href\'):\\n                    sub_content = get_content(letter_page_url[\'href\'])\\n                else:\\n                    sub_content = None\\n            for sub_con in sub_content:\\n                topic_url = sub_con.find(\\"a\\")\\n                topic = scrape_data(topic_url)\\n                create_json(topic)\\n            if letter == \'y\' or letter == \'z\':\\n                break\\n```\\n\\nTo scrape this site, we are basically figuring out the pattern which generates these pages. LinkedIn organizes these topics first by letter:\\n\\n```\\nhttps://www.linkedin.com/directory/topics-{letter}/\\n```\\n\\nThen on each \\"letter page\\", they group the topics by alphabetical order, in groups:\\n\\n```\\nhttps://www.linkedin.com/directory/topics-{letter}-{number}/\\n```\\n\\nFinally, if you navigate to the specific topic, you will get the final page with data:\\n\\n```\\nhttps://www.linkedin.com/topic/{topic}\\n```\\n\\nThere are a few exceptions to this pattern, which added complexity to the scraper. Basically the letters Y and Z do not have enough topics to be able to put them in groups, which means instead of navigating 3 pages deep to get the data, we need to navigate only 2 pages deep. You can see I handle this situation in my scraper. Other than that, once I get the data off the page, I put it into a JSON file for later usage!\\n\\nOne thing to note, but that I will not go into detail here about is that LinkedIn actually blocks scrapers in general, by creating a 999 response when you try to get data using a bot. If you want to run this script, you will have to overcome this. If you look online, people mention that you might need to update the user-agent passed in the headers of the web requests, but this did not work for me. I might go into detail about this during another post.\\n\\n## Results\\n\\nSo, let\'s look at some of the data. I can import the JSON as an array of dictionaries in Python, and then try and write some queries to get data from it. I am not claiming to write the best or most efficient queries, but hopefully they will get the correct data.\\n\\n### Loading the data:\\n\\n```python\\nwith open(r\'C:\\\\Users\\\\shawn\\\\Documents\\\\GitHubVisualStudio\\\\LinkedIn-Topic-Skill-Analysis\\\\results\\\\linkedin_topics_7-23-17.json\') as data_file:\\n    data = json.load(data_file)\\n```\\n\\n### How many topics are there total?\\n\\n```python\\nlen(data)\\n```\\n\\n<pre>33188</pre>\\n\\n### What are the most popular overall topics/skills?\\n\\n```python\\nordered_by_count = sorted(data, key=lambda k: k[\'count\'] if isinstance(k[\'count\'],int) else 0, reverse=True)\\nfor skill in ordered_by_count[:20]:\\n    print(skill[\'name\'])\\n```\\n\\n<pre>\\nManagement - 69725749\\nMicrosoft - 55910552\\nOffice - 46632581\\nMicrosoft Office - 45351678\\nPlanning - 34397412\\nMicrosoft Excel - 32966966\\nLeadership - 31017503\\nCustomer Service - 30810924\\nLeadership Management - 25854094\\nWord - 25793371\\nProject - 25766790\\nProject+ - 25766790\\nMicrosoft Word - 25567902\\nBusiness - 25374740\\nCustomer Management - 24946045\\nManagement Development - 24207445\\nDevelopment Management - 24207409\\nProject Management - 23922491\\nMarketing - 23047665\\nCustomer Service Management - 22856920\\n</pre>\\n\\n### What are the top <company>Skills?</company>\\n\\n```python\\ncompany = \'Microsoft\'\\ncompany_skills = []\\nfor skill in ordered_by_count:\\n    if skill[\'companies\'] is not None:\\n        if company in skill[\'companies\']:\\n            company_skills.append(skill)\\n\\norder_by_company = sorted(company_skills, key=lambda k: k[\'companies\'][company], reverse=True)\\nfor skill in order_by_company[:20]:\\n     print(skill[\'name\'], \\"-\\", skill[\'companies\'][company])\\n```\\n\\n#### Microsoft\\n\\n<pre>\\nCloud - 74817\\nCloud Computing - 74817\\nCloud-Computing - 74817\\nCloud Services - 74817\\nManagement - 73123\\nManagement Skills - 73123\\nMulti-Unit Management - 73123\\nEnterprise - 54516\\nEnterprise Software - 54516\\nSoftware Development - 53201\\nProject Management - 52083\\nProject Management Skills - 52083\\nPMP - 52083\\nPMI - 52083\\nStrategy - 43983\\nSaaS - 41450\\nSoftware as a Service - 41450\\nProgram Management - 40749\\nBusiness Intelligence - 39291\\nC# - 39158\\n</pre>\\n\\n#### Google\\n\\n<pre>\\nJava - 23225\\nStrategy - 22235\\nMarketing - 21672\\nData-driven Marketing - 21672\\nPython - 20788\\nSoftware Development - 20406\\nC++ - 20199\\nSocial Media - 20082\\nSocial Networks - 20082\\nDigital Marketing - 19942\\nOnline Advertising - 19922\\nMarketing Strategy - 16882\\nLinux - 16272\\nJavaScript - 14567\\nJavaScript Frameworks - 14567\\nC - 14460\\nC Programming - 14460\\nOnline Marketing - 13925\\nOnline-Marketing - 13925\\nSocial Media Marketing - 12931\\n</pre>\\n\\n#### Amazon\\n\\n<pre>\\nLeadership - 44329\\nLeadership Skills - 44329\\nMicrosoft Office - 42713\\nOffice for Mac - 42713\\nCustomer Service - 36176\\nMicrosoft Excel - 33403\\nJava - 25609\\nWord - 23314\\nMicrosoft Word - 23314\\nPowerPoint - 22318\\nMicrosoft PowerPoint - 22318\\nSocial Media - 22110\\nSocial Networks - 22110\\nC++ - 19619\\nTraining - 19250\\nMarketing - 18826\\nData-driven Marketing - 18826\\nSoftware Development - 18521\\nPublic Speaking - 17366\\nC - 16813\\n</pre>\\n\\n#### Facebook\\n\\n<pre>\\nDigital Marketing - 4973\\nOnline Advertising - 4334\\nDigital Strategy - 3399\\nOnline Marketing - 3012\\nOnline-Marketing - 3012\\nFacebook - 2883\\nAlgorithms - 2881\\nMobile Marketing - 2163\\nMachine Learning - 2103\\nDistributed Systems - 2033\\nUser Experience - 1971\\nUX - 1971\\nWeb Analytics - 1682\\nSEM - 1626\\nComputer Science - 1440\\nGoogle Analytics - 1261\\nAdwords - 1093\\nGoogle AdWords - 1093\\nScalability - 1057\\nMobile Advertising - 919\\n</pre>\\n\\n### What are the top interconnected skills?\\n\\n```python\\nskill_count = {}\\nfor topic in data:\\n    if topic[\'skills\'] is not None:\\n        for top_skill in topic[\'skills\']:\\n            if top_skill not in skill_count:\\n                skill_count[top_skill] = 1\\n            else:\\n                skill_count[top_skill] += 1\\n    if topic[\'topSkills\'] is not None:\\n        for top_skill in topic[\'topSkills\']:\\n            if top_skill not in skill_count:\\n                skill_count[top_skill] = 1\\n            else:\\n                skill_count[top_skill] += 1\\n\\nfor skill in sorted(skill_count, key=skill_count.get, reverse = True)[:20]:\\n    print(skill, \\"-\\", skill_count[skill])\\n```\\n\\n<pre>\\nMicrosoft Office - 11081\\nManagement - 8845\\nCustomer Service - 7010\\nProject Management - 6902\\nMicrosoft Excel - 4884\\nLeadership - 4682\\nSocial Media - 3883\\nResearch - 3798\\nPublic Speaking - 3243\\nMarketing - 2644\\nMicrosoft Word - 2426\\nSales - 2335\\nSQL - 2322\\nEngineering - 2300\\nBusiness Development - 2071\\nStrategic Planning - 1879\\nJava - 1792\\nAdobe Photoshop - 1555\\nJavaScript - 1488\\nMicrosoft PowerPoint - 1483\\n</pre>\\n\\nThere is so much more we can do with this data, and I do have plans! I just can\'t talk about them here. In a related note, I am super excited for the Microsoft Hackathon happening this next week. I will be using these tools, and hopefully more to accomplish an awesome project. Maybe more to share here in the future!"},{"id":"/aad/clients-tokens-claims-oh-my/","metadata":{"permalink":"/blog/aad/clients-tokens-claims-oh-my/","source":"@site/blog/2017-07-16-clients-tokens-claims-oh-my.md","title":"Clients and Tokens and Claims! Oh My!","description":"Let me just jump to the point with this post: Client applications should not depend on claims in access tokens to gather data about the signed-in user or anything about the authenticated session.","date":"2017-07-17T06:52:31.000Z","tags":[{"inline":true,"label":"access token","permalink":"/blog/tags/access-token"},{"inline":true,"label":"application","permalink":"/blog/tags/application"},{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"claims","permalink":"/blog/tags/claims"},{"inline":true,"label":"client","permalink":"/blog/tags/client"},{"inline":true,"label":"group membership claims","permalink":"/blog/tags/group-membership-claims"},{"inline":true,"label":"resource","permalink":"/blog/tags/resource"}],"readingTime":4.02,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Clients and Tokens and Claims! Oh My!","date":"2017-07-17T06:52:31.000Z","authors":"shawntabrizi","slug":"/aad/clients-tokens-claims-oh-my/","categories":["AAD"],"tags":["access token","application","azure active directory","claims","client","group membership claims","resource"]},"unlisted":false,"prevItem":{"title":"Scraping LinkedIn Topics and Skills Data","permalink":"/blog/code/scraping-linkedin-topics-skills-data/"},"nextItem":{"title":"Customizing WordPress\'s Twenty Seventeen Theme","permalink":"/blog/code/customizing-wordpresss-twenty-seventeen-theme/"}},"content":"Let me just jump to the point with this post: Client applications should not depend on claims in access tokens to gather data about the signed-in user or anything about the authenticated session.\\n\\nTime and time again, I have seen client applications complain to me that certain claims, like group membership claims, are not appearing in the access token they receive, and they ask me how to enable this. They incorrectly assume that if they go into their application manifest, and change the \\"groupMembershipClaims\\" settings, that they will be able to start getting claims, but everyone eventually finds out... it doesn\'t work!\\n\\nLet\'s take a look at source material; from the [OAuth 2 specification](https://tools.ietf.org/html/rfc6749#section-1.4):\\n\\n> An access token is a string representing an authorization issued to the client. **The string is usually opaque to the client**.\\n\\nUnfortunately, the OAuth 2 specification is intentionally broad, but in summary, the \'access token\' that is given to a client should only really be explored by the specified audience of the token. Some implementations of OAuth 2 do not even pass a JWT token to the client. Instead they pass a unique string, and then the resource exchanges that string for the actual token using a signed request. Alternatively, other implementations pass an encrypted JWT token rather than just a signed token. This means that the resource application uploads a token signing key which the authorization server uses to encrypt the full token. That means that the only person who can look at the claims in the token is the resource who also has the private key for decryption.\\n\\nThe implementation of OAuth 2 that I am most familiar with, Azure Active Directory, issues a signed token, which means that its content is completely visible to the client. In the future, Azure AD may add support for encrypted tokens, which means that clients are going to have to start following the correct practices.\\n\\nNeed to know about the user signed into your web application?\\n\\n- Get an [ID token](https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-token-and-claims#idtokens)! These are meant for client consumption!\\n\\nNeed to know which groups a user is a member of?\\n\\n- Get an access token to the AAD or Microsoft Graph API and query the API!\\n\\nNow lets go back to the original problem. If groupMembershipClaims are not meant for clients to get the claims in the access token, what are they used for? You might have figured out by now, but they are for **resource** applications to get the claims in the access token!\\n\\nLets show an example. To set up, I have registered two Azure AD Web Apps/APIs called Web API 1 and Web API 2\\\\. Both of these applications are identical, except Web API 1 has the setting \\"groupMembershipClaims\\": \\"All\\", and the other is set to null, which is default. I have to set up a fake App ID URI for both apps, and I have to make sure that each application has the other set as a \\"required permission\\".\\n\\nI will be using my [PowerShell Scripts](https://shawntabrizi.com/aad/azure-ad-authentication-with-powershell-and-adal/) to quickly get two access tokens. One where the client is Web API 1 and the resource is Web API 2, and vice versa.\\n\\nLet\'s look at the results, using my JWT Decoder to look at the payload:\\n\\n**Payload 1: Client = Web API 1, Resource = Web API 2**\\n\\n```json\\n{\\n  \\"aud\\": \\"https://shawntest.onmicrosoft.com/WebApi2\\",\\n  \\"iss\\": \\"https://sts.windows.net/4a4d599f-e69d-4cd8-a9e1-9882ea340fb5/\\",\\n  \\"iat\\": 1500243353,\\n  \\"nbf\\": 1500243353,\\n  \\"exp\\": 1500247253,\\n  \\"acr\\": \\"1\\",\\n  \\"aio\\": \\"ATQAy/.../oU\\",\\n  \\"amr\\": [\\"rsa\\", \\"mfa\\"],\\n  \\"appid\\": \\"eb7b6208-538c-487b-b5b5-137ac6ab6646\\",\\n  \\"appidacr\\": \\"1\\",\\n  \\"email\\": \\"shtabriz@microsoft.com\\",\\n  \\"family_name\\": \\"Tabrizi\\",\\n  \\"given_name\\": \\"Shawn\\",\\n  \\"idp\\": \\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\",\\n  \\"in_corp\\": \\"true\\",\\n  \\"ipaddr\\": \\"XX.XXX.XXX.XXX\\",\\n  \\"name\\": \\"Shawn Tabrizi\\",\\n  \\"oid\\": \\"41bdce9b-3940-40a9-b2f2-03a003ad599c\\",\\n  \\"platf\\": \\"3\\",\\n  \\"scp\\": \\"user_impersonation\\",\\n  \\"sub\\": \\"hfS9IZ_..._JW8c5Gg\\",\\n  \\"tid\\": \\"4a4d599f-e69d-4cd8-a9e1-9882ea340fb5\\",\\n  \\"unique_name\\": \\"shtabriz@microsoft.com\\",\\n  \\"ver\\": \\"1.0\\"\\n}\\n```\\n\\n**Payload 2: Client = Web API 2, Resource = Web API 1**\\n\\n```json\\n{\\n  \\"aud\\": \\"https://shawntest.onmicrosoft.com/WebApi1\\",\\n  \\"iss\\": \\"https://sts.windows.net/4a4d599f-e69d-4cd8-a9e1-9882ea340fb5/\\",\\n  \\"iat\\": 1500243330,\\n  \\"nbf\\": 1500243330,\\n  \\"exp\\": 1500247230,\\n  \\"acr\\": \\"1\\",\\n  \\"aio\\": \\"ATQAy/...BLDunA\\",\\n  \\"amr\\": [ \\"rsa\\", \\"mfa\\" ],\\n  \\"appid\\": \\"554e427d-36c3-4a77-89a5-a082ee333e12\\",\\n  \\"appidacr\\": \\"1\\",\\n  \\"email\\": \\"shtabriz@microsoft.com\\",\\n  \\"family_name\\": \\"Tabrizi\\",\\n  \\"given_name\\": \\"Shawn\\",\\n  \\"groups\\": [ \\"0f4374e6-8131-413e-b32b-f98bfdb371ed\\" ],     <-- Groups!\\n  \\"idp\\": \\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\",\\n  \\"in_corp\\": \\"true\\",\\n  \\"ipaddr\\": \\"XX.XXX.XXX.XXX\\",\\n  \\"name\\": \\"Shawn Tabrizi\\",\\n  \\"oid\\": \\"41bdce9b-3940-40a9-b2f2-03a003ad599c\\",\\n  \\"platf\\": \\"3\\",\\n  \\"scp\\": \\"user_impersonation\\",\\n  \\"sub\\": \\"xy..._zGJEZnIB4\\",\\n  \\"tid\\": \\"4a4d599f-e69d-4cd8-a9e1-9882ea340fb5\\",\\n  \\"unique_name\\": \\"shtabriz@microsoft.com\\",\\n  \\"ver\\": \\"1.0\\",\\n  \\"wids\\": [ \\"62e90394-69f5-4237-9190-012177145e10\\" ]\\n}\\n```\\n\\nNote that we only get the group membership claims when the **resource** application has this setting, not the client application. The client application has no power to control the types of claims in the token, because ultimately the token is not for them!\\n\\nIf you are building a client application using Azure Active Directory, please do not use the access token to try and get information about the authenticated session. The correct practice here is to request separately an ID Token , or to call the AAD / Microsoft Graph API to get the information you need. I hope you learned exactly how to use the \\"groupMembershipClaims\\" property and I hope this helps you build better apps in the future!"},{"id":"/code/customizing-wordpresss-twenty-seventeen-theme/","metadata":{"permalink":"/blog/code/customizing-wordpresss-twenty-seventeen-theme/","source":"@site/blog/2017-07-13-customizing-wordpresss-twenty-seventeen-theme.md","title":"Customizing WordPress\'s Twenty Seventeen Theme","description":"As I mentioned in my first post on this new blog, this isn\'t my first rodeo with WordPress or blogging in general. I have actually used a number of different content management systems in the past like Drupal, older versions of WordPress, and even forum platforms like ProBoards, phpBB, and vBulletin.","date":"2017-07-13T17:14:10.000Z","tags":[{"inline":true,"label":"css","permalink":"/blog/tags/css"},{"inline":true,"label":"theme","permalink":"/blog/tags/theme"},{"inline":true,"label":"twenty seventeen","permalink":"/blog/tags/twenty-seventeen"},{"inline":true,"label":"wordpress","permalink":"/blog/tags/wordpress"}],"readingTime":6.105,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Customizing WordPress\'s Twenty Seventeen Theme","date":"2017-07-13T17:14:10.000Z","authors":"shawntabrizi","slug":"/code/customizing-wordpresss-twenty-seventeen-theme/","categories":["Code"],"tags":["css","theme","twenty seventeen","wordpress"]},"unlisted":false,"prevItem":{"title":"Clients and Tokens and Claims! Oh My!","permalink":"/blog/aad/clients-tokens-claims-oh-my/"},"nextItem":{"title":"Does Company \\"X\\" have an Azure Active Directory Tenant?","permalink":"/blog/aad/does-company-x-have-an-azure-active-directory-tenant/"}},"content":"As I mentioned in my first post on this new blog, this isn\'t my first rodeo with WordPress or blogging in general. I have actually used a number of different content management systems in the past like Drupal, older versions of WordPress, and even forum platforms like ProBoards, phpBB, and vBulletin.\\n\\nAll of these tools are amazingly simple to use in comparison to how much they offer to their users. However, this latest version of Wordpress (version 4.8) has been above an beyond a great experience.\\n\\nI value the ability to make quick and accurate customization to my website, and with WordPress\'s huge library of plugins and their latest customization features like Additional CSS, it has been easier than ever to do just that. I wanted to share with you all a few of the specific customizations I made, and just how simple it was to do those things.\\n\\n## Adjusting the Social Links Footer\\n\\nOne of my top goals for this site was to have a single location for all of my various social media profiles. WordPress\'s new Social Links Menu makes it super easy to achieve this, however their default implementation was not very good looking in my opinion. Specifically, they made the \'social-navigation\' section have a width of 36%, with a max-width of 1000px for the container. This means the menu can have at most 5 icons before wrapping to a second line, and even less if the window is smaller.\\n\\n![](/assets/images/img_59672cc771bb5.png)\\n\\nI wanted to fix this. I wanted the Social Links Menu to take the entire bottom footer space, and I also wanted the links to align right, so that it would be right underneath my Home Page text. This also meant I needed to get rid of the \\"Proudly powered by WordPress\\" text. I am not against giving credit where credit is due, but not if it means compromising a good look.\\n\\nTo achieve my goals, I made the following simple CSS update in the Additional CSS settings:\\n\\n```css\\n.site-info {\\n  display: none;\\n}\\n.social-navigation {\\n  width: 100%;\\n  text-align: right;\\n}\\n```\\n\\nHere is the final result:\\n\\n![](/assets/images/img_59672e036f9d9.png)\\n\\n## Increasing the max-width of the theme\\n\\nSurprisingly, the twenty seventeen theme has a max-width of 1000px. We are in a generation of increasingly higher and higher resolution screens, and I think that 1000px was really choking the available space for a text based blog. I wanted to increase the amount of space used by the site to 1200px (+200px), so that my blog posts and code samples would be easier to read. To do this I made another CSS update on the Additional CSS settings:\\n\\n```css\\n.wrap {\\n  max-width: 1200px;\\n}\\n.navigation-top .wrap {\\n  max-width: 1200px;\\n}\\n```\\n\\nThis allowed me to change the base wrapper for most elements on the site to have a max width of 1200px, as well as the top navigation menu which adopted another parent style. The results were already great, but now the secondary content (which contains the blog sidebar) was taking too much space. The default setting for the twenty seventeen theme has the primary content take up 58% of the width, while the secondary content takes up 36% (with 6% spacing implicitly between the two). We added extra width to the overall content, but we really wanted all of that extra space to go to the primary content. So we simply need to update the percentages used to define the width for these two content divs:\\n\\n```css\\n@media screen and (min-width: 48em) {\\n  .has-sidebar:not(.error404) #primary {\\n    width: 68%;\\n  }\\n  .has-sidebar #secondary {\\n    width: 26%;\\n  }\\n}\\n```\\n\\nNote that we only wanted to adjust the CSS when the viewport is large. At lower screen sizes, the responsive layout of the theme takes over and it looks good out of the box.\\n\\nThe final results look great:\\n\\nBefore:\\n\\n![](/assets/images/img_5967329b806b6.png)\\n\\nAfter:\\n\\n![](/assets/images/img_596731d138858.png)\\n\\nYou can see my opening block of text which used to take 6 lines now only takes 4 lines. The difference is most notable when you look at the space between the primary and secondary content. You will see that it has significantly shifted to the right, while the primary content also extends further to the left. I think overall this makes the site look much better, and I might even consider increasing the max-width again.\\n\\n## Creating an About Me page as my Home Page\\n\\nThe last thing I wanted to do was to create a minimalist, yet good looking home page that would introduce me to the viewer, and act as a starting point for people to discover my site and other projects I might be working on. I needed a picture of myself, and a blurb about me. (If you have been paying attention, you will notice a sneak peak at the final result above.)\\n\\nSo what is the problem? Well, lets look at what the default theme does if you write some text and add an image:\\n\\n![](/assets/images/img_596735c8542b0.png)\\n\\nEw. This is such an unbalanced use of space. The title of the page eats up nearly half of the page\'s overall space, and the picture forces the text off the screen, which creates a disconnect between my image and my bio. Wouldn\'t it be better to have the image in that empty space to the left? I found the easiest way to do this was to simply add the html as the title of the page.\\n\\n![](/assets/images/img_596736809faf0.png)\\n\\nWhich gives us the following result:\\n\\n![](/assets/images/img_596736a48bb9f.png)\\n\\nMuch better! But I wanted to make a few more small tweaks. Lets make the image bigger, and lets make sure that it has no text alignment, so text doesn\'t start wrapping into it when the page changes size. Finally, I wanted to make the image an circle / oval rather than square. This is pretty common for bio pictures and quite easy to do, again using the Additional CSS settings. This was my final title:\\n\\n```html\\n<img\\n  class=\\"img-circle wp-image-103 size-large alignnone\\"\\n  src=\\"https://shawntabrizi.com/wordpress/wp-content/uploads/2017/07/19453121_1568143886560829_337872348308545095_o-927x1024.jpg\\"\\n  alt=\\"\\"\\n  width=\\"525\\"\\n  height=\\"580\\"\\n/>\\n```\\n\\nNote that I added a special class to the image called \'img-circle\'. This points to a configuration in my Additional CSS which changes the border-radius to 50%.\\n\\nHere is the final result:\\n\\n![](/assets/images/img_5967381f5095a.png)\\n\\nThis was exactly what I was going for, and I think makes a slick home page for any personal site. I expect that I will continue to make a few smaller changes, adding additional styles to the page, but really this was the starting point I needed to feel comfortable with this page being on the web, and it was done almost entirely using the Additional CSS settings available in the latest version of WordPress. Just amazing how simple WordPress makes it for the end user.\\n\\n## Final CSS Settings\\n\\nIf you liked the changes I made, and would like to do the same to your own instance of the twenty seventeen theme, you can copy and paste this CSS into your Additional CSS settings:\\n\\n```css\\n.site-info {\\n  display: none;\\n}\\n.social-navigation {\\n  width: 100%;\\n  text-align: right;\\n}\\n.wrap {\\n  max-width: 1200px;\\n}\\n.navigation-top .wrap {\\n  max-width: 1200px;\\n}\\n\\n@media screen and (min-width: 48em) {\\n  .has-sidebar:not(.error404) #primary {\\n    width: 68%;\\n  }\\n  .has-sidebar #secondary {\\n    width: 26%;\\n  }\\n}\\n\\n.img-circle {\\n  border-radius: 50%;\\n}\\n```\\n\\nLet me know if you found any other tricks or have iterated on the changes I made!"},{"id":"/aad/does-company-x-have-an-azure-active-directory-tenant/","metadata":{"permalink":"/blog/aad/does-company-x-have-an-azure-active-directory-tenant/","source":"@site/blog/2017-07-12-does-company-x-have-an-azure-active-directory-tenant.md","title":"Does Company \\"X\\" have an Azure Active Directory Tenant?","description":"One of the cool things about the Open ID Configuration endpoint is that it not only tells us random facts about the tenant, but it confirms that the tenant exists! Make sure to check out my last post to learn more about this. Using some clever scripting and this endpoint behavior, we could probably figure out which companies have an Azure Active Directory Tenant.","date":"2017-07-12T12:50:35.000Z","tags":[{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"hack","permalink":"/blog/tags/hack"},{"inline":true,"label":"powershell","permalink":"/blog/tags/powershell"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"script","permalink":"/blog/tags/script"},{"inline":true,"label":"tenant","permalink":"/blog/tags/tenant"}],"readingTime":2.335,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Does Company \\"X\\" have an Azure Active Directory Tenant?","date":"2017-07-12T12:50:35.000Z","authors":"shawntabrizi","slug":"/aad/does-company-x-have-an-azure-active-directory-tenant/","categories":["AAD"],"tags":["azure active directory","hack","powershell","programming","script","tenant"]},"unlisted":false,"prevItem":{"title":"Customizing WordPress\'s Twenty Seventeen Theme","permalink":"/blog/code/customizing-wordpresss-twenty-seventeen-theme/"},"nextItem":{"title":"Secret APIs in Azure Active Directory and Azure Resource Manager","permalink":"/blog/aad/secret-apis-in-azure-active-directory-and-azure-resource-manager/"}},"content":"One of the cool things about the Open ID Configuration endpoint is that it not only tells us random facts about the tenant, but it confirms that the tenant exists! Make sure to check out [my last post](https://shawntabrizi.com/aad/secret-apis-in-azure-active-directory-and-azure-resource-manager/) to learn more about this. Using some clever scripting and this endpoint behavior, we could probably figure out which companies have an Azure Active Directory Tenant.\\n\\nLet\'s try that!\\n\\n```powershell\\n$csv = Import-Csv -Path .\\\\input.csv\\n$output = @()\\n\\nforeach ($line in $csv)\\n{\\n    $companyname = $line.CompanyName\\n    $companynameencoded = [System.Net.WebUtility]::UrlEncode($companyname)\\n\\n    $GoogleURI = \'https://www.google.com/search?q=\' + $companynameencoded + \'&amp;btnI\'\\n\\n    try {\\n        $GoogleResult = Invoke-WebRequest -Uri $GoogleURI\\n        $CompanyURI = ([System.Uri]$GoogleResult.BaseResponse.ResponseUri).Host.split(\'.\')[-2..-1] -join \'.\'\\n    } catch {\\n        write-host $_.Exception\\n        $CompanyURI = \\"error\\"\\n    }\\n\\n    $OpenIDConfigURL = \'https://login.microsoftonline.com/\' + $CompanyURI + \'/.well-known/openid-configuration\'\\n\\n    try {\\n        $OpenIDResult = (Invoke-WebRequest -Uri $OpenIDConfigURL).StatusCode\\n    } catch {\\n        $OpenIDResult = $_.Exception.Response.StatusCode.value__\\n    }\\n\\n    if ($OpenIDResult -eq 200) {\\n        $tenant = $true\\n    } else {\\n        $tenant = $false\\n    }\\n\\n    $result = [pscustomobject]@{\\n        CompanyName = $companyname.ToString()\\n        HomepageURI = $CompanyURI.ToString()\\n        OpenIDResult = $OpenIDResult.ToString()\\n        HasTenant = $tenant.ToString()\\n    }\\n\\n    Write-Host $result\\n    $output += $result\\n}\\n\\n$output | Export-Csv -Path output.csv -NoTypeInformation\\n```\\n\\nSo in summary what does this script do?\\n\\n- We take a CSV which lists a bunch of Company Names.\\n- We then do a Google search, and go to the first result (\'I\'m Feeling Lucky\').\\n- We assume the first result is the homepage of that company, and the domain they would use for their tenant.\\n- We pull out the host name, and then check it against the Open ID Configuration endpoint.\\n  - If we get a valid response from the endpoint, then we say that they have a tenant!\\n  - Otherwise, we say they do not have a tenant.\\n\\nOne thing to note about these results is that when we get a result that says the company has a tenant, we are nearly 100% correct in that fact. However, if we say that a company does not have a tenant, we are not necessarily correct. It is possible that the google result did not point to their actual domain name, or they are using a different domain name for their AAD Tenant.\\n\\nIf you wanted to do this really robustly, you would probably want to get a better source for your domain names than automated google search results. You might want to also look at other combinations like \\"companyname.onmicrosoft.com\\", however we are doing just rough estimates here.\\n\\nSo lets look at the results for the Fortune 500.\\n\\nA quick Google search later, and I have a CSV with a list of all the Company Names for all 500 companies. Running it through this script, I find that 417, or 83.4% of companies have AAD, which is just a little off from Microsoft\'s public claim of 85%.\\n\\nNot bad for a quick and dirty script!"},{"id":"/aad/secret-apis-in-azure-active-directory-and-azure-resource-manager/","metadata":{"permalink":"/blog/aad/secret-apis-in-azure-active-directory-and-azure-resource-manager/","source":"@site/blog/2017-07-11-secret-apis-in-azure-active-directory-and-azure-resource-manager.md","title":"Secret APIs in Azure Active Directory and Azure Resource Manager","description":"Have you ever wondered what the Tenant ID for Microsoft (microsoft.com) or any other domain is? Have you ever wondered how you can find the right Tenant ID to sign in a user given their Azure Subscription ID?","date":"2017-07-11T16:40:44.000Z","tags":[{"inline":true,"label":"arm","permalink":"/blog/tags/arm"},{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"azure resource manager","permalink":"/blog/tags/azure-resource-manager"},{"inline":true,"label":"powershell","permalink":"/blog/tags/powershell"},{"inline":true,"label":"rest api","permalink":"/blog/tags/rest-api"},{"inline":true,"label":"subscription","permalink":"/blog/tags/subscription"},{"inline":true,"label":"tenant","permalink":"/blog/tags/tenant"}],"readingTime":4.17,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Secret APIs in Azure Active Directory and Azure Resource Manager","date":"2017-07-11T16:40:44.000Z","authors":"shawntabrizi","slug":"/aad/secret-apis-in-azure-active-directory-and-azure-resource-manager/","categories":["AAD"],"tags":["arm","azure active directory","azure resource manager","powershell","rest api","subscription","tenant"]},"unlisted":false,"prevItem":{"title":"Does Company \\"X\\" have an Azure Active Directory Tenant?","permalink":"/blog/aad/does-company-x-have-an-azure-active-directory-tenant/"},"nextItem":{"title":"Azure AD Authentication with PowerShell and ADAL","permalink":"/blog/aad/azure-ad-authentication-with-powershell-and-adal/"}},"content":"Have you ever wondered what the Tenant ID for Microsoft (microsoft.com) or any other domain is? Have you ever wondered how you can find the right Tenant ID to sign in a user given their Azure Subscription ID?\\n\\n...\\n\\nOh, you haven\'t? Well that is certainly more reasonable than the fact that I have; but, if for some reason you are asking the same questions as I am, let me tell you about some of the \\"secret APIs\\" that are available to answer those questions.\\n\\n## Getting the Tenant ID for a Verified Domain in Azure Active Directory\\n\\nAzure Active Directory tenants have a special type of domain called a \'verified domain\'. Verified domains are what they sound like, domains which a user has proven they own through DNS verification. These domains are unique across all tenants, and can act as a alternative domain to the initial domain given to all tenants (\\\\*.onmicrosoft.com).\\n\\nWhile authentication and even the AAD Graph API both support the use of these domains for referencing a tenant, not all APIs support this. Sometimes you might need to convert the tenant domain to a Tenant ID... but how?\\n\\n### Well known open id config\\n\\nCheck out the specification [here](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig). This Open ID configuration endpoint is required for all Open ID Providers, which AAD is one of. Let\'s take a look at what the response looks like for the Microsoft tenant using the verified domain \'microsoft.com\':\\n\\n[https://login.microsoftonline.com/**microsoft.com**/.well-known/openid-configuration](https://login.microsoftonline.com/microsoft.com/.well-known/openid-configuration)\\n\\n```json\\n{\\n  \\"authorization_endpoint\\": \\"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/authorize\\",\\n  \\"token_endpoint\\": \\"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/token\\",\\n  \\"token_endpoint_auth_methods_supported\\": [\\n    \\"client_secret_post\\",\\n    \\"private_key_jwt\\"\\n  ],\\n  \\"jwks_uri\\": \\"https://login.microsoftonline.com/common/discovery/keys\\",\\n  \\"response_modes_supported\\": [\\"query\\", \\"fragment\\", \\"form_post\\"],\\n  \\"subject_types_supported\\": [\\"pairwise\\"],\\n  \\"id_token_signing_alg_values_supported\\": [\\"RS256\\"],\\n  \\"http_logout_supported\\": true,\\n  \\"frontchannel_logout_supported\\": true,\\n  \\"end_session_endpoint\\": \\"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/logout\\",\\n  \\"response_types_supported\\": [\\n    \\"code\\",\\n    \\"id_token\\",\\n    \\"code id_token\\",\\n    \\"token id_token\\",\\n    \\"token\\"\\n  ],\\n  \\"scopes_supported\\": [\\"openid\\"],\\n  \\"issuer\\": \\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\",\\n  \\"claims_supported\\": [\\n    \\"sub\\",\\n    \\"iss\\",\\n    \\"cloud_instance_name\\",\\n    \\"cloud_graph_host_name\\",\\n    \\"aud\\",\\n    \\"exp\\",\\n    \\"iat\\",\\n    \\"auth_time\\",\\n    \\"acr\\",\\n    \\"amr\\",\\n    \\"nonce\\",\\n    \\"email\\",\\n    \\"given_name\\",\\n    \\"family_name\\",\\n    \\"nickname\\"\\n  ],\\n  \\"microsoft_multi_refresh_token\\": true,\\n  \\"check_session_iframe\\": \\"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/checksession\\",\\n  \\"userinfo_endpoint\\": \\"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/openid/userinfo\\",\\n  \\"tenant_region_scope\\": \\"WW\\",\\n  \\"cloud_instance_name\\": \\"microsoftonline.com\\",\\n  \\"cloud_graph_host_name\\": \\"graph.windows.net\\"\\n}\\n```\\n\\nI honestly have never used most of the data in this JSON, and I am not really sure where it gets used... BUT, you will notice that all of the various authentication endpoints now have a Tenant ID GUID rather than a domain name! This tells us two things:\\n\\n1.  The Tenant ID for Microsoft.com is 72f988bf-86f1-41af-91ab-2d7cd011db47\\n2.  (maybe this is obvious already.... but) Microsoft has a tenant!\\n\\nNow the second realization is kind of a super-set of the first, but it makes me think about something else cool we can do. What if we wanted to get a count and see which companies have an Azure Active Directory Tenant? As long as we know their Domain Name, we should be able to use this endpoint to confirm if a tenant exists! I will save this exploration for [my next blog post](https://shawntabrizi.com/aad/does-company-x-have-an-azure-active-directory-tenant/).\\n\\n## Get the Tenant ID for a Specific Azure Subscription ID\\n\\nThe world of Azure Subscriptions is one of the most complicated spaces that shouldn\'t be complicated. Depending on how you start using Azure, you may never even know that you have an Azure Active Directory Tenant. You just have your Live ID, which you use to sign on to the Azure Portal, and from there you can access your Subscription ID! You can\'t even use the \'common\' endpoint with Live IDs on AAD V1, so your lack of knowledge can be really painful here for app developers. We **need** your Tenant ID to know the right login endpoint to send you to. Luckily, we can find that using helpful error messages from Azure Resource Manager! All we need is an application for which we can get a token to Azure Resource Manager.\\n\\nWe can easily execute this plan using my [PowerShell Scripts](https://shawntabrizi.com/aad/azure-ad-authentication-with-powershell-and-adal/). Update the scripts to have the following configuration:\\n\\n- Pick any Tenant ID and Application Information relative to that tenant\\n- Set Resource ID to \\"https://management.azure.com/\\"\\n- Create a variable \\"$subscriptionId\\" and set it to the Azure Subscription ID you are looking to investigate.\\n- Set up the REST call like this:\\n\\n```powershell\\ntry {\\n    Invoke-RestMethod -Method Get -Uri (\\"{0}/subscriptions/{1}?api-version=2016-06-01\\" -f $resourceId, $subscriptionId) -Headers $headers\\n} catch {\\n    Write-Host $_.ErrorDetails.Message\\n}\\n```\\n\\nHmm... why would I be catching an error? Well let\'s run it and see what gets outputted:\\n\\n```json\\n{\\n  \\"error\\": {\\n    \\"code\\": \\"InvalidAuthenticationTokenTenant\\",\\n    \\"message\\": \\"The access token is from the wrong issuer \'https://sts.windows.net/4a4d599f-e69d-4cd8-a9e1-9882ea340fb5/\'. It must match the tenant \'https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\' associated with this subscription. Please use the authority (URL) \'https://login.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47\' to get the token. Note, if the subscription is transferred to another tenant there is no impact to the services, but information about new tenant could take time to propagate (up to an hour). If you just transferred your subscription and see this error message, please try back later.\\"\\n  }\\n}\\n```\\n\\nRight in the error they tell us the correct tenant for this Subscription ID!\\n\\n```\\nPlease use the authority (URL) \'https://login.windows.net/**72f988bf-86f1-41af-91ab-2d7cd011db47**\'\\n```\\n\\nThis really is a \\"secret API\\", and we can use it to consistently get back the right tenant for a user, as long as they know what their Azure Subscription is."},{"id":"/aad/azure-ad-authentication-with-powershell-and-adal/","metadata":{"permalink":"/blog/aad/azure-ad-authentication-with-powershell-and-adal/","source":"@site/blog/2017-07-09-azure-ad-authentication-with-powershell-and-adal.md","title":"Azure AD Authentication with PowerShell and ADAL","description":"In the 3 years I spent on the Azure AD team, I learned a number of useful \'tricks\' to make my job (and usually the jobs of others) a ton easier. However, if I had to pick just one trick to share to others trying to learn, it would probably be the PowerShell scripts I wrote to quickly get an access token to Azure Active Directory and then call AAD protected APIs like the AAD Graph API.","date":"2017-07-09T17:54:45.000Z","tags":[{"inline":true,"label":"access token","permalink":"/blog/tags/access-token"},{"inline":true,"label":"adal","permalink":"/blog/tags/adal"},{"inline":true,"label":"authentication","permalink":"/blog/tags/authentication"},{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"graph api","permalink":"/blog/tags/graph-api"},{"inline":true,"label":"powershell","permalink":"/blog/tags/powershell"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"},{"inline":true,"label":"script","permalink":"/blog/tags/script"}],"readingTime":4.445,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Azure AD Authentication with PowerShell and ADAL","date":"2017-07-09T17:54:45.000Z","authors":"shawntabrizi","slug":"/aad/azure-ad-authentication-with-powershell-and-adal/","categories":["AAD"],"tags":["access token","adal","authentication","azure active directory","graph api","powershell","programming","script"],"github":"Azure-AD-Authentication-with-PowerShell-and-ADAL"},"unlisted":false,"prevItem":{"title":"Secret APIs in Azure Active Directory and Azure Resource Manager","permalink":"/blog/aad/secret-apis-in-azure-active-directory-and-azure-resource-manager/"},"nextItem":{"title":"Decoding JWT Tokens","permalink":"/blog/aad/decoding-jwt-tokens/"}},"content":"In the 3 years I spent on the Azure AD team, I learned a number of useful \'tricks\' to make my job (and usually the jobs of others) a ton easier. However, if I had to pick just one trick to share to others trying to learn, it would probably be the PowerShell scripts I wrote to quickly get an access token to Azure Active Directory and then call AAD protected APIs like the AAD Graph API.\\n\\nIn general, authentication is hard, and requires way more set up than should be needed for simple testing. To get AAD authentication working on other platforms, you may need to write a ton of code, compile it, or even publish it to the web. With these scripts, you can get authentication and REST API calls done with as little as 13 lines of PowerShell. Running the code is instant, and modifying the REST calls or even the authentication parameters takes seconds rather than minutes.\\n\\n## How to get the samples\\n\\nYou can find all the basic scripts I have written on GitHub here:\\n\\n[https://github.com/shawntabrizi/Azure-AD-Authentication-with-PowerShell-and-ADAL](https://github.com/shawntabrizi/Azure-AD-Authentication-with-PowerShell-and-ADAL)\\n\\nI provide different scripts for different authentication flows:\\n\\n1.  Authorization Code Grant Flow for Confidential Client\\n2.  Native Client Authentication\\n3.  Client Credential Flow\\n    1.  Using Application Key\\n    2.  Using Client Certificate\\n\\nEach script ends with a REST API call to get the list of Users in your tenant using the AAD Graph API. You should be able to do this with any application because it uses the \\"Sign in and read basic profile\\" permission which is assigned to all AAD Applications by default.\\n\\nNote that to get these samples running, you will need to add the .NET dlls for ADAL v2 into the ADAL folder. You can find those files [on NuGet](https://www.nuget.org/packages/Microsoft.IdentityModel.Clients.ActiveDirectory/2.28.4).\\n\\n## Why it is so darn useful\\n\\nSo now that you have the scripts downloaded, and hopefully working, let me illustrate to you just a few of the different scenarios where I have used this tool to greatly simplify my work.\\n\\n### Verifying Token Claims\\n\\nSo many errors in AAD app development come from some sort of wrong setting, which may manifest itself in your access token. You might want to check the \'scp\' claims to see if your app has the right permissions. You might want to check the \'tid\' claim to make sure that you are getting a token to the right tenant! Or even the \'aud\' claim to make sure the token is for the correct resource. You can simply pump in the settings for your application into the appropriate PowerShell script, run the script, and you will get a .txt file with your access token in it. Then you can pop that JWT token into a JWT decoder like [the one I created...](https://shawntabrizi.com/JWT-Decoder/) and viola! There are your claims, and it took literally seconds.\\n\\n### Making quick REST API calls\\n\\nAnother thing that comes up very often around work is just pulling random data from AAD. Let\'s say that someone wants to know the settings of a certain Application Object, Service Principal, or even User. You may be able to do this with tools like the [Graph Explorer](https://graphexplorer.azurewebsites.net/), but what about some more complicated queries, or ones that you want to download to a file for later? Or how about simply wanting to test that YOUR app can make those queries rather than the Graph Explorer app. Not to mention the fact that you can call ANY AAD protected API, not just the AAD Graph API with these scripts. Simply update the Invoke-RestMethod command and bam, results will be saved into a .json file!\\n\\n### Making scripted REST API calls\\n\\nMaybe you are still not convinced that these scripts are useful. Most of what I showed above can be done if you want to use multiple other tools. However, I challenge you to find a quicker way to create \\"scripted\\" REST API calls. What do I mean by that? Lets say you wanted to pull a list of all the users in your company. Well the AAD Graph API can return at most 999 results in a single call, so you probably want to create a loop that iterates over the paged results that the Graph API returns. This is SIMPLE!\\n\\nHere is the loop I wrote to solve this exact problem:\\n\\n```powershell\\n$result = Invoke-RestMethod -Method Get -Uri (\'{0}/{1}/users/?api-version=1.6&amp;amp;amp;$top=999\' -f $resourceId,$tenantId) -Headers $headers\\n$count = 0\\n$result.value | Export-Csv ([String]$count + \\"_\\" +$output) -Encoding UTF8\\n\\nwhile (($result.\'odata.nextLink\' -split \'skiptoken=\')[1] -ne $null)\\n{\\n  $skiptoken = ($result.\'odata.nextLink\' -split \'skiptoken=\')[1]\\n  Write-Host (\'{0}/{1}/users/?api-version=1.6&amp;amp;amp;$top=999&amp;amp;amp;$skiptoken={2}\' -f $resourceId,$tenantId,$skiptoken)\\n\\n  try\\n  {\\n    $result = Invoke-RestMethod -Method Get -Uri (\'{0}/{1}/users/?api-version=1.6&amp;amp;amp;$top=999&amp;amp;amp;$skiptoken={2}\' -f $resourceId,$tenantId,$skiptoken) -Headers $headers\\n    $count += 1\\n    $result.value | Export-Csv ([String]$count + \\"_\\" + $output) -Encoding UTF8\\n  }\\n  catch\\n  {\\n    Write-Host \\"Error with Invoke Rest Method!\\"\\n    Write-Host $result.\'odata.nextLink\'\\n    break\\n  }\\n}\\n```\\n\\nThe result is a folder of CSV files all numbered and ready to be merged. If the script fails at some point (like if I lose an internet connection), I can use the outputted \'odata.nextLink\' and just pick up where I left off. I couldn\'t imagine doing this any other way for my needs.\\n\\n## Convinced?\\n\\nI hope that you too will be able to find this little tool helpful for your day to day needs. Let me know if you find some other unconventional uses for this!"},{"id":"/aad/decoding-jwt-tokens/","metadata":{"permalink":"/blog/aad/decoding-jwt-tokens/","source":"@site/blog/2017-07-05-decoding-jwt-tokens.md","title":"Decoding JWT Tokens","description":"Forewarning: I know that \\"JWT Tokens\\" is case of RAS syndrome... but I can\'t help it!","date":"2017-07-05T15:38:50.000Z","tags":[{"inline":true,"label":"azure active directory","permalink":"/blog/tags/azure-active-directory"},{"inline":true,"label":"javascript","permalink":"/blog/tags/javascript"},{"inline":true,"label":"json web token","permalink":"/blog/tags/json-web-token"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"}],"readingTime":3.24,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Decoding JWT Tokens","date":"2017-07-05T15:38:50.000Z","authors":"shawntabrizi","slug":"/aad/decoding-jwt-tokens/","categories":["AAD"],"tags":["azure active directory","javascript","json web token","programming"],"github":"JWT-Decoder-Javascript"},"unlisted":false,"prevItem":{"title":"Azure AD Authentication with PowerShell and ADAL","permalink":"/blog/aad/azure-ad-authentication-with-powershell-and-adal/"},"nextItem":{"title":"Discovery through Experience","permalink":"/blog/personal/discovery-through-experience/"}},"content":"> Forewarning: I know that \\"JWT Tokens\\" is case of [RAS syndrome](https://en.wikipedia.org/wiki/RAS_syndrome)... but I can\'t help it!\\n\\n## Are your tokens safe when using online decoders?\\n\\nIn the identity space, decoding JSON Web Tokens (JWT tokens) is a regular event. One of the first things we do in order to try and debug issues that customers or partners are having is taking a quick peek into the access tokens they are using, and seeing if anything is wrong.\\n\\nIn Azure Active Directory, we are commonly looking at the \\"audience\\" claim or the \\"scopes\\" in the token to make sure that they have the token to the right resource, and they have the right level of permissions for the task. But sometimes problems can be even more subtle than that. For example, the \\"tenant\\" information can be wrong, and people may never notice the subtle difference in GUID.\\n\\nEither way, being able to read the contents of a token is crucial, and so I have always relied on small web apps created by others to do this. However, at work recently, there was discussion about how the most popular site for this (https://jwt.io/) may be storing the tokens that are submitted into the app. If someone submits a token that is still active, there is a possibility that the site could use that token and impersonate you! Furthermore, the website was created by a Microsoft competitor, Auth0... so just bad news in general.\\n\\nI wanted to create my own JWT decoder so that I know for certain that my tokens are not being used maliciously, and so I could learn a little more about JWT tokens in general.\\n\\nI created this very basic page: [https://shawntabrizi.com/JWT-Decoder/](https://shawntabrizi.com/JWT-Decoder/)\\n\\nYou can find the GitHub source [here](https://github.com/shawntabrizi/JWT-Decoder). Let\'s talk about what I did.\\n\\n## JSON Web Token Structure\\n\\nA JWT token is broken up into 3 sections, all separated by periods. The first section is the Header, which contains information about the token type and the algorithm used to sign or encrypt that token. The second section is the Payload, where all the main claims are stored for the token. Finally, the third section is the token signature, where a token issuer can prove that they were the ones that actually minted the token. Tokens do not need to be signed, and if they are not, the third section will be empty. However, they will still contain a period to separate it from the second section as [shown here](https://tools.ietf.org/html/rfc7519#section-6.1).\\n\\nThe problem I needed to solve was pretty simple: Take the encoded JWT token, and get the claims out of it. I think the easiest way to explain the steps is simply to look at my commented code:\\n\\n```javascript\\n//This function takes a base 64 url encoded string, and converts it to a JSON object... using a few steps.\\nfunction decoder(base64url) {\\n  try {\\n    //Convert base 64 url to base 64\\n    var base64 = base64url.replace(\\"-\\", \\"+\\").replace(\\"_\\", \\"/\\");\\n    //atob() is a built in JS function that decodes a base-64 encoded string\\n    var utf8 = atob(base64);\\n    //Then parse that into JSON\\n    var json = JSON.parse(utf8);\\n    //Then make that JSON look pretty\\n    var json_string = JSON.stringify(json, null, 4);\\n  } catch (err) {\\n    json_string = \\"Bad Section.\\\\nError: \\" + err.message;\\n  }\\n  return json_string;\\n}\\n```\\n\\nJWT tokens are Base 64 URL encoded. While they are nearly the same, characters like \\"+\\" and \\"/\\" turn into \\"-\\" and \\"\\\\_\\" respectively. Learn more [here](https://en.wikipedia.org/wiki/Base64#URL_applications). From there, converting a Base 64 encoded string to a pretty JSON string is really self explanatory.\\n\\nThe rest of the work beyond this is just handling random user inputs. We have checks to verify the individual parts of the token are good, and whether or not the token contains a signature. As I suspected, creating a site to decode JWT tokens is really quite simple, and now I have my own site to do it on!"},{"id":"/personal/discovery-through-experience/","metadata":{"permalink":"/blog/personal/discovery-through-experience/","source":"@site/blog/2017-07-03-discovery-through-experience.md","title":"Discovery through Experience","description":"I have had a blog more than once in the past.","date":"2017-07-03T17:20:18.000Z","tags":[{"inline":true,"label":"hello world","permalink":"/blog/tags/hello-world"}],"readingTime":0.55,"hasTruncateMarker":false,"authors":[{"name":"Shawn Tabrizi","title":"Software Engineer","url":"https://github.com/shawntabrizi","imageURL":"https://github.com/shawntabrizi.png","key":"shawntabrizi","page":null}],"frontMatter":{"title":"Discovery through Experience","date":"2017-07-03T17:20:18.000Z","authors":"shawntabrizi","slug":"/personal/discovery-through-experience/","categories":["Personal"],"tags":["hello world"]},"unlisted":false,"prevItem":{"title":"Decoding JWT Tokens","permalink":"/blog/aad/decoding-jwt-tokens/"}},"content":"I have had a blog more than once in the past.\\n\\nAt different points in my life, blogging meant different things. The problem with blogging about myself or other personal things is that I find often I do not relate to my past self.\\n\\nThis blog should hopefully be more permanent because I hope to mostly talk about things related to technology, development, and problem solving.\\n\\nI think the subtitle of my blog is well fitting for the kind of content I hope to have on here. I want to learn, and then teach, all through experience and action.\\n\\nI hope that you will discover something new along with me."}]}}')}}]);